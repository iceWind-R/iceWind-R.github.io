<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Thorine</title>
  
  <subtitle>凡是过往，皆为序章</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://icewind-r.github.io/"/>
  <updated>2020-11-09T14:47:31.623Z</updated>
  <id>http://icewind-r.github.io/</id>
  
  <author>
    <name>大雪初晴丶</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spring_01</title>
    <link href="http://icewind-r.github.io/2020/11/09/Spring-01/"/>
    <id>http://icewind-r.github.io/2020/11/09/Spring-01/</id>
    <published>2020-11-09T11:24:21.000Z</published>
    <updated>2020-11-09T14:47:31.623Z</updated>
    
    <content type="html"><![CDATA[<p>Spring 入门。</p><a id="more"></a><hr><p>Spring 全家桶：spring，springMVC，Spring Boot，Spring Cloud，它们都是框架，负责Java开发的各个方面。先开始学习最基础的Spring。</p><h1 id="一、Spring概述"><a href="#一、Spring概述" class="headerlink" title="一、Spring概述"></a>一、Spring概述</h1><p>优点：减轻对项目模块的管理，类和类之间的管理，帮助开发人员创建对象，管理对象之间的关系。</p><p>Spring核心技术：IOC，AOP。能实现模块之间、类之间的解耦合。</p><p>spring是一个开源组织，有很多项目，在官网中可以看到有如下project，每一个project就是一个开源框架。</p><img src="/2020/11/09/Spring-01/1.png" class><p>我们学习的spring框架，就是其中的spring framework。</p><h2 id="Spring-优点"><a href="#Spring-优点" class="headerlink" title="Spring 优点"></a>Spring 优点</h2><ul><li><strong>轻量</strong>：核心功能依赖的jar包少，总共3M左右。且占用资源少。</li><li><strong>解耦合</strong>：针对接口编程，提供了IOC控制反转，由容器管理对象之间的依赖关系。原来在程序代码中的对象创建方式，现在由容器完成。</li><li><strong>AOP</strong>：进行面向切面的编程，许多不容易用传统OOP实现的功能可以通过AOP轻松应付。</li><li>方便集成各种优秀的框架：Spring对一些优秀的框架提供了直接的支持。</li><li>声明式事务的支持：只需要通过配置就可以完成对事务的管理，而无须手动编程。</li><li>方便程序的测试：Spring 支持 JUnit4，可以通过注解方便地测试 Spring 程序。</li></ul><h1 id="二、Spring体系结构详解"><a href="#二、Spring体系结构详解" class="headerlink" title="二、Spring体系结构详解"></a>二、Spring体系结构详解</h1><p>Spring框架采用分层架构，根据不同的功能被划分成了多个模块，这些模块大体可分为 Data Access/Integration、Web、AOP、Aspects、Messaging、Instrumentation、Core Container 和 Test。</p><img src="/2020/11/09/Spring-01/1.gif" class><p>图中包含了 Spring 框架的所有模块，这些模块可以满足一切企业级应用开发的需求，在开发过程中可以根据需求有选择性地使用所需要的模块。下面分别对这些模块的作用进行简单介绍。</p><h2 id="1-Data-Access-Integration（数据访问／集成）"><a href="#1-Data-Access-Integration（数据访问／集成）" class="headerlink" title="1. Data Access/Integration（数据访问／集成）"></a>1. Data Access/Integration（数据访问／集成）</h2><p>数据访问/集成层包括 JDBC、ORM、OXM、JMS 和 Transactions 模块，具体介绍如下。</p><ul><li>JDBC 模块：提供了一个 JDBC 的抽象层，大幅度减少了在开发过程中对数据库操作的编码。</li><li>ORM 模块：对流行的对象关系映射 API，包括 JPA、JDO、<a href="http://c.biancheng.net/hibernate/" target="_blank" rel="noopener">Hibernate</a> 和 iBatis 提供了的集成层。</li><li>OXM 模块：提供了一个支持对象/XML 映射的抽象层实现，如 JAXB、Castor、XMLBeans、JiBX 和 XStream。</li><li>JMS 模块：指 Java 消息服务，包含的功能为生产和消费的信息。</li><li>Transactions 事务模块：支持编程和声明式事务管理实现特殊接口类，并为所有的 POJO。</li></ul><h2 id="2-Web-模块"><a href="#2-Web-模块" class="headerlink" title="2. Web 模块"></a>2. Web 模块</h2><p>Spring 的 Web 层包括 Web、Servlet、Struts 和 Portlet 组件，具体介绍如下。</p><ul><li>Web 模块：提供了基本的 Web 开发集成特性，例如多文件上传功能、使用的 Servlet 监听器的 IoC 容器初始化以及 Web 应用上下文。</li><li>Servlet模块：包括 Spring 模型—视图—控制器（MVC）实现 Web 应用程序。</li><li>Struts 模块：包含支持类内的 Spring 应用程序，集成了经典的 Struts Web 层。</li><li>Portlet 模块：提供了在 Portlet 环境中使用 MV C实现，类似 Web-Servlet 模块的功能。</li></ul><h2 id="3-Core-Container（核心容器）"><a href="#3-Core-Container（核心容器）" class="headerlink" title="3. Core Container（核心容器）"></a>3. Core Container（核心容器）</h2><p>Spring 的核心容器是其他模块建立的基础，由 Beans 模块、Core 核心模块、Context 上下文模块和 Expression Language 表达式语言模块组成，具体介绍如下。</p><ul><li>Beans 模块：提供了 BeanFactory，是工厂模式的经典实现，Spring 将管理对象称为 Bean。</li><li>Core 核心模块：提供了 Spring 框架的基本组成部分，包括 IoC 和 DI 功能。</li><li>Context 上下文模块：建立在核心和 Beans 模块的基础之上，它是访问定义和配置任何对象的媒介。ApplicationContext 接口是上下文模块的焦点。</li><li>Expression Language 模块：是运行时查询和操作对象图的强大的表达式语言。</li></ul><h2 id="4-其他模块"><a href="#4-其他模块" class="headerlink" title="4. 其他模块"></a>4. 其他模块</h2><p>Spring的其他模块还有 AOP、Aspects、Instrumentation 以及 Test 模块，具体介绍如下。</p><ul><li>AOP 模块：提供了面向切面编程实现，允许定义方法拦截器和切入点，将代码按照功能进行分离，以降低耦合性。</li><li>Aspects 模块：提供与 AspectJ 的集成，是一个功能强大且成熟的面向切面编程（AOP）框架。</li><li>Instrumentation 模块：提供了类工具的支持和类加载器的实现，可以在特定的应用服务器中使用。</li><li>Test 模块：支持 Spring 组件，使用 JUnit 或 TestNG 框架的测试。</li></ul><h1 id="三、Spring-IOC-控制反转"><a href="#三、Spring-IOC-控制反转" class="headerlink" title="三、Spring IOC 控制反转"></a>三、Spring IOC 控制反转</h1><p>控制反转（IoC，Inversion of Control），指将把对象的创建、赋值、管理工作都交给代码之外的容器实现。</p><p>控制：创建对象，对象的属性赋值，对象之间的关系管理。</p><p>正转：使用new关键字构造方法创建对象，开发人员主动管理对象。</p><p>反转：把原来的开发人员管理创建对象的权限转移给代码之外的容器实现，由容器代替开发人员管理对象。</p><p>容器：是一个服务器软件，一个框架，Spring就是这样一个框架。</p><p>为什么IoC：减少对代码的改动，实现解耦合。</p><h2 id="IoC体现"><a href="#IoC体现" class="headerlink" title="IoC体现"></a>IoC体现</h2><p>Tomcat作为容器，里面存放着Servlet，Listener，Filter等对象，不用手动创建对象。</p><h2 id="IoC技术实现"><a href="#IoC技术实现" class="headerlink" title="IoC技术实现"></a>IoC技术实现</h2><p>使用DI（Dependency Injection）实现IoC的功能。</p><p>DI：依赖注入，只需要在程序中提供所使用对象的名称就可以由 DI 自动创建，赋值和查找。使用的是反射机制。</p><h1 id="四、Spring-的第一个程序"><a href="#四、Spring-的第一个程序" class="headerlink" title="四、Spring 的第一个程序"></a>四、Spring 的第一个程序</h1><p>步骤：</p><p>1、创建Maven项目，导入spring 的相关依赖</p><p>2、创建类（接口类和其实现类），和没有使用框架一样，普通的类。</p><p>3、创建spring需要使用的配置文件：声明类的信息，这些类由spring创建和管理。</p><p>4、测试对象创建</p><h2 id="1、新建Maven工程"><a href="#1、新建Maven工程" class="headerlink" title="1、新建Maven工程"></a>1、新建Maven工程</h2><p>导入spring 的 maven依赖。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-context<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.2.5.RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="2、创建接口和实现类"><a href="#2、创建接口和实现类" class="headerlink" title="2、创建接口和实现类"></a>2、创建接口和实现类</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">SomeService</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">doSome</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.thorine.service.SomeService;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SomeServiceImpl</span> <span class="keyword">implements</span> <span class="title">SomeService</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">doSome</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"执行了SomeServiceImpl的 doSome() 方法。"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3、创建配置文件"><a href="#3、创建配置文件" class="headerlink" title="3、创建配置文件"></a>3、创建配置文件</h2><p>给Maven项目创建resources资源目录，在该目录下创建spring的xml配置文件，依赖导入成功后，会出现spring xml提示。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">beans</span> <span class="attr">xmlns</span>=<span class="string">"http://www.springframework.org/schema/beans"</span></span></span><br><span class="line"><span class="tag">       <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">       <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">        告诉spring创建对象</span></span><br><span class="line"><span class="comment">        声明bean，就是告诉spring创建某个类的对象</span></span><br><span class="line"><span class="comment">        id：对象的自定义名称，符合Java的命名规则，是唯一的。</span></span><br><span class="line"><span class="comment">        class：类的全限定名称（必须是类，不是接口）</span></span><br><span class="line"><span class="comment">    --&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"someService"</span> <span class="attr">class</span>=<span class="string">"com.thorine.service.impl.SomeServiceImpl"</span> /&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">        完成 ：SomeService service = new SomeServiceImpl();</span></span><br><span class="line"><span class="comment">        spring 是把创建好的对象放入map集合中，spring框架有一个map存放对象</span></span><br><span class="line"><span class="comment">        springMap.put(id的值, 对象);</span></span><br><span class="line"><span class="comment">        例如：springMap.put(someService, new SomeServiceImpl());</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">        注意：一个bean标签声明一个对象，想声明多个对象，就用多个bean标签</span></span><br><span class="line"><span class="comment">     --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">beans</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="4、测试对象创建"><a href="#4、测试对象创建" class="headerlink" title="4、测试对象创建"></a>4、测试对象创建</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 使用spring容器创建对象</span><br><span class="line">&#x2F;&#x2F; 指定spring配置文件的名称</span><br><span class="line">String config &#x3D; &quot;beans.xml&quot;;</span><br><span class="line">&#x2F;&#x2F; 创建表示spring容器的对象，ApplicationContext，就是表示Spring容器，通过容器获取对象</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;ClassPathXmlApplicationContext 表示从类路径中加载spring的配置文件</span><br><span class="line">ApplicationContext ac &#x3D; new ClassPathXmlApplicationContext(config);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 从容器中获取某个对象，你要调用对象的方法</span><br><span class="line">&#x2F;&#x2F; getBean(&quot;配置文件中的bean的id值&quot;)</span><br><span class="line">SomeService service &#x3D; (SomeService) ac.getBean(&quot;someService&quot;);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 使用spring创建好的对象</span><br><span class="line">service.doSome();</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Spring 入门。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Spring" scheme="http://iceWind-R.github.io/tags/Spring/"/>
    
  </entry>
  
  <entry>
    <title>Docker_01</title>
    <link href="http://icewind-r.github.io/2020/10/29/Docker-01/"/>
    <id>http://icewind-r.github.io/2020/10/29/Docker-01/</id>
    <published>2020-10-29T06:31:58.000Z</published>
    <updated>2020-10-29T08:07:40.158Z</updated>
    
    <content type="html"><![CDATA[<p>Docker 入门。</p><a id="more"></a><hr><h1 id="Docker简介"><a href="#Docker简介" class="headerlink" title="Docker简介"></a>Docker简介</h1><h2 id="什么是docker？"><a href="#什么是docker？" class="headerlink" title="什么是docker？"></a>什么是docker？</h2><p>Docker 属于 Linux 容器的一种封装，提供简单易用的容器使用接口。它是目前最流行的 Linux 容器解决方案。Docker  将应用程序与该程序的依赖，打包在一个文件里面。运行这个文件，就会生成一个虚拟容器。程序在这个虚拟容器里运行，就好像在真实的物理机上运行一样。有了 Docker，就不用担心环境问题。</p><h2 id="为什么使用Docker？"><a href="#为什么使用Docker？" class="headerlink" title="为什么使用Docker？"></a>为什么使用Docker？</h2><p>容器除了运行其中应用外，基本不消耗额外的系统资源，使得应用的性能很高，同时系统的开销尽量小。传统虚拟机方式运行 10 个不同的应用就要起 10 个虚拟机，而Docker 只需要启动 10 个隔离的应用即可。</p><p><strong>1、更快速的交付和部署</strong></p><p>对开发和运维（devop）人员来说，最希望的就是一次创建或配置，可以在任意地方正常运行。</p><p>开发者可以使用一个标准的镜像来构建一套开发容器，开发完成之后，运维人员可以直接使用这个容器来部署代码。 Docker  可以快速创建容器，快速迭代应用程序，并让整个过程全程可见，使团队中的其他成员更容易理解应用程序是如何创建和工作的。 Docker  容器很轻很快！容器的启动时间是秒级的，大量地节约开发、测试、部署的时间。</p><p><strong>2、更高效的虚拟化</strong></p><p>Docker 容器的运行不需要额外的 hypervisor 支持，它是内核级的虚拟化，因此可以实现更高的性能和效率。</p><p><strong>3、更轻松的迁移和扩展</strong></p><p>Docker 容器几乎可以在任意的平台上运行，包括物理机、虚拟机、公有云、私有云、个人电脑、服务器等。 这种兼容性可以让用户把一个应用程序从一个平台直接迁移到另外一个。</p><p><strong>4、更简单的管理</strong></p><p>使用 Docker，只需要小小的修改，就可以替代以往大量的更新工作。所有的修改都以增量的方式被分发和更新，从而实现自动化并且高效的管理。</p><h2 id="Docker相关概念"><a href="#Docker相关概念" class="headerlink" title="Docker相关概念"></a>Docker相关概念</h2><p>Docker是CS架构，主要有两个概念：</p><ul><li><strong>Docker daemon</strong>: 运行在宿主机上，Docker守护进程，用户通过Docker client(Docker命令)与Docker daemon交互</li><li><strong>Docker client</strong>: Docker 命令行工具，是用户使用Docker的主要方式，Docker client与Docker daemon通信并将结果返回给用户，Docker client也可以通过socket或者RESTful api访问远程的Docker daemon</li></ul><img src="/2020/10/29/Docker-01/1.png" class><p>了解了Docker的组成，再来了解一下Docker的三个主要概念：</p><ul><li><strong>Docker image</strong>：镜像是只读的，镜像中包含有需要运行的文件。镜像用来创建container，一个镜像可以运行多个container；镜像可以通过Dockerfile创建，也可以从Docker hub/registry上下载。</li><li><strong>Docker container</strong>：容器是Docker的运行组件，启动一个镜像就是一个容器，容器是一个隔离环境，多个容器之间不会相互影响，保证容器中的程序运行在一个相对安全的环境中。</li><li><strong>Docker hub/registry</strong>: 共享和管理Docker镜像，用户可以上传或者下载上面的镜像，官方地址为<code>https://registry.hub.docker.com/</code>，也可以搭建自己私有的Docker registry。</li></ul><p>镜像就相当于打包好的版本，镜像启动之后运行在容器中，仓库就是装存储镜像的地方。</p><h1 id="Docker安装配置"><a href="#Docker安装配置" class="headerlink" title="Docker安装配置"></a>Docker安装配置</h1><p>建议在linux环境下安装Docker，window环境搭建比较复杂且容易出错，使用Centos7+yum来安装Docker环境很方便。</p><p>Docker 软件包已经包括在默认的 CentOS-Extras 软件源里。因此想要安装 docker，只需要运行下面的 yum 命令：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install docker</span><br></pre></td></tr></table></figure><p>安装完成后，使用下面的命令来启动 docker 服务，并将其设置为开机启动：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">service docker start</span><br><span class="line">chkconfig docker on</span><br></pre></td></tr></table></figure><blockquote><p>LCTT 译注：此处采用了旧式的 sysv 语法，如采用CentOS 7中支持的新式 systemd 语法，如下：</p></blockquote><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl start docker.service</span><br><span class="line">systemctl <span class="built_in">enable</span> docker.service</span><br></pre></td></tr></table></figure><h2 id="Hello-World-案例"><a href="#Hello-World-案例" class="headerlink" title="Hello World 案例"></a>Hello World 案例</h2><p>下面，我们通过最简单的 image 文件”hello world”，感受一下 Docker。</p><p>因为国内连接 Docker 的官方仓库很慢，因此我们在日常使用中会使用Docker 中国加速器。通过 Docker  官方镜像加速，中国区用户能够快速访问最流行的 Docker  镜像。该镜像托管于中国大陆，本地用户现在将会享受到更快的下载速度和更强的稳定性，从而能够更敏捷地开发和交付 Docker 化应用。</p><p>Docker 中国官方镜像加速可通过<code>registry.docker-cn.com</code>访问。该镜像库只包含流行的公有镜像，私有镜像仍需要从美国镜像库中拉取。</p><p>修改系统中docker对应的配置文件即可，如下：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vi  /etc/docker/daemon.json</span><br><span class="line"><span class="comment">#添加后</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"registry-mirrors"</span>: [<span class="string">"https://registry.docker-cn.com"</span>],</span><br><span class="line">    <span class="string">"live-restore"</span>: <span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行下面的命令，将 image 文件从仓库抓取到本地。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull library/hello-world</span><br></pre></td></tr></table></figure><p>上面代码中，docker image pull是抓取 image 文件的命令。library/hello-world是 image 文件在仓库里面的位置，其中library是 image 文件所在的组，hello-world是 image 文件的名字。</p><p>抓取成功以后，就可以在本机看到这个 image 文件了。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker images</span><br><span class="line"><span class="comment">#显示结果</span></span><br><span class="line">REPOSITORY                      TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">docker.io/hello-world           latest              f2a91732366c        3 months ago        1.848 kB</span><br></pre></td></tr></table></figure><p>现在，运行这个 image 文件。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run hello-world</span><br><span class="line"></span><br><span class="line"><span class="comment">#显示结果</span></span><br><span class="line">Hello from Docker!</span><br><span class="line">This message shows that your installation appears to be working correctly.</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>输出这段提示以后，hello world就会停止运行，容器自动终止。有些容器不会自动终止，因为提供的是服务，比如Mysql镜像等。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Docker 入门。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://iceWind-R.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Docker" scheme="http://iceWind-R.github.io/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>MongoDB_01</title>
    <link href="http://icewind-r.github.io/2020/10/22/MongoDB-01/"/>
    <id>http://icewind-r.github.io/2020/10/22/MongoDB-01/</id>
    <published>2020-10-22T02:30:26.000Z</published>
    <updated>2020-10-22T15:29:07.687Z</updated>
    
    <content type="html"><![CDATA[<p>今天安装了mongoDB，记录一下。</p><a id="more"></a><hr><h1 id="一、MongoDB-centos7安装"><a href="#一、MongoDB-centos7安装" class="headerlink" title="一、MongoDB centos7安装"></a>一、MongoDB centos7安装</h1><p>首先在官网下载安装包，我这里选择的是RedHat,Linux， 得到的安装包为 ：mongodb-linux-x86_64-rhel70-3.6.20.tgz。上传到虚拟机解压。</p><img src="/2020/10/22/MongoDB-01/1.png" class><p>重命名：mv mongodb-linux-x86_64-rhel70-4.0.10  mongodb-4.0.101。</p><p>然后进行如下配置即可。</p><p>首先配置环境变量：（似乎，不配也可以？）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure><p>在 export PATH USER LOGNAME MAIL HOSTNAME HISTSIZE HISTCONTROL 一行的上面添加如下内容:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH&#x3D;&#x2F;usr&#x2F;mongodb&#x2F;mongodb-4.0.10&#x2F;bin:$PATH</span><br></pre></td></tr></table></figure><p>然后在mongoDB的根目录下创建配置文件：touch mongodb.conf。</p><p>填写如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">port=27017 #端口</span><br><span class="line">dbpath= /usr/mongodb/mongodb-4.0.10/db #数据库存文件存放目录</span><br><span class="line">logpath= /usr/mongodb/mongodb-4.0.10/log/mongodb.log #日志文件存放路径</span><br><span class="line">logappend=true #使用追加的方式写日志</span><br><span class="line">fork=true #以守护进程的方式运行，创建服务器进程</span><br><span class="line">maxConns=100 #最大同时连接数</span><br><span class="line">noauth=true #不启用验证</span><br><span class="line">journal=true #每次写入会记录一条操作日志（通过journal可以重新构造出写入的数据）。</span><br><span class="line">             #即使宕机，启动时wiredtiger会先将数据恢复到最近一次的checkpoint点，然后重放后续的journal日志来恢复。</span><br><span class="line">storageEngine=wiredTiger  #存储引擎，有mmapv1、wiretiger、mongorocks</span><br><span class="line">bind_ip=0.0.0.0  #设置成全部ip可以访问，这样就可以在windows中去连虚拟机的MongoDB，也可以设置成某个网段或者某个p1234567891011</span><br></pre></td></tr></table></figure><p>然后创建数据库和日志相应的文件夹。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir db # 数据库目录</span><br><span class="line">mkdir log # 日志目录</span><br><span class="line">cd log/</span><br><span class="line">touch mongodb.log</span><br></pre></td></tr></table></figure><p>然后在bin目录下使用命令：</p><p>mongod –config ../mongodb.conf  开启服务。</p><p>之后可以在本地navicate测试连接。</p><h1 id="二、MongoDB概念解析"><a href="#二、MongoDB概念解析" class="headerlink" title="二、MongoDB概念解析"></a>二、MongoDB概念解析</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>在mongodb中基本的概念是文档、集合、数据库，如下表：</p><table><thead><tr><th align="left">SQL术语/概念</th><th align="left">MongoDB术语/概念</th><th align="left">解释/说明</th></tr></thead><tbody><tr><td align="left">database</td><td align="left">database</td><td align="left">数据库</td></tr><tr><td align="left">table</td><td align="left">collection</td><td align="left">数据库表/集合</td></tr><tr><td align="left">row</td><td align="left">document</td><td align="left">数据记录行/文档</td></tr><tr><td align="left">column</td><td align="left">field</td><td align="left">数据字段/域</td></tr><tr><td align="left">index</td><td align="left">index</td><td align="left">索引</td></tr><tr><td align="left">table joins</td><td align="left"></td><td align="left">表连接,MongoDB不支持</td></tr><tr><td align="left">primary key</td><td align="left">primary key</td><td align="left">主键,MongoDB自动将_id字段设置为主键</td></tr></tbody></table><p>具体如下图：</p><img src="/2020/10/22/MongoDB-01/2.png" class><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><p>(1）面向集合存储，易于存储对象类型的数据</p><p>(2）模式自由</p><p>(3）支持动态查询</p><p>(4）支持完全索引，包含内部对象(5）支持复制和故障恢复</p><p>(6）使用高效的二进制数据存储，包括大型对象（如视频等)</p><p>(7）自动处理碎片，以支持云计算层次的扩展性</p><p>(8）支持Python，PHP，Ruby，Java，C，C#，Javascript，Perl及C++语言的驱动程序，社区中也提供了对Erlang及.NET等平台的驱动程序</p><p>(9）文件存储格式为BSON(一种JSON的扩展)</p><h2 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h2><ul><li><p>null:用于表示空值或者不存在的字段，{““x”:null}</p></li><li><p>布尔型:布尔类型有两个值true和false，{“x”:true}</p></li><li><p>数值: shell默认使用64位<strong>浮点型</strong>数值。{“x”:3.14}或{“x”:3}。对于整型值，可以使用NumberInt(4字节符号整数）或NumberLong （8字节符号整数），”×”:NumberInt(“3’”))X”x”:NumberLong(“3”))</p></li><li><p>字符串:UTF-8字符串都可以表示为字符串类型的数据，”×”:“呵呵”</p></li><li><p>日期:日期被存储为自新纪元依赖经过的毫秒数，不存储时区，{ “x” : new Date() }</p></li><li><p>正则表达式:查询时，使用正则表达式作为限定条件，语法与JavaScript的正则表达式相同，”x”:/[abc]/} （正则表达式写在两个斜杠之间）</p></li><li><p>数组:数据列表或数据集可以表示为数组，{X”:[“a”,”b”,”c”]}</p></li><li><p>内嵌文档:文档可以嵌套其他文档，被嵌套的文档作为值来处理，{“x”;{“y”:3 }}</p></li><li><p>对象ld:对象id是一个12字节的字符串，是文档的唯一标识，{“x”: objectld() }</p></li><li><p>二进制数据:二进制数据是一个任意字节的字符串。它不能直接在shell中使用。如果要将非utf-字符保存到数据库中，二进制数据是唯一的方式。</p></li><li><p>代码:查询和文档中可以包括任何JavaScript代码，{“x”:function(){/…/}}</p></li></ul><h1 id="三、Shell命令"><a href="#三、Shell命令" class="headerlink" title="三、Shell命令"></a>三、Shell命令</h1><h2 id="基本命令"><a href="#基本命令" class="headerlink" title="基本命令"></a>基本命令</h2><p>查看数据库：show dbs</p><p>选择 /  创建 数据库 ： use 数据库名</p><p>（隐式创建：在mongoDB中选择不存在的数据库时不会报错，后期当该数据库有数据时，系统自动创建）</p><p>查看集合：show collections</p><p>创建集合：db.createCollection(‘集合名’) 。注：后期插入数据时隐式创建集合</p><p>删除集合：db.集合名.drop()</p><p>删除数据库：通过use语法选中数据库，通过db.dropDatabase()删除数据库。</p><h2 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h2><h3 id="增"><a href="#增" class="headerlink" title="增"></a>增</h3><p>语法：db.集合名.insert(JSON数据)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 在test数据库的c1集合中插入数据，并且 键 统一不加引号</span><br><span class="line">use test</span><br><span class="line">db.c1.insert(&#123;uname:&#39;zhangsan&#39;, age &#x3D; 18&#125;)</span><br></pre></td></tr></table></figure><p>通过 db.集合名.find() 查看该集合内的数据</p><img src="/2020/10/22/MongoDB-01/3.png" class><p>其中，mongoDB会给每条数据增加一个全球唯一的 _id 键。</p><h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><ol><li><p>可以自定义 _id 值，只需要给插入的JSON数据增加 _id 键即可覆盖 （但强烈不推荐）</p></li><li><p>一次性插入多条数据？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">db.c1.insert([</span><br><span class="line">    &#123;uname:&quot;lisi&quot;, age&#x3D;19&#125;,</span><br><span class="line">    &#123;uname:&quot;lisi2&quot;, age&#x3D;29&#125;,</span><br><span class="line">    &#123;uname:&quot;lisi3&quot;, age&#x3D;39&#125;</span><br><span class="line">])</span><br></pre></td></tr></table></figure></li><li><p>插入多条数据</p><p>mongoDB底层使用JS引擎实现的，所以支持部分JS语法</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">var</span> i = <span class="number">1</span>; i &lt;= <span class="number">10</span>; i++) print(i)</span><br><span class="line"></span><br><span class="line">需求：在test中c1集合插入十条数据</span><br><span class="line">use test</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">var</span> i = <span class="number">1</span>; i &lt;= <span class="number">10</span>; i++)&#123;</span><br><span class="line">    db.c1.insert(&#123;<span class="attr">uname</span>: <span class="string">"a"</span> + i, <span class="attr">age</span> : i&#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>for 循环是一条一条插入数据的，前九条数据的提示没有显示，只显示最后一条的提示信息。</p></li></ol><h3 id="删"><a href="#删" class="headerlink" title="删"></a>删</h3><p>语法：db.集合名.remove(条件 [,是否删除一条])</p><p>注意：是否删除一条，true 是，false，否 默认</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">db.c1.remove(&#123;&#125;, true) # 删除一条数据</span><br><span class="line"></span><br><span class="line">db.c1.remove(&#123;&#125;) # 删除c1下所有数据</span><br></pre></td></tr></table></figure><h3 id="改"><a href="#改" class="headerlink" title="改"></a>改</h3><p><strong>基础语法</strong>：db.集合名.update(条件，新数据 [, 是否新增，是否修改多条])</p><p>是否新增：指条件匹配不到数据则插入(true是插入，false 不插入，默认)</p><p>是否修改多条：指将匹配成功的数据都修改（true 是，false 不是 默认）</p><p><strong>升级语法：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">db.集合名.update(条件，新数据)</span><br><span class="line"></span><br><span class="line">新数据格式：&#123;修改器: &#123;键:值&#125;&#125;</span><br></pre></td></tr></table></figure><p><strong>修改器表</strong></p><table><thead><tr><th>修改器</th><th>作用</th></tr></thead><tbody><tr><td>$inc</td><td>递增</td></tr><tr><td>$rename</td><td>重命名列</td></tr><tr><td>$set</td><td>修改列值</td></tr><tr><td>$unset</td><td>删除列</td></tr></tbody></table><p>案例：</p><p>假设表中有数据，<code>uname:&quot;zhangsan&quot;, age=19</code> ，修改uname为lisi。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">db.c1.update(&#123;uname:&quot;zhangsan&quot;&#125;,&#123;uname:&quot;lisi&quot;&#125;)</span><br><span class="line"> # 会发现age列不见，则默认语法实则为替换，而不是修改</span><br><span class="line"> </span><br><span class="line">db.c1.update(&#123;uname:&quot;zhangsan&quot;&#125;,&#123;$set:&#123;uname:&quot;lisi&quot;&#125;&#125;) # 修改成功</span><br><span class="line"></span><br><span class="line"># 若修改多级(score.Math), 则必须加引号</span><br><span class="line">db.student.update(&#123;name:&quot;lisi&quot;&#125;,&#123;$set:&#123;&quot;score.Math&quot;:95&#125;&#125;)</span><br></pre></td></tr></table></figure><p><strong>练习</strong>：给{uname;”zhangsan”} 的年龄 +2 或 -2</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">db.c1.update(&#123;uname:&quot;a10&quot;&#125;,&#123;$inc:&#123;age:2&#125;&#125;)</span><br><span class="line">db.c1.update(&#123;uname:&quot;a10&quot;&#125;,&#123;$inc:&#123;age:-2&#125;&#125;)</span><br></pre></td></tr></table></figure><p><strong>综合案例</strong>：</p><p>插入数据：db.c4.insert({uname:”神龙教主”,age:888,who:”男”,other:”非国人”})</p><p>完成需求：</p><ul><li><p>uname 改为 ”飞天教主“    （修改器：$set）</p></li><li><p>age 增加 111                     （修改器：$inc）</p></li><li><p>who 改字段名 sex            （修改器：rename）</p></li><li><p>other 字段删除                  （修改器：$unset)</p></li></ul><p>那么我们如何一次性写多个修改器呢？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">db.c4.update(&#123;uname:&quot;神龙教主&quot;&#125;,&#123;$set:&#123;uname:&quot;飞天教主&quot;&#125;&#125;)</span><br><span class="line">   &#123;$inc:&#123;age:111&#125;&#125;</span><br><span class="line">   &#123;$rename:&#123;who:&quot;sex&quot;&#125;&#125;</span><br><span class="line">                               &#123;$unset:&#123;other:true&#125;&#125;</span><br><span class="line">                               </span><br><span class="line">db.c4.update(&#123;uname:&quot;神龙教主&quot;&#125;,&#123;</span><br><span class="line">      $set:&#123;uname:&quot;飞天教主&quot;&#125;,</span><br><span class="line">      $inc:&#123;age:111&#125;,</span><br><span class="line">  $rename:&#123;who:&quot;sex&quot;&#125;,</span><br><span class="line">      $unset:&#123;other:true&#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h3 id="查"><a href="#查" class="headerlink" title="查"></a>查</h3><p><strong>基础语法</strong>：db.集合名.find(条件 [,查询的列])</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">条件</span><br><span class="line">查询所有数据  &#123;&#125;</span><br><span class="line">查询age&#x3D;6的数据   &#123;age:6&#125;</span><br><span class="line">既要age&#x3D;6又要性别&#x3D;男 &#123;age:6,sex:&#39;男&#39;&#125;</span><br><span class="line"></span><br><span class="line">查询的列（可选参数）</span><br><span class="line">不写则查询所有字段</span><br><span class="line">&#123;age:1&#125; 只显示age列</span><br><span class="line">&#123;age:0&#125; 除了age列都显示</span><br><span class="line">注意：不管怎么写，_id 字段都会在</span><br></pre></td></tr></table></figure><p><strong>高级语法</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">db.集合名.find(&#123;</span><br><span class="line">     键:&#123;运算符:值&#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>运算符表：</p><table><thead><tr><th align="center">运算符</th><th>作用</th></tr></thead><tbody><tr><td align="center">$gt</td><td>大于</td></tr><tr><td align="center">$gte</td><td>大于等于</td></tr><tr><td align="center">$lt</td><td>小于</td></tr><tr><td align="center">$lte</td><td>小于等于</td></tr><tr><td align="center">$ne</td><td>不等于</td></tr><tr><td align="center">$in</td><td>in</td></tr><tr><td align="center">$nin</td><td>not in</td></tr></tbody></table><p><strong>例子：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 查询c1集合下所有的uname</span><br><span class="line">db.c1.find(&#123;&#125;,&#123;uname:1&#125;)</span><br><span class="line"></span><br><span class="line"># 查询年龄大于5岁的数据</span><br><span class="line">db.c1.find(&#123;age: &#123;$gt:5&#125;&#125;)</span><br><span class="line"></span><br><span class="line"># 查询年龄是5岁、8岁、10岁的数据</span><br><span class="line">db.c1.find(&#123;age:&#123;$in:[5,8,10]&#125;&#125;) # 集合里</span><br></pre></td></tr></table></figure><h1 id="四、JAVA-API"><a href="#四、JAVA-API" class="headerlink" title="四、JAVA　API"></a>四、JAVA　API</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天安装了mongoDB，记录一下。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>HBase_04</title>
    <link href="http://icewind-r.github.io/2020/10/17/HBase-04/"/>
    <id>http://icewind-r.github.io/2020/10/17/HBase-04/</id>
    <published>2020-10-17T14:44:21.000Z</published>
    <updated>2020-10-17T15:34:11.037Z</updated>
    
    <content type="html"><![CDATA[<p>HBase 与 MapReduce 的交互。</p><a id="more"></a><hr><p>1、查看HBase的MapReduce任务的执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./hbase mapredcp</span><br></pre></td></tr></table></figure><p>2、环境变量的导入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HBASE_HOME=hbase安装目录</span><br><span class="line">export HADOOP_HOME=Hadoop安装目录</span><br></pre></td></tr></table></figure><p>并在hadoop-env.sh中配置：（注意：在for循环之后配）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/export/servers/hbase-1.3.1/lib/*</span><br></pre></td></tr></table></figure><img src="/2020/10/17/HBase-04/1.png" class>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;HBase 与 MapReduce 的交互。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://iceWind-R.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="HBase" scheme="http://iceWind-R.github.io/tags/HBase/"/>
    
  </entry>
  
  <entry>
    <title>HBase_03</title>
    <link href="http://icewind-r.github.io/2020/10/17/HBase-03/"/>
    <id>http://icewind-r.github.io/2020/10/17/HBase-03/</id>
    <published>2020-10-17T02:12:02.000Z</published>
    <updated>2020-10-17T12:57:51.176Z</updated>
    
    <content type="html"><![CDATA[<p>总结在本机Windows上通过JAVA 的 API 操作 HBase。</p><a id="more"></a><hr><p>pom.xml 的配置信息</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-protocol<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>本篇代码进行了一定程度的封装，在本类的main方法中实现具体函数功能。</p><p><strong>DDL的操作</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.thorine.test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Admin;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Connection;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.ConnectionFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">* DDL:</span></span><br><span class="line"><span class="comment">*   1、判断表是否存在</span></span><br><span class="line"><span class="comment">*   2、创建表</span></span><br><span class="line"><span class="comment">*   3、创建namespace</span></span><br><span class="line"><span class="comment">*   4、删除表</span></span><br><span class="line"><span class="comment">* DML：</span></span><br><span class="line"><span class="comment">*   5、插入数据</span></span><br><span class="line"><span class="comment">*   6、查询数据（get）</span></span><br><span class="line"><span class="comment">*   7、查询（scan）</span></span><br><span class="line"><span class="comment">*   8、删除数据</span></span><br><span class="line"><span class="comment">* */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestDDL</span> </span>&#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * Connection代表客户端和集群的一个连接,这个连接包含对master的连接，和zk的连接</span></span><br><span class="line"><span class="comment">    * Connection的创建是重量级的，因此建议一个应用只创建一个Connection对象.</span></span><br><span class="line"><span class="comment">    * Connection是线程安全的，可以在多个线程中共享同一个Connection实例.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 从Connection中获取Table和Admin对象的实例！Table和Admin对象的创建是轻量级，且不是线程安全的！</span></span><br><span class="line"><span class="comment">    * 因此不建议池化或缓存Table和Admin对象的实例，每个线程有自己的Table和Admin对象的实例！</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Connection connection = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Admin admin = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">            configuration.set(<span class="string">"hbase.zookeeper.quorum"</span>, <span class="string">"bigdata1:2181,bigdata2:2181,bigdata3:2181"</span>);</span><br><span class="line">            <span class="comment">// 创建连接对象</span></span><br><span class="line">            connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            <span class="comment">// 创建管理员对象</span></span><br><span class="line">            admin = connection.getAdmin();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (admin != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                admin.close();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (connection != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                connection.close();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1、判断表是否存在</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isTableExist</span><span class="params">(String tableName)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> admin.tableExists(TableName.valueOf(tableName));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//2、创建表</span></span><br><span class="line">    <span class="comment">// 第一个参数表明，第二个为可变参数，列族信息</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">createTable</span><span class="params">(String tableName, String... cfs)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">// 判断是否存在列族信息</span></span><br><span class="line">        <span class="keyword">if</span> (cfs.length &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">"请设置列族信息"</span>);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 判断表是否存在</span></span><br><span class="line">        <span class="keyword">if</span> (isTableExist(tableName)) &#123;</span><br><span class="line">            System.out.println(tableName + <span class="string">" 表 已存在。"</span>);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3、创建表</span></span><br><span class="line">        <span class="comment">// 创建表描述器</span></span><br><span class="line">        HTableDescriptor hTableDescriptor = <span class="keyword">new</span> HTableDescriptor(TableName.valueOf(tableName));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 循环添加列族信息</span></span><br><span class="line">        <span class="keyword">for</span> (String cf : cfs) &#123;</span><br><span class="line">            <span class="comment">// 创建列族描述器</span></span><br><span class="line">            HColumnDescriptor hColumnDescriptor = <span class="keyword">new</span> HColumnDescriptor(cf);</span><br><span class="line">         <span class="comment">// hColumnDescriptor.setMaxVersions(5); // 版本数</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">// 添加具体列族信息</span></span><br><span class="line">            hTableDescriptor.addFamily(hColumnDescriptor);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 创建表</span></span><br><span class="line">        admin.createTable(hTableDescriptor);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4、删除表</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">dropTable</span><span class="params">(String tableName)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!isTableExist(tableName)) &#123;</span><br><span class="line">            System.out.println(tableName + <span class="string">" 表不存在，无法删除！"</span>);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 使表下线</span></span><br><span class="line">        admin.disableTable(TableName.valueOf(tableName));</span><br><span class="line">        <span class="comment">// 删除表</span></span><br><span class="line">        admin.deleteTable(TableName.valueOf(tableName));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建命名空间</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">createNamespace</span><span class="params">(String nsName)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 创建命名空间描述器</span></span><br><span class="line">        NamespaceDescriptor namespaceDescriptor = NamespaceDescriptor.create(nsName).build();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建命令空间</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            admin.createNamespace(namespaceDescriptor);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (NamespaceExistException e) &#123;</span><br><span class="line">            System.out.println(nsName + <span class="string">" 命令空间已存在！"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"命令空间已存在，但我仍可以走到这！！"</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        System.out.println(isTableExist(<span class="string">"stu1"</span>));</span><br><span class="line">        <span class="comment">// 这个表名表示在命名空间0408下创建表stu1</span></span><br><span class="line">        createTable(<span class="string">"0408:stu1"</span>,<span class="string">"info1"</span>,<span class="string">"info2"</span>);</span><br><span class="line"></span><br><span class="line">        dropTable(<span class="string">"stu1"</span>);</span><br><span class="line"></span><br><span class="line">        createNamespace(<span class="string">"0408"</span>);</span><br><span class="line"></span><br><span class="line">        System.out.println(isTableExist(<span class="string">"stu1"</span>));</span><br><span class="line"></span><br><span class="line">        close(); <span class="comment">// 关闭资源</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>DML的操作</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.thorine.test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * DDL:</span></span><br><span class="line"><span class="comment"> *   1、判断表是否存在</span></span><br><span class="line"><span class="comment"> *   2、创建表</span></span><br><span class="line"><span class="comment"> *   3、创建namespace</span></span><br><span class="line"><span class="comment"> *   4、删除表</span></span><br><span class="line"><span class="comment"> * DML：</span></span><br><span class="line"><span class="comment"> *   5、插入数据</span></span><br><span class="line"><span class="comment"> *   6、查询数据（get）</span></span><br><span class="line"><span class="comment"> *   7、查询（scan）</span></span><br><span class="line"><span class="comment"> *   8、删除数据</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestDML</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Connection connection = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Admin admin = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">            configuration.set(<span class="string">"hbase.zookeeper.quorum"</span>, <span class="string">"bigdata1:2181,bigdata2:2181,bigdata3:2181"</span>);</span><br><span class="line">            <span class="comment">// 创建连接对象</span></span><br><span class="line">            connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            <span class="comment">// 创建管理员对象</span></span><br><span class="line">            admin = connection.getAdmin();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (admin != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                admin.close();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (connection != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                connection.close();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 向表中插入数据</span></span><br><span class="line">    <span class="comment">// 参数：表名，rowKey，列族，列名，value</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">putData</span><span class="params">(String tableName, String rowKey, String cf, String cn, String value)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">// 获取表对象</span></span><br><span class="line">       Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 创建put对象</span></span><br><span class="line">       Put put = <span class="keyword">new</span> Put(Bytes.toBytes(rowKey));<span class="comment">// Bytes 为 hbase.utils 下的工具包，将目标类型转为字节数组</span></span><br><span class="line"></span><br><span class="line">       <span class="comment">// 给put对象赋值</span></span><br><span class="line">       put.addColumn(Bytes.toBytes(cf),Bytes.toBytes(cn),Bytes.toBytes(value));</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 若想添加多个列，多调用下面方法即可</span></span><br><span class="line">       <span class="comment">// put.addColumn(Bytes.toBytes(cf),Bytes.toBytes(cn),Bytes.toBytes(value));</span></span><br><span class="line">       <span class="comment">// put.addColumn(Bytes.toBytes(cf),Bytes.toBytes(cn),Bytes.toBytes(value));</span></span><br><span class="line"></span><br><span class="line">       <span class="comment">// 插入数据</span></span><br><span class="line">       <span class="comment">// 若想添加多个RowKey，只需创建多个Put对象，放到一个集合里，再用下面的put方法，插入一个集合对象。</span></span><br><span class="line">       table.put(put);</span><br><span class="line">       <span class="comment">// 关闭表连接</span></span><br><span class="line">       table.close();</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// 获取数据 get</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getData</span><span class="params">(String tableName, String rowKey, String cf, String cn)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">// 获取表对象</span></span><br><span class="line">        Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建get对象</span></span><br><span class="line">        Get get = <span class="keyword">new</span> Get(Bytes.toBytes(rowKey));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定获取的列族</span></span><br><span class="line">        <span class="comment">// get.addFamily(Bytes.toBytes(cf));</span></span><br><span class="line">        <span class="comment">// 指定获取的列族 和 列</span></span><br><span class="line">        get.addColumn(Bytes.toBytes(cf),Bytes.toBytes(cn));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置获取数据的版本数</span></span><br><span class="line">        get.setMaxVersions(<span class="number">5</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取数据</span></span><br><span class="line">        Result result = table.get(get);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 解析result</span></span><br><span class="line">        <span class="keyword">for</span> (Cell cell : result.rawCells()) &#123;</span><br><span class="line">            System.out.print(<span class="string">"ROW:"</span> + Bytes.toString(CellUtil.cloneRow(cell)));</span><br><span class="line">            System.out.print(<span class="string">", CN:"</span> + Bytes.toString(CellUtil.cloneQualifier(cell)));</span><br><span class="line">            System.out.print(<span class="string">", VALUE:"</span> + Bytes.toString(CellUtil.cloneValue(cell)));</span><br><span class="line">            System.out.println(<span class="string">", CF:"</span> + Bytes.toString(CellUtil.cloneFamily(cell)));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 关闭表连接</span></span><br><span class="line">        table.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取数据 scan</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">scanTable</span><span class="params">(String tableName)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 构建Scan对象</span></span><br><span class="line">        <span class="comment">// Scan scan = new Scan();</span></span><br><span class="line">        <span class="comment">// 指定RowKey范围从 10010 到 1003 ，左闭右开</span></span><br><span class="line">        Scan scan = <span class="keyword">new</span> Scan(Bytes.toBytes(<span class="string">"10010"</span>),Bytes.toBytes(<span class="string">"1003"</span>));</span><br><span class="line">        <span class="comment">// 扫描表</span></span><br><span class="line">        ResultScanner resultScanner = table.getScanner(scan);</span><br><span class="line">        <span class="comment">//解析</span></span><br><span class="line">        <span class="keyword">for</span> (Result result : resultScanner) &#123;</span><br><span class="line">            <span class="comment">// 打印</span></span><br><span class="line">            <span class="keyword">for</span> (Cell cell : result.rawCells()) &#123;</span><br><span class="line">                System.out.print(<span class="string">"ROW:"</span> + Bytes.toString(CellUtil.cloneRow(cell)));</span><br><span class="line">                System.out.print(<span class="string">", CN:"</span> + Bytes.toString(CellUtil.cloneQualifier(cell)));</span><br><span class="line">                System.out.print(<span class="string">", VALUE:"</span> + Bytes.toString(CellUtil.cloneValue(cell)));</span><br><span class="line">                System.out.println(<span class="string">", CF:"</span> + Bytes.toString(CellUtil.cloneFamily(cell)));</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        table.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">deleteData</span><span class="params">(String tableName,String rowKey, String cf, String cn)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">        <span class="comment">// 构造删除对象</span></span><br><span class="line">        Delete delete = <span class="keyword">new</span> Delete(Bytes.toBytes(rowKey));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置删除的列</span></span><br><span class="line">        <span class="comment">// delete.addColumn();  // 删除最新的版本 ，十分诡异，慎用</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 给指定的列删除所有的版本，若加上第三个参数时间戳，则表示删除指定时间戳以前的数据</span></span><br><span class="line">        delete.addColumns(Bytes.toBytes(cf),Bytes.toBytes(cn));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 删除指定的列族</span></span><br><span class="line">        delete.addFamily(Bytes.toBytes(cf));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 执行删除操作</span></span><br><span class="line">        table.delete(delete);</span><br><span class="line">        table.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">// putData("stu","1004","info1","name","dongao");</span></span><br><span class="line"><span class="comment">//        getData("student","1001","info","name");</span></span><br><span class="line"><span class="comment">//        scanTable("stu");</span></span><br><span class="line"></span><br><span class="line">        deleteData(<span class="string">"stu"</span>,<span class="string">"1002"</span>,<span class="string">""</span>,<span class="string">""</span>);</span><br><span class="line">        close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;总结在本机Windows上通过JAVA 的 API 操作 HBase。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://iceWind-R.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="HBase" scheme="http://iceWind-R.github.io/tags/HBase/"/>
    
  </entry>
  
  <entry>
    <title>HBase_02(HBase的安装使用)</title>
    <link href="http://icewind-r.github.io/2020/10/16/HBase-02/"/>
    <id>http://icewind-r.github.io/2020/10/16/HBase-02/</id>
    <published>2020-10-16T09:21:05.000Z</published>
    <updated>2020-10-23T00:35:09.209Z</updated>
    
    <content type="html"><![CDATA[<p>HBase的安装和部署，和基本命令使用。</p><a id="more"></a><hr><h1 id="一、HBase的安装部署"><a href="#一、HBase的安装部署" class="headerlink" title="一、HBase的安装部署"></a>一、HBase的安装部署</h1><h2 id="Zookeeper的正常部署"><a href="#Zookeeper的正常部署" class="headerlink" title="Zookeeper的正常部署"></a>Zookeeper的正常部署</h2><p>首先保证zookeeper集群的正常部署，并启动：</p><p>在三台虚拟机的zookeeper安装目录的bin目录下，分别使用命令启动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./zkServer.sh start</span><br></pre></td></tr></table></figure><h2 id="Hadoop正常部署"><a href="#Hadoop正常部署" class="headerlink" title="Hadoop正常部署"></a>Hadoop正常部署</h2><p>Hadoop集群的正常部署并启动。</p><h2 id="HBase的下载安装"><a href="#HBase的下载安装" class="headerlink" title="HBase的下载安装"></a>HBase的下载安装</h2><p>HBase使用的是1.3.1 版本，下载地址 <a href="http://archive.apache.org/dist/hbase/1.3.1/" target="_blank" rel="noopener">http://archive.apache.org/dist/hbase/1.3.1/</a> ，下载hbase-1.3.1-bin.tar.gz 压缩包，上传到bigdata1虚拟机，并解压到相应目录 ( /export/servers/ ) 。</p><h2 id="Hbase配置文件的修改"><a href="#Hbase配置文件的修改" class="headerlink" title="Hbase配置文件的修改"></a>Hbase配置文件的修改</h2><p>HBase的配置文件都在 conf 目录下，这里我们主要对以下几个文件进行修改。</p><h3 id="regionservers"><a href="#regionservers" class="headerlink" title="regionservers"></a>regionservers</h3><p>即RegionServer，region所在的集群，这里写上我们的三台虚拟机主机名，如下。</p><h3 id="hbase-env-sh"><a href="#hbase-env-sh" class="headerlink" title="hbase-env.sh"></a>hbase-env.sh</h3><p>改三个位置，第一个是 27行左右的 JAVA_HOME，去掉注释，改为本地配好的 JDK 地址。</p><p>然后是45行左右，去掉两个export，因为在jdk1.8时已经不再需要。</p><p>第三个地方：128行左右，取消使用HBase内置的zookeeper，很不方便，而且会与我们自己的zookeeper冲突，由true改为法false。</p><h3 id="hbase-site-xml"><a href="#hbase-site-xml" class="headerlink" title="hbase-site.xml"></a>hbase-site.xml</h3><p>增加configuration 内的引用，代码如下。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://bigdata1:8020/HBase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!-- 默认端口号：16000，默认的web访问：16010 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>16000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!--  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>bigdata1,bigdata2,bigdata3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/export/servers/zookeeper-3.4.9/zkdatas<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="软连接Hadoop配置文件到HBase"><a href="#软连接Hadoop配置文件到HBase" class="headerlink" title="软连接Hadoop配置文件到HBase"></a>软连接Hadoop配置文件到HBase</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ln -s /export/servers/hadoop-2.7.7/etc/hadoop/core-site.xml /export/servers/hbase-1.3.1/conf/core-site.xml</span><br><span class="line"></span><br><span class="line">ln -s /export/servers/hadoop-2.7.7/etc/hadoop/hdfs-site.xml /export/servers/hbase-1.3.1/conf/hdfs-site.xml</span><br></pre></td></tr></table></figure><h2 id="HBase远程发送到集群其他主机"><a href="#HBase远程发送到集群其他主机" class="headerlink" title="HBase远程发送到集群其他主机"></a>HBase远程发送到集群其他主机</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r hbase-1.3.1 bigdata2:$PWD</span><br><span class="line">scp -r hbase-1.3.1 bigdata3:$PWD</span><br></pre></td></tr></table></figure><h2 id="HBase服务的启动"><a href="#HBase服务的启动" class="headerlink" title="HBase服务的启动"></a>HBase服务的启动</h2><p>首先对bin目录下的启动方式做个简单介绍：</p><img src="/2020/10/16/HBase-02/1.png" class><h3 id="启动方式"><a href="#启动方式" class="headerlink" title="启动方式"></a>启动方式</h3><h4 id="HBase单节点启动"><a href="#HBase单节点启动" class="headerlink" title="HBase单节点启动"></a>HBase单节点启动</h4><p>在主机bigdata1的 HBase  bin目录下，使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;hbase-daemon.sh start master</span><br></pre></td></tr></table></figure><p>启动后，即可访问16010端口查看Web版的HBase。</p><p>启动regionserver。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./hbase-daemon.sh start regionserver</span><br></pre></td></tr></table></figure><p>可以看到，region为 启动的bigdata1。</p><img src="/2020/10/16/HBase-02/2.png" class><h4 id="HBase群起群关"><a href="#HBase群起群关" class="headerlink" title="HBase群起群关"></a>HBase群起群关</h4><p>我们可以通过 <code>./stop-hbase.sh</code> 关闭HBase，但此命令只能在master上使用。</p><p>集群启动./start-hbase.sh，此命令在那个主机上启动，则该主机就默认为master。</p><blockquote><p>提示：如果集群之间的节点时间不同步，会导致regionserver无法启动，抛出ClockOutOfSyncException 异常。</p></blockquote><h1 id="二、HBase-Shell操作"><a href="#二、HBase-Shell操作" class="headerlink" title="二、HBase Shell操作"></a>二、HBase Shell操作</h1><h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><h3 id="进入HBase客户端命令行"><a href="#进入HBase客户端命令行" class="headerlink" title="进入HBase客户端命令行"></a>进入HBase客户端命令行</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./hbase shell</span><br></pre></td></tr></table></figure><h3 id="查看帮助命令"><a href="#查看帮助命令" class="headerlink" title="查看帮助命令"></a>查看帮助命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):001:0&gt; help</span><br></pre></td></tr></table></figure><img src="/2020/10/16/HBase-02/3.png" class><p>ddl是对表的操作，dml是对数据的操作。</p><h3 id="查看当前数据库有哪些表"><a href="#查看当前数据库有哪些表" class="headerlink" title="查看当前数据库有哪些表"></a>查看当前数据库有哪些表</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):002:0&gt; list</span><br></pre></td></tr></table></figure><p>显示为 0，list只能查用户建的表，系统表则查询不到。</p><h3 id="其他命令"><a href="#其他命令" class="headerlink" title="其他命令"></a>其他命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">status whoami version</span><br></pre></td></tr></table></figure><p>查询服务器状态、查询当前用户、当前<em>hbase</em>使用的版本号。</p><h3 id="使用HBase客户端的注意事项"><a href="#使用HBase客户端的注意事项" class="headerlink" title="使用HBase客户端的注意事项"></a>使用HBase客户端的注意事项</h3><ol><li>backspace为删除后面的字符，若想删除前面，则需要按住Ctrl。</li><li>不熟悉某个命令，可以输入该命令，直接回车，则会给出提示和例子。</li></ol><h2 id="DDL（对表的操作）"><a href="#DDL（对表的操作）" class="headerlink" title="DDL（对表的操作）"></a>DDL（对表的操作）</h2><p>1、<em><em>create *</em>: 创建数据库表，创建命令可看帮助*help ‘create’</em></p><p>语法：<code>create &lt;table&gt;, {NAME =&gt;&lt;family&gt;, VERSIONS =&gt; &lt;VERSIONS&gt;}</code></p><p>例示：<code>create &#39;product&#39;,{NAME =&gt; &#39;computer&#39;, VERSIONS =&gt; 5},{ NAME =&gt; &#39;food&#39; , VERSIONS =&gt; 3}</code></p><p>描述：创建一张名叫<em>‘product’*数据库表，并且创建两个列族，分别为：</em>‘computer’<em>、</em>‘food’*</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create &#39;product&#39;,&#39;computer&#39;,&#39;food&#39;,...</span><br></pre></td></tr></table></figure><p>2、*<em>describe *</em>:  查看表结构描述</p><p>用法：<code>describe &#39;product&#39;</code></p><p>3、<strong>alter</strong> : 修改表</p><p>用法：修改表结构必须先<em>disable</em>，再修改表，修改完成后，再<em>enable</em>表。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#96;disable &#39;product&#39;</span><br><span class="line"></span><br><span class="line">alter &#39;product&#39;,&#123;NAME &#x3D;&gt; &#39;food&#39;,VERSIONS&#x3D;&gt; 3&#125;</span><br><span class="line"></span><br><span class="line">alter &#39;表名&#39;,&#123;NAME&#x3D;&gt;&#39;列名&#39;,METHOD&#x3D;&gt;&#39;delete&#39;&#125; # 删除指定的列</span><br><span class="line"></span><br><span class="line">enable &#39;product&#39;</span><br></pre></td></tr></table></figure><p>4、<strong>drop</strong>：删除表</p><p>用法：首先<em>disable</em>，然后<em>drop</em>。</p><p><code>disable &#39;product&#39;</code></p><p><code>drop &#39;product&#39;</code></p><h2 id="DML（对数据的操作）"><a href="#DML（对数据的操作）" class="headerlink" title="DML（对数据的操作）"></a>DML（对数据的操作）</h2><h3 id="put"><a href="#put" class="headerlink" title="put"></a>put</h3><p>插入（修改）数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">put 表名, RowKey, 列族:列名, 数据</span><br><span class="line"></span><br><span class="line">put &#39;stu&#39;,&#39;1001&#39;,&#39;info1:name&#39;,&#39;zhangsan&#39; # 若stu表的info1:name 字段没有值，则代表插入</span><br><span class="line"></span><br><span class="line">put &#39;stu&#39;,&#39;1001&#39;,&#39;info1:name&#39;,&#39;zhangsan1&#39; # 若stu表的info1:name 字段有值，则代表修改</span><br></pre></td></tr></table></figure><h3 id="scan"><a href="#scan" class="headerlink" title="scan"></a>scan</h3><p>查询数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scan 表名</span><br><span class="line">scan &#39;stu&#39;</span><br><span class="line">scan &#39;stu&#39;,&#123;STARTROW&#x3D;&gt;&#39;1001&#39;,STOPROW&#x3D;&gt;&#39;1003&#39;&#125; # 指定查询的范围，从 RowKey 1001 ~ 1003 ，左闭右开，若前或后不写，则代表负无穷 或 正无穷</span><br><span class="line"></span><br><span class="line">scan &#39;stu&#39;,&#123;RAW &#x3D;&gt; true,VERSIONS &#x3D;&gt; 10&#125; # 查询stu表的 10个版本内的所有数据，包括修改之前的数据</span><br></pre></td></tr></table></figure><h3 id="get"><a href="#get" class="headerlink" title="get"></a>get</h3><p>查询数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">get 表名, RowKey, 列族</span><br><span class="line"></span><br><span class="line">get &#39;stu&#39;, &#39;1001&#39;, &#39;info1&#39;</span><br><span class="line">get &#39;stu&#39;, &#39;1001&#39;, &#39;info1:name&#39;</span><br></pre></td></tr></table></figure><h3 id="delete"><a href="#delete" class="headerlink" title="delete"></a>delete</h3><p>删除一个RowKwy里的一个字段。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">delete 表名, RowKey, 列族:列名</span><br><span class="line"></span><br><span class="line">delete &#39;stu&#39;,&#39;1001&#39;,&#39;info1:name&#39; # 删除列，1001 数据下的 info1下的name数据全部删除（包括修改前的数据）</span><br></pre></td></tr></table></figure><p>type 会变为 DeleteColumn</p><img src="/2020/10/16/HBase-02/4.png" class><h3 id="deleteall"><a href="#deleteall" class="headerlink" title="deleteall"></a>deleteall</h3><p>删除一个RowKey。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deleteall &#39;stu&#39;,&#39;1001&#39; # 删除stu表的 RowKey&#x3D;1001 行数据</span><br></pre></td></tr></table></figure><h3 id="truncate"><a href="#truncate" class="headerlink" title="truncate"></a>truncate</h3><p>清空一张表（删除所有数据，即所有RowKey，所有列族），不建议使用。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">truncate &#39;stu&#39;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;HBase的安装和部署，和基本命令使用。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://iceWind-R.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="HBase" scheme="http://iceWind-R.github.io/tags/HBase/"/>
    
  </entry>
  
  <entry>
    <title>HBase_01(HBase概念简介)</title>
    <link href="http://icewind-r.github.io/2020/10/15/HBase-01/"/>
    <id>http://icewind-r.github.io/2020/10/15/HBase-01/</id>
    <published>2020-10-15T08:35:54.000Z</published>
    <updated>2020-10-17T12:30:21.896Z</updated>
    
    <content type="html"><![CDATA[<p>就今天吧，稀里糊涂的，入门学习个HBase！就记些比较重要的、需要重点掌握的知识点吧。别问为什么，死记硬背就完事了！</p><a id="more"></a><hr><h1 id="一、HBase简介"><a href="#一、HBase简介" class="headerlink" title="一、HBase简介"></a>一、HBase简介</h1><h2 id="HBase是什么？"><a href="#HBase是什么？" class="headerlink" title="HBase是什么？"></a>HBase是什么？</h2><p>Hbase是一种NoSQL数据库，这意味着它不像传统的RDBMS数据库那样支持SQL作为查询语言。</p><p>HBase是一种构建在HDFS之上的<strong>分布式、面向列</strong>的存储系统。在需要实时读写、随机访问超大规模数据集时，可以使用HBase。</p><h2 id="HBase的特点"><a href="#HBase的特点" class="headerlink" title="HBase的特点"></a>HBase的特点</h2><ol><li><strong>大</strong>：一个表可以有上亿行，上百万列。</li><li><strong>面向列</strong>：面向列表（簇）的存储和权限控制，列（簇）独立检索。</li><li><strong>稀疏</strong>：对于为空（NULL）的列，并不占用存储空间，因此，表可以设计的非常稀疏。</li><li><strong>无模式</strong>：每一行都有一个可以排序的主键和任意多的列，列可以根据需要动态增加，同一张表中不同的行可以有截然不同的列。</li><li><strong>数据多版本</strong>：每个单元中的数据可以有多个版本，默认情况下，版本号自动分配，版本号就是单元格插入时的时间戳。</li><li><strong>数据类型单一</strong>：HBase中的数据都是字符串，没有类型。</li></ol><h1 id="二、数据模型"><a href="#二、数据模型" class="headerlink" title="二、数据模型"></a>二、数据模型</h1><h2 id="1、RowKey"><a href="#1、RowKey" class="headerlink" title="1、RowKey"></a>1、RowKey</h2><p><strong>RowKey</strong> 是一行记录的 <strong>主键</strong> ，用于检索记录数据的。HBase的数据是按照RowKey的 <strong>字典顺序</strong> 进行全局排序的，所有的查询都只能依赖于这一个排序维度。访问 HBase table 中的行，只有三种方式：</p><ol><li><p>通过单个 <strong>RowKey</strong> 访问。</p></li><li><p>通过 <strong>RowKey</strong> 的 range 全表扫描。</p></li><li><p><strong>RowKey</strong> 可以是任意字符串（最大长度是64KB，实际应用中长度一般为 10 ~ 100bytes），在HBase 内部，<strong>RowKey</strong> 保存为字节数组。</p></li></ol><blockquote><p> 通过下面一个例子来说明一下” <strong>字典排序</strong> “的原理：</p><p>RowKey列表{“abc”, “a”, “bdf”, “cdf”, “def”}按字典排序后的结果为{“a”, “abc”, “bdf”, “cdf”, “defg”}</p></blockquote><h2 id="2、Row（行）"><a href="#2、Row（行）" class="headerlink" title="2、Row（行）"></a>2、Row（行）</h2><p>HBase表中的每行数据都是一个RowKey和 多个Column（列）组成，数据是按照RowKey的字典顺序存储的。</p><h2 id="3、Column（列）"><a href="#3、Column（列）" class="headerlink" title="3、Column（列）"></a>3、Column（列）</h2><p>HBase每个列都由列族和列限定符进行限定，例如info：name , info :  age。建表时，只需要指定列族，而列限定符无需预先定义。</p><h2 id="4、Column-Family（列族）"><a href="#4、Column-Family（列族）" class="headerlink" title="4、Column Family（列族）"></a>4、Column Family（列族）</h2><p>如果将Region看成是一个表的 <strong>横向切割</strong> ，那么，一个Region中的数据列的 <strong>纵向切割</strong> ，称之为一个 <strong>Column Family</strong> 。每一个列，都必须归属于一个Column Family，这个归属关系是在写数据时指定的，而不是建表时预先定义。</p><h2 id="5、Cell"><a href="#5、Cell" class="headerlink" title="5、Cell"></a>5、Cell</h2><p>Cell 是由 {RowKey，ColumnFamily : Column Qualifier，TimeStamp} 唯一确定的单元。Cell 中的数据是没有类型的，全部是<strong>字节码</strong>形式存储。</p><h2 id="6、Region"><a href="#6、Region" class="headerlink" title="6、Region"></a>6、Region</h2><p>HBase中采用了”Range分区”，将Key的完整区间切割成一个个的”Key Range” ，每一个”Key Range”称之为一个Region。</p><p>也可以这么理解：将HBase中拥有数亿行的一个大表， <strong>横向切割</strong> 成一个个” <strong>子表</strong> “，这一个个” <strong>子表</strong> “就是 <strong>Region</strong> 。Region是HBase中分布式存储和负载均衡的最小单元，即不同的region可以分别在不同的Region Server上，但同一个Region是不会拆分到多个server上。 </p><p>当一个Region增长到一定大小以后，会自动分裂成两个。</p><p>每个region由以下信息标识：</p><ol><li>&lt; 表名,startRowkey,创建时间&gt;</li><li>由目录表(-ROOT-和.META.)记录该region的endRowkey</li></ol><h2 id="7、TimeStamp（时间戳）"><a href="#7、TimeStamp（时间戳）" class="headerlink" title="7、TimeStamp（时间戳）"></a>7、TimeStamp（时间戳）</h2><p>HBase 中通过 Row 和 Columns 确定的一个存储单元称为 Cell。每个 Cell 都保存着同一份数据的多个版本。 版本通过时间戳来索引，时间戳的类型是 64 位整型。时间戳可以由HBase（在数据写入时自动）赋值，<br>此时时间戳是精确到毫秒的当前系统时间。时间戳也 可以由客户显示赋值。如果应用程序要避免数据版本冲突，就必须自己生成具有唯一性的时间戳。每个 Cell 中，不同版本的数据按照时间倒序排序，即最新的数据排在最前面。</p><p>为了避免数据存在过多版本造成的管理（包括存储和索引）负担，HBase 提供了两种数据版本回收方式。 一是保存数据的最后 n 个版本，二是保存最近一段时间内的版本（比如最近七天）。用户可以针对每个列族进行设置。</p><h2 id="8、稀疏矩阵"><a href="#8、稀疏矩阵" class="headerlink" title="8、稀疏矩阵"></a>8、稀疏矩阵</h2><p>HBase中一个表的数据是按照稀疏矩阵的方式组织的，行与行之间也无须遵循一致的定义，而这种定义恰好符合半结构化数据或非结构化数据的特点。</p><p>HBase定义表是只需要声明 *<em>列族 *</em>即可，不需要声明具体的列，这意味着，往HBase写入数据时，字段可以 动态、按需 指定。</p><h2 id="9、Store"><a href="#9、Store" class="headerlink" title="9、Store"></a>9、Store</h2><p>每一个region由一个或多个store组成，至少是一个store，hbase会把一起访问的数据放在一个store里面，即为每个 ColumnFamily建一个store，如果有几个ColumnFamily，也就有几个Store。一个Store由一个memStore和0或者 多个StoreFile组成。 HBase以store的大小来判断是否需要切分region</p><img src="/2020/10/15/HBase-01/1.png" class><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>Hregion是Hbase中分布式存储和负载均衡的最小单元。最小单元就表示不同的Hregion可以分布在不同的HRegion server上。但一个Hregion是不会拆分到多个server上的。</p><img src="/2020/10/15/HBase-01/3.png" class><p>HRegion虽然是分布式存储的最小单元，但并不是存储的最小单元。HRegion由一个或者多个Store组成，每个store保存一个columns family。每个Strore又由一个memStore和0至多个StoreFile组成。如图：</p><img src="/2020/10/15/HBase-01/4.png" class><p>StoreFile以HFile格式保存在HDFS上。</p><h1 id="三、HBase系统架构"><a href="#三、HBase系统架构" class="headerlink" title="三、HBase系统架构"></a>三、HBase系统架构</h1><img src="/2020/10/15/HBase-01/5.png" class><h2 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h2><p>包含访问hbase的接口，client维护着一些cache来加快对hbase的访问，比如regione的位置信息。</p><h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h2><p>1 保证任何时候，集群中只有一个master</p><p>2 存贮所有Region的寻址入口。</p><p>3 实时监控Region Server的状态，将Region server的上线和下线信息实时通知给Master</p><p>4 存储Hbase的schema,包括有哪些table，每个table有哪些column family</p><h2 id="RegionServer"><a href="#RegionServer" class="headerlink" title="RegionServer"></a>RegionServer</h2><p>1 Region server维护Master分配给它的region，处理对这些region的IO请求</p><p>2 Region server负责切分在运行过程中变得过大的region</p><p>Data： get，put，delete （HBase通过时间戳控制每条数据，get最新数据，put写入数据，若没有此条数据则代表插入，有则代表修改，delete删除数据）</p><p>Region：splitRegion（切片），compactRegion（合并）</p><h2 id="Master（高可用）"><a href="#Master（高可用）" class="headerlink" title="Master（高可用）"></a>Master（高可用）</h2><p>1 为Region server分配region</p><p>2 负责region server的负载均衡</p><p>3 发现失效的region server并重新分配其上的region</p><p>4 GFS上的垃圾文件回收</p><p>5 处理schema更新请求</p><p>Table：create，alter，delete</p><p>RegionServer：分配region给每个regionServer，监控每个regionServer的状态</p><img src="/2020/10/15/HBase-01/2.png" class><blockquote><p>RegionServer管理着Region，对数据的操作，DML。</p><p>Master管理表的增删改查，以及Region的维护信息，DDL。</p></blockquote><p><strong>总结</strong></p><p>可以看到，client访问hbase上数据的过程并不需要master参与（寻址访问zookeeper和region server，数据读写访问regione server），master仅仅维护者table和region的元数据信息，负载很低。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;就今天吧，稀里糊涂的，入门学习个HBase！就记些比较重要的、需要重点掌握的知识点吧。别问为什么，死记硬背就完事了！&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://iceWind-R.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="HBase" scheme="http://iceWind-R.github.io/tags/HBase/"/>
    
  </entry>
  
  <entry>
    <title>大三上_第3周总结</title>
    <link href="http://icewind-r.github.io/2020/10/07/%E5%A4%A7%E4%B8%89%E4%B8%8A-%E7%AC%AC3%E5%91%A8%E6%80%BB%E7%BB%93/"/>
    <id>http://icewind-r.github.io/2020/10/07/%E5%A4%A7%E4%B8%89%E4%B8%8A-%E7%AC%AC3%E5%91%A8%E6%80%BB%E7%BB%93/</id>
    <published>2020-10-07T03:29:27.000Z</published>
    <updated>2020-10-07T04:54:16.739Z</updated>
    
    <content type="html"><![CDATA[<p>10.7 ~ 10.10 日</p><hr><p>今天上午逃了刘丹的四节课，想着好好做建民的重大需求，但是竟然，一点进度都没有。明明有着超快的网络，但还是，唉。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;10.7 ~ 10.10 日&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;今天上午逃了刘丹的四节课，想着好好做建民的重大需求，但是竟然，一点进度都没有。明明有着超快的网络，但还是，唉。&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Learning" scheme="http://iceWind-R.github.io/categories/Learning/"/>
    
    
      <category term="plan" scheme="http://iceWind-R.github.io/tags/plan/"/>
    
  </entry>
  
  <entry>
    <title>Excel表格操作</title>
    <link href="http://icewind-r.github.io/2020/09/22/Excel%E8%A1%A8%E6%A0%BC%E6%93%8D%E4%BD%9C/"/>
    <id>http://icewind-r.github.io/2020/09/22/Excel%E8%A1%A8%E6%A0%BC%E6%93%8D%E4%BD%9C/</id>
    <published>2020-09-22T11:28:00.000Z</published>
    <updated>2020-09-23T10:02:52.451Z</updated>
    
    <content type="html"><![CDATA[<p>Excel 的重要性不言而喻，学会使用可以使自己的效率大大提高，今天开始学习，做此纪录。</p><a id="more"></a><hr><h1 id="Excel入门"><a href="#Excel入门" class="headerlink" title="Excel入门"></a>Excel入门</h1><p>1、<strong>开始</strong> 菜单的 <strong>合并后居中</strong>，可以实现跨单元格的合并居中。</p><p>2、自动匹配单元格长度，双击<strong>表头</strong>，<strong>列头</strong>，可以使该行（列）自动匹配本行（列）最长单元格。</p><p>3、可以选中若干列，点击<strong>开始</strong>，<strong>单元格样式</strong>，对选中的标题进行样式改变。</p><img src="/2020/09/22/Excel%E8%A1%A8%E6%A0%BC%E6%93%8D%E4%BD%9C/1.png" class><p>4、可以选中所有列，使其所有单元格 宽度 相同。（对行同样适用）</p><img src="/2020/09/22/Excel%E8%A1%A8%E6%A0%BC%E6%93%8D%E4%BD%9C/1.gif" class><p>5、可以快速产生相同规律（等差数列）的一组数据，选取一个单元格或一组单元格，右下角鼠标变为 “+”，拖动即可实现。</p><img src="/2020/09/22/Excel%E8%A1%A8%E6%A0%BC%E6%93%8D%E4%BD%9C/2.gif" class><p>6、关于<strong>日期</strong>操作</p><ul><li><p>日期如果手动输入，最好采用excel可以识别的格式，即 “/” 标识，如：2020/9/22</p></li><li><p>如果想插入当前日期，可以使用快捷键 ctrl + ;(分号)</p></li><li><p>并且可以在当前单元格右键选择 <strong>设置单元格格式</strong>，进行日期格式的选择。</p><img src="/2020/09/22/Excel%E8%A1%A8%E6%A0%BC%E6%93%8D%E4%BD%9C/2.png" class><p>注意：若出现全部“#”，则代表当前单元格长度不够，未能显示，只需双击表头的间隔处，使单元格匹配内容长度即可。</p></li><li><p>同样，可以选择单元格右下角的 +，进行依次递增的日期。</p></li></ul><p>7、关于数学公式计算。例如上例中的总金额 = 单价 * 数量，这时我们可以用excel的计算能力，我们 在总金额单元格 输入等于号（=），然后单击 单价 之后 输入乘号 ，然后点击 数量，输入回车即可得到计算结果。</p><img src="/2020/09/22/Excel%E8%A1%A8%E6%A0%BC%E6%93%8D%E4%BD%9C/3.gif" class><p>8、可以改变数字的显示。例如金额，可以加上金额符号，并指定数字小数位数。</p><img src="/2020/09/22/Excel%E8%A1%A8%E6%A0%BC%E6%93%8D%E4%BD%9C/3.png" class><p>9、格式刷的使用，选中被格式的单元格，再选中待格式的单元格即可将复制前者的格式。</p><p>10、打印表格。可以点击左上角的<strong>打印预览和打印</strong>，在默认情况下只会打印数据，而表格线等则不会打印，可以在 <strong>页面布局</strong>，勾选 <strong>网格线</strong> 下的 <strong>打印</strong> 按钮。即可在打印时出现网格线。</p><img src="/2020/09/22/Excel%E8%A1%A8%E6%A0%BC%E6%93%8D%E4%BD%9C/4.png" class><p>11、关于表格的格线操作。快捷键 按住 <strong>shift</strong>，选中单元格删除格线，<strong>Esc</strong> 退出。</p><img src="/2020/09/22/Excel%E8%A1%A8%E6%A0%BC%E6%93%8D%E4%BD%9C/4.gif" class><p>12、背景图片。在页面布局中添加背景，可以取消<strong>网格线</strong>的<strong>查看</strong>按钮，来不显示图片上的表格线。也可以删除背景。</p><p>13、函数（自动求和 等）。在生成总和的单元格上，点击该按钮，选择被求和的所有单元格，然后enter，即可得到结果。</p><img src="/2020/09/22/Excel%E8%A1%A8%E6%A0%BC%E6%93%8D%E4%BD%9C/5.png" class><p>14、冻结表格栏 与 拆分视窗。在<strong>视图</strong>选项卡操作。</p><img src="/2020/09/22/Excel%E8%A1%A8%E6%A0%BC%E6%93%8D%E4%BD%9C/5.gif" class><p>15、排序。在排序中，选择待排序列的一个单元格即可，不要选择多个。在 <strong>开始</strong> 的右侧，<strong>排序和筛查</strong>。</p><p>要指定多层排序，可以在 <strong>排序和筛查</strong> 中选择 <strong>自定义排序</strong>，点击 <strong>添加条件</strong>，实现多条件排序。如下界面。</p><img src="/2020/09/22/Excel%E8%A1%A8%E6%A0%BC%E6%93%8D%E4%BD%9C/6.png" class><p>16、自定义序列，在上面的自定义序列中，选择排序次序为自定义，跳到下面选项卡。</p><img src="/2020/09/22/Excel%E8%A1%A8%E6%A0%BC%E6%93%8D%E4%BD%9C/7.png" class>，输入自定义顺序，以回车间隔。<img src="/2020/09/22/Excel%E8%A1%A8%E6%A0%BC%E6%93%8D%E4%BD%9C/6.gif" class>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Excel 的重要性不言而喻，学会使用可以使自己的效率大大提高，今天开始学习，做此纪录。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Offices" scheme="http://iceWind-R.github.io/categories/Offices/"/>
    
    
      <category term="excel" scheme="http://iceWind-R.github.io/tags/excel/"/>
    
  </entry>
  
  <entry>
    <title>我的考研必看</title>
    <link href="http://icewind-r.github.io/2020/09/13/%E6%88%91%E7%9A%84%E8%80%83%E7%A0%94%E5%BF%85%E7%9C%8B/"/>
    <id>http://icewind-r.github.io/2020/09/13/%E6%88%91%E7%9A%84%E8%80%83%E7%A0%94%E5%BF%85%E7%9C%8B/</id>
    <published>2020-09-13T11:45:17.000Z</published>
    <updated>2020-09-21T14:07:10.807Z</updated>
    
    <content type="html"><![CDATA[<p>为什么考研？自己适合考研吗？这两个问题，想要考研的你，真的思考过吗？</p><a id="more"></a><hr><p>给自己定的初步目标：<strong>中下游211本专业</strong></p><p>说到需求，当然就是你考研是想要达到什么目的。我举几个例子，假如说你打算将来考选调生，那么985院校就是目标，专业视具体要求而定；假如你想回老家考个公务员，那选择一个匹配岗位的学校和专业即可；假如你想进入某些知名企业，那么应该调研清楚它的目标院校是什么档次，然后作出选择。</p><p>先说竞争对手方面的因素，这一方面主要从目标院校历年的考生和录取情况来评估。</p><p>1.初试科目是什么</p><p>2.专业课参考书是什么</p><p>3.最近三年的分数线（这一定要看地区，不要盲目对比，二区的380和北京上海的380完全不是一回事）</p><p>4.最近三年的报录比</p><p>5.报考学生的生源大概水平</p><p>6.复试什么形式，在总分中占比多少</p><p>7.最后实际录取学生的总分及单科分数分布（和其他院校对比，要对比同地区的学校公共课的分数，专业课不具可比性）</p><p>8.最后录取学生的本科背景</p><p>9.录取学生中二战以以上的学生比例</p><p>10.录取的跨校跨专业学生的比例</p><p>以上信息都不是能百度的，需要自己去搜集，像5、8、9、10这些问题不需要多么精准，知道大概水平即可。至于怎么搜集，最直接的就是找上一届录取的前辈。<strong>（最近很多人私信我去哪里找已经录取的前辈，大家可以下载一个APP叫做经验超市，这个APP是由北大的几位考研学长开发的，可以去上面看看有没有学长学姐，上面的学长学姐都是经过身份认证的。）</strong></p><p>自身因素方面：</p><p>1.高中学习状态及高考成绩（高中是否尽力学习，高考是正常发挥还是失常或者超常发挥。这一点可以衡量潜在的学习能力，毕竟高考和考研都是类似的应试）</p><p>2.本科院校及专业（正常录取还是填志愿失误造成滑档）</p><p>3.本科成绩（结合自己学习努力程度来看）</p><p>4.个人学习习惯和自制力（十几年的读书经历，想必自己心中应该有数。在考研这种靠自己自律的环境下，能够发生重大改变的人少之又少，不要高估自己的决心和毅力）</p><p>5.考研时间（自己有多少时间准备，本科课程能不能应付，学校的琐事会不会消耗太多时间）</p><p>6.自身心态（自己心态建设做的怎么样，是不是过于焦虑和敏感，是不是能够承压）</p><p>7.公共课基础如何</p><p>8.是否跨专业，专业课是否零基础</p><p><strong>人生终究是回归均值的一个过程，不要以为自己能够轻易的突破。考研之前请认真思考，把握好自己的定位、需求、成本和风险承担能力。每个人的天赋，成长过程，家庭环境都是不同的，即便要向命运挑战，也要结合自己的实际情况。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;为什么考研？自己适合考研吗？这两个问题，想要考研的你，真的思考过吗？&lt;/p&gt;
    
    </summary>
    
    
      <category term="考研" scheme="http://iceWind-R.github.io/categories/%E8%80%83%E7%A0%94/"/>
    
    
      <category term="idea" scheme="http://iceWind-R.github.io/tags/idea/"/>
    
  </entry>
  
  <entry>
    <title>C++文本操作</title>
    <link href="http://icewind-r.github.io/2020/09/09/C-%E6%96%87%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    <id>http://icewind-r.github.io/2020/09/09/C-%E6%96%87%E6%9C%AC%E6%93%8D%E4%BD%9C/</id>
    <published>2020-09-09T01:09:40.000Z</published>
    <updated>2020-09-10T01:07:47.533Z</updated>
    
    <content type="html"><![CDATA[<p>本篇总结 C++ 的文本操作，即C++文件和流的概念。</p><a id="more"></a><hr><h1 id="文件流"><a href="#文件流" class="headerlink" title="文件流"></a>文件流</h1><p>到目前为止，我们使用了iostream标准库，它提供了cin 和 cout 方法分别用于从<strong>标准输入读取流</strong>和向<strong>标准输入写入流</strong>。</p><img src="/2020/09/09/C-%E6%96%87%E6%9C%AC%E6%93%8D%E4%BD%9C/1.gif" class><p>如何从<strong>标准输入读取流</strong>和向<strong>标准输入写入流</strong>？这就需要用到 C++ 中另一个标准库fstream，它定义了三个新的数据类型：</p><table><thead><tr><th>数据类型</th><th>描述</th></tr></thead><tbody><tr><td>ofstream</td><td>该数据类型表示输出文件流，用于创建文件并向文件写入信息</td></tr><tr><td>ifstream</td><td>该数据类型表示输入文件流，用于从文件读取信息</td></tr><tr><td>fstream</td><td>该数据类型通常表示文件流，且同时具有 ofstream 和 ifstream 两种功能，这意味着它可以创建文件，向文件写入信息，从文件读取信息。</td></tr></tbody></table><p>要在C++中进行文件处理，必须在 C++ 代码中包含头文件 <code>&lt;iostream&gt;</code> 和 <code>&lt;fstream&gt;</code></p><h2 id="打开文件"><a href="#打开文件" class="headerlink" title="打开文件"></a>打开文件</h2><p>在从文件读取信息或者向文件写入信息之前，必须先打开文件。<strong>ofstream</strong> 和 <strong>fstream</strong>对象都可以用来打开文件进行<strong>写</strong>操作；如果只需要打开文件进行<strong>读</strong>操作，则使用 <strong>ifstream</strong> 对象。</p><p>下面是open()函数的标准语法。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">open</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *fileName, ios::openmode mode)</span></span>;</span><br></pre></td></tr></table></figure><p>在这里，open()成员函数的第一参数指定要打开的文件名字，第二个参数指定打开文件的模式。</p><table><thead><tr><th>模式标志</th><th>描述</th></tr></thead><tbody><tr><td>ios::app</td><td>追加模式，所有写入都追加到文件末尾</td></tr><tr><td>ios::ate</td><td>文件打开后将文件指针位置定位到文件末尾</td></tr><tr><td>ios::in</td><td>打开文件用于读取</td></tr><tr><td>ios::out</td><td>打开文件用于写入</td></tr><tr><td>ios::trunc</td><td>如果该文件已经存在，其内容将在打开之前被截断，即把文件长度设为0.</td></tr></tbody></table><p>可以把以上两种或两种以上的模式结合使用，以“或”运算（“|”）的方式。例如：如果想要以写入模式打开文件，并希望截断文件，以防文件已经存在，那么可以使用下面的语法：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ofstream outFile;</span><br><span class="line">outFile.<span class="built_in">open</span>( <span class="string">"file.dat"</span>, ios::out | ios::trunc );</span><br></pre></td></tr></table></figure><p>类似的，如果想要打开一个文件用于读写，可以使用下面的语法：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ifstream afile;</span><br><span class="line">afile.<span class="built_in">open</span>(<span class="string">"file.dat"</span>, ios::out | ios::in)</span><br></pre></td></tr></table></figure><p>很多程序中，可能会碰到ofstream out(“Hello.txt”), ifstream in(“…”),fstream foi(“…”)这样的的使用，并没有显式的去调用open（）函数就进行文件的操作，直接调用了其默认的打开方式，因为在stream类的构造函数中调用了open()函数,并拥有同样的构造函数，所以在这里可以直接使用流对象进行文件的操作，默认方式如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ofstream <span class="title">out</span><span class="params">(<span class="string">"..."</span>, ios::out)</span></span>;</span><br><span class="line"><span class="function">ifstream <span class="title">in</span><span class="params">(<span class="string">"..."</span>, ios::in)</span></span>;</span><br><span class="line"><span class="function">fstream <span class="title">foi</span><span class="params">(<span class="string">"..."</span>, ios::in|ios::out)</span></span>;</span><br></pre></td></tr></table></figure><h2 id="关闭文件"><a href="#关闭文件" class="headerlink" title="关闭文件"></a>关闭文件</h2><p>使用close() 函数关闭打开的文件，它是 fstream，ifstream 和 ofstream 对象的一个成员。</p><h2 id="写入文件"><a href="#写入文件" class="headerlink" title="写入文件"></a>写入文件</h2><p>我们使用 流插入运算符（ &lt;&lt; ）向文件写入信息。</p><h2 id="读取文件"><a href="#读取文件" class="headerlink" title="读取文件"></a>读取文件</h2><p>我们使用 流提取运算符（ &gt;&gt; ）从文件读取信息。</p><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p>下面给出一个例子，读取hello.txt文件中的字符串，写入out.txt中：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fstream&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt; </span></span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>; </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    <span class="function">ifstream <span class="title">myfile</span><span class="params">(<span class="string">"hello.txt"</span>)</span></span>; </span><br><span class="line">    <span class="function">ofstream <span class="title">outfile</span><span class="params">(<span class="string">"out.txt"</span>, ios::app)</span></span>; </span><br><span class="line">    <span class="built_in">string</span> temp; </span><br><span class="line">    <span class="keyword">if</span> (!myfile.is_open()) </span><br><span class="line">    &#123; </span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"未成功打开文件"</span> &lt;&lt; <span class="built_in">endl</span>; </span><br><span class="line">    &#125; </span><br><span class="line">    <span class="keyword">while</span>(getline(myfile,temp)) </span><br><span class="line">    &#123; </span><br><span class="line">        outfile &lt;&lt; temp; </span><br><span class="line">        outfile &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125; </span><br><span class="line">    myfile.<span class="built_in">close</span>(); </span><br><span class="line">    outfile.<span class="built_in">close</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中getline(stream, string)函数的功能：按行从输入流中读入字符，存到string变量</p><ul><li><p>直到出现以下情况为止：</p></li><li><p>读入了文件结束标志</p></li><li><p>读到一个新行</p></li><li><p>达到字符串的最大长度</p></li></ul><p>如果getline()没有读入字符，将返回false，可用于判断文件是否结束。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> v, w, weight;</span><br><span class="line">ifstream infile;   <span class="comment">//输入流</span></span><br><span class="line"> </span><br><span class="line">infile.<span class="built_in">open</span>(<span class="string">"data.txt"</span>, ios::in); </span><br><span class="line"><span class="keyword">if</span>(!infile.is_open ())</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"Open file failure"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">while</span> (!infile.eof())            <span class="comment">// 到达文件末尾时返回true</span></span><br><span class="line">&#123;</span><br><span class="line">    infile &gt;&gt; v &gt;&gt; w &gt;&gt; weight;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; v &lt;&lt; <span class="string">"\t"</span> &lt;&lt; w &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"> infile.<span class="built_in">close</span>();   <span class="comment">//关闭文件</span></span><br></pre></td></tr></table></figure><blockquote><p>上述代码的功能是读取data.txt文件的数据，注意，此时要求data.txt文件中的数据是三个一行，每个数据用空格或换行符隔开。</p></blockquote><h2 id="文件指针"><a href="#文件指针" class="headerlink" title="文件指针"></a>文件指针</h2><p> 文件指针位置在c++中的用法：</p><table><thead><tr><th>标准</th><th>描述</th></tr></thead><tbody><tr><td>ios::beg</td><td>文件头</td></tr><tr><td>ios::end</td><td>文件尾</td></tr><tr><td>ios::cur</td><td>当前位置</td></tr></tbody></table><p>例如：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">file.seekg(<span class="number">0</span>,ios::beg);   <span class="comment">//让文件指针定位到文件开头 </span></span><br><span class="line">file.seekg(<span class="number">0</span>,ios::<span class="built_in">end</span>);   <span class="comment">//让文件指针定位到文件末尾 </span></span><br><span class="line">file.seekg(<span class="number">10</span>,ios::cur);   <span class="comment">//让文件指针从当前位置向文件末方向移动10个字节 </span></span><br><span class="line">file.seekg(<span class="number">-10</span>,ios::cur);   <span class="comment">//让文件指针从当前位置向文件开始方向移动10个字节 </span></span><br><span class="line">file.seekg(<span class="number">10</span>,ios::beg);   <span class="comment">//让文件指针定位到离文件开头10个字节的位置</span></span><br></pre></td></tr></table></figure><p><strong>注意：移动的单位是字节，而不是行</strong>。</p><h2 id="状态标志符的验证-Verification-of-state-flags"><a href="#状态标志符的验证-Verification-of-state-flags" class="headerlink" title="状态标志符的验证(Verification of state flags)"></a>状态标志符的验证(Verification of state flags)</h2><p>除了eof()以外，还有一些验证流的状态的成员函数（所有都返回bool型返回值）：</p><ul><li><p><strong>bad()</strong></p><p>如果在读写过程中出错，返回 true 。例如：当我们要对一个不是打开为写状态的文件进行写入时，或者我们要写入的设备没有剩余空间的时候。</p></li><li><p><strong>fail()</strong></p><p>除了与bad() 同样的情况下会返回 true 以外，加上格式错误时也返回true ，例如当想要读入一个整数，而获得了一个字母的时候。</p></li><li><p><strong>eof()</strong></p><p>如果读文件到达文件末尾，返回true。</p></li><li><p><strong>good()</strong></p><p>这是最通用的：如果调用以上任何一个函数返回true 的话，此函数返回 false 。</p></li></ul><p>要想重置以上成员函数所检查的状态标志，你可以使用成员函数clear()，没有参数。</p><h1 id="string流"><a href="#string流" class="headerlink" title="string流"></a>string流</h1><p>string头文件定义了三个类型来支持内存IO，istringstream向string读取数据，ostringstream从string写数据，stringstream既可从string读取数据也可向string写数据，就像string是一个IO流一样。</p><h2 id="istringstream的用法"><a href="#istringstream的用法" class="headerlink" title="istringstream的用法"></a>istringstream的用法</h2><p>例子如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sstream&gt;    //使用istringstream所需要的头文件 </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt; </span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    <span class="built_in">string</span> str = <span class="string">"Hello world! I am Lee"</span>; </span><br><span class="line">    <span class="function"><span class="built_in">istringstream</span> <span class="title">is</span><span class="params">(str)</span></span>;    <span class="comment">//将is绑定到str</span></span><br><span class="line">    <span class="built_in">string</span> s; </span><br><span class="line">    <span class="keyword">while</span> (is &gt;&gt; s) </span><br><span class="line">    &#123; </span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; s &lt;&lt; <span class="built_in">endl</span>; </span><br><span class="line">    &#125; </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这相当于把一个句子拆分成单词，联系到前文提到的从文件中读取string的方法，如果读取到的string对象为一个句子，包含很多单词，那么我们就可以运用这种方法把string对象拆分开来。</p><p>结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Hello</span><br><span class="line">world!</span><br><span class="line">I</span><br><span class="line">am</span><br><span class="line">Lee</span><br></pre></td></tr></table></figure><h2 id="ostringstream的用法"><a href="#ostringstream的用法" class="headerlink" title="ostringstream的用法"></a>ostringstream的用法</h2><p>​    ostringstream同样是由一个string对象构造而来，ostringstream类向一个string插入字符。 </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sstream&gt;    //使用ostringstream所需要的头文件 </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;  </span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span>    </span></span><br><span class="line"><span class="function"></span>&#123;  </span><br><span class="line">    <span class="built_in">ostringstream</span> ostr;  </span><br><span class="line">   <span class="comment">// ostr.str("abc");//如果构造的时候设置了字符串参数,那么增长操作的时候不会从结尾开始增加,而是修改原有数据,超出的部分增长  </span></span><br><span class="line">    ostr.<span class="built_in">put</span>(<span class="string">'d'</span>);  </span><br><span class="line">    ostr.<span class="built_in">put</span>(<span class="string">'e'</span>);  </span><br><span class="line">    ostr&lt;&lt;<span class="string">"fg"</span>;    </span><br><span class="line">    <span class="built_in">string</span> gstr = ostr.str();  </span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;gstr &lt;&lt; <span class="built_in">endl</span>; </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在上例代码中，我们通过put()或者左移操作符可以不断向ostr插入单个字符或者是字符串，通过str()函数返回增长过后的完整字符串数据，但值 得注意的一点是，当构造的时候对象内已经存在字符串数据的时候，那么增长操作的时候不会从结尾开始增加,而是修改原有数据,超出的部分增长。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇总结 C++ 的文本操作，即C++文件和流的概念。&lt;/p&gt;
    
    </summary>
    
    
      <category term="C++" scheme="http://iceWind-R.github.io/categories/C/"/>
    
    
      <category term="文本操作" scheme="http://iceWind-R.github.io/tags/%E6%96%87%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>停歇_在此靠岸</title>
    <link href="http://icewind-r.github.io/2020/09/04/%E5%81%9C%E6%AD%87-%E5%9C%A8%E6%AD%A4%E9%9D%A0%E5%B2%B8/"/>
    <id>http://icewind-r.github.io/2020/09/04/%E5%81%9C%E6%AD%87-%E5%9C%A8%E6%AD%A4%E9%9D%A0%E5%B2%B8/</id>
    <published>2020-09-04T04:53:07.000Z</published>
    <updated>2020-09-04T05:01:51.817Z</updated>
    
    <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=522353195&auto=1&height=66"></iframe>]]></content>
    
    <summary type="html">
    
      
      
        &lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=&quot;330&quot; height=&quot;86&quot; src=&quot;//music.163.com/outchain/player?type=2&amp;id=
      
    
    </summary>
    
    
      <category term="随笔" scheme="http://iceWind-R.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="idea" scheme="http://iceWind-R.github.io/tags/idea/"/>
    
  </entry>
  
  <entry>
    <title>大数据_11(函数,数据压缩存储,调优)</title>
    <link href="http://icewind-r.github.io/2020/08/29/%E5%A4%A7%E6%95%B0%E6%8D%AE-11/"/>
    <id>http://icewind-r.github.io/2020/08/29/%E5%A4%A7%E6%95%B0%E6%8D%AE-11/</id>
    <published>2020-08-29T01:32:29.000Z</published>
    <updated>2020-08-29T02:01:53.487Z</updated>
    
    <content type="html"><![CDATA[<p>本篇总结Hive的函数、数据压缩和数据存储格式 以及 Hive调优等。</p><a id="more"></a><hr><h1 id="Hive函数"><a href="#Hive函数" class="headerlink" title="Hive函数"></a>Hive函数</h1><h2 id="内置函数"><a href="#内置函数" class="headerlink" title="内置函数"></a>内置函数</h2><p>内容较多，见<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF" target="_blank" rel="noopener">《Hive官方文档》</a>。</p><ol><li><p>查看系统自带的函数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> show <span class="built_in">functions</span>;</span></span><br></pre></td></tr></table></figure></li><li><p>显示自带的函数的用法</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> desc <span class="keyword">function</span> upper;</span></span><br></pre></td></tr></table></figure></li><li><p>详细显示自带的函数的用法</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> desc <span class="keyword">function</span> extended upper;</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="常用内置函数"><a href="#常用内置函数" class="headerlink" title="常用内置函数"></a>常用内置函数</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">字符串连接函数： concat </span></span><br><span class="line">  select concat('abc','def’,'gh');</span><br><span class="line"><span class="meta">#</span><span class="bash">带分隔符字符串连接函数： concat_ws </span></span><br><span class="line">  select concat_ws(',','abc','def','gh');</span><br><span class="line"><span class="meta">#</span><span class="bash">cast类型转换</span></span><br><span class="line">  select cast(1.5 as int);</span><br><span class="line"><span class="meta">#</span><span class="bash">get_json_object(json 解析函数，用来处理json，必须是json格式)</span></span><br><span class="line">   select get_json_object('&#123;"name":"jack","age":"20"&#125;','$.name');</span><br><span class="line"><span class="meta">#</span><span class="bash">URL解析函数</span></span><br><span class="line">   select parse_url('http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1', 'HOST');</span><br><span class="line"><span class="meta">#</span><span class="bash">explode：把map集合中每个键值对或数组中的每个元素都单独生成一行的形式</span></span><br></pre></td></tr></table></figure><h2 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><ol><li>Hive 自带了一些函数，比如：max/min等，当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数(UDF).</li><li>根据用户自定义函数类别分为以下三种：<ol><li>UDF（User-Defined-Function）<ul><li>一进一出</li></ul></li><li>UDAF（User-Defined Aggregation Function）<ul><li>聚集函数，多进一出</li><li>类似于：<code>count</code>/<code>max</code>/<code>min</code></li></ul></li><li>UDTF（User-Defined Table-Generating Functions）<ul><li>一进多出</li><li>如 <code>lateral</code> <code>view</code> <code>explore()</code></li></ul></li></ol></li><li>编程步骤：<ol><li>继承org.apache.hadoop.hive.ql.UDF</li><li>需要实现evaluate函数；evaluate函数支持重载；</li></ol></li><li>注意事项<ol><li>UDF必须要有返回类型，可以返回null，但是返回类型不能为void；</li><li>UDF中常用Text/LongWritable等类型，不推荐使用java类型；</li></ol></li></ol><h3 id="UDF-开发实例"><a href="#UDF-开发实例" class="headerlink" title="UDF 开发实例"></a>UDF 开发实例</h3><p>需求，建立的自己的my_upper方法，将输入的字符串第一个字符大写。</p><h4 id="1、创建maven工程"><a href="#1、创建maven工程" class="headerlink" title="1、创建maven工程"></a>1、创建maven工程</h4><p>其中的pom.xml文件如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-exec --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">encoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">encoding</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><p>注意：这一步踩了坑，hive-exec坐标引用后爆红（org\pentaho\pentaho-aggdesigner-algorithm\5.1.5-jhyde.jar有红线），<strong>解决方法：</strong></p><ul><li><p>进入<a href="https://mvnrepository.com/artifact/org.pentaho/pentaho-aggdesigner-algorithm/5.1.5-jhyde下载相应jar包" target="_blank" rel="noopener">https://mvnrepository.com/artifact/org.pentaho/pentaho-aggdesigner-algorithm/5.1.5-jhyde下载相应jar包</a></p></li><li><p>放入本地maven仓库中（我的目录为D:\Server\Tools\maven_repository\org\pentaho\pentaho-aggdesigner-algorithm\5.1.5-jhyde）</p></li><li><p>重启IDEA，并在pom.xml中导入以下依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.pentaho<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>pentaho-aggdesigner-algorithm<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.5-jhyde<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>完成。</p></li></ul><h4 id="2、编写程序代码"><a href="#2、编写程序代码" class="headerlink" title="2、编写程序代码"></a>2、编写程序代码</h4><p><strong>开发 Java 类集成 UDF</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyUDF</span>  <span class="keyword">extends</span> <span class="title">UDF</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Text <span class="title">evaluate</span><span class="params">(<span class="keyword">final</span> Text str)</span></span>&#123;</span><br><span class="line">        String tmp_str = str.toString();</span><br><span class="line">        <span class="keyword">if</span>(str != <span class="keyword">null</span> &amp;&amp; !tmp_str.equals(<span class="string">""</span>))&#123;</span><br><span class="line">          String str_ret = tmp_str.substring(<span class="number">0</span>, <span class="number">1</span>).toUpperCase() + tmp_str.substring(<span class="number">1</span>);</span><br><span class="line">          <span class="keyword">return</span>  <span class="keyword">new</span> Text(str_ret);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span>  <span class="keyword">new</span> Text(<span class="string">""</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="3、项目打包"><a href="#3、项目打包" class="headerlink" title="3、项目打包"></a>3、项目打包</h4><p>利用maven 的package命令打成jar包，并上传到集群（bigdata3）的hive的lib目录下。</p><h4 id="4、添加jar包到hive中"><a href="#4、添加jar包到hive中" class="headerlink" title="4、添加jar包到hive中"></a>4、添加jar包到hive中</h4><p>重命名我们的jar包名称</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/apache-hive-2.7.5-bin/lib</span><br><span class="line">mv hive-1.0-SNAPSHOT.jar my_upper.jar</span><br></pre></td></tr></table></figure><p>hive的客户端添加我们的jar包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> add jar /<span class="built_in">export</span>/servers/apache-hive-2.7.5-bin/lib/my_upper.jar;</span></span><br></pre></td></tr></table></figure><h4 id="5、设置函数与我们的自定义函数关联"><a href="#5、设置函数与我们的自定义函数关联" class="headerlink" title="5、设置函数与我们的自定义函数关联"></a>5、设置函数与我们的自定义函数关联</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create temporary function my_upper as 'udf.MyUDF';</span><br></pre></td></tr></table></figure><h4 id="6、使用自定义函数"><a href="#6、使用自定义函数" class="headerlink" title="6、使用自定义函数"></a>6、使用自定义函数</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> my_upper(<span class="string">'hello world!'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果 Hello world!</span></span><br></pre></td></tr></table></figure><h1 id="数据压缩、数据存储格式"><a href="#数据压缩、数据存储格式" class="headerlink" title="数据压缩、数据存储格式"></a>数据压缩、数据存储格式</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇总结Hive的函数、数据压缩和数据存储格式 以及 Hive调优等。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://iceWind-R.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://iceWind-R.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据_10(查询、Shell参数)</title>
    <link href="http://icewind-r.github.io/2020/08/24/%E5%A4%A7%E6%95%B0%E6%8D%AE-10/"/>
    <id>http://icewind-r.github.io/2020/08/24/%E5%A4%A7%E6%95%B0%E6%8D%AE-10/</id>
    <published>2020-08-24T08:14:15.000Z</published>
    <updated>2020-08-29T01:35:36.068Z</updated>
    
    <content type="html"><![CDATA[<p>本篇总结Hive的查询语法、Shell命令。</p><a id="more"></a><hr><h1 id="Hive-查询语法"><a href="#Hive-查询语法" class="headerlink" title="Hive 查询语法"></a>Hive 查询语法</h1><h2 id="SELECT"><a href="#SELECT" class="headerlink" title="SELECT"></a>SELECT</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> [<span class="keyword">ALL</span> | <span class="keyword">DISTINCT</span>] select_expr, select_expr, ...</span><br><span class="line"><span class="keyword">FROM</span> table_reference</span><br><span class="line">[<span class="keyword">WHERE</span> where_condition]</span><br><span class="line">[<span class="keyword">GROUP</span> <span class="keyword">BY</span> col_list [<span class="keyword">HAVING</span> condition]]</span><br><span class="line">[CLUSTER <span class="keyword">BY</span> col_list</span><br><span class="line">| [<span class="keyword">DISTRIBUTE</span> <span class="keyword">BY</span> col_list] [<span class="keyword">SORT</span> <span class="keyword">BY</span>| <span class="keyword">ORDER</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">]</span><br><span class="line">[<span class="keyword">LIMIT</span> <span class="built_in">number</span>]</span><br></pre></td></tr></table></figure><ol><li>order by 会对输入做全局排序，因此只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。</li><li>sort by不是全局排序，其在数据进入reducer前完成排序。因此，如果用sort by进行排序，并且设置mapred.reduce.tasks&gt;1，则sort by只保证每个reducer的输出有序，不保证全局有序。</li><li>distribute by(字段)根据指定的字段将数据分到不同的reducer，且分发算法是hash散列。</li><li>cluster by(字段) 除了具有distribute by的功能外，还会对该字段进行排序.</li><li>因此，如果distribute 和sort字段是同一个时，此时，<code>cluster by = distribute by + sort by</code></li></ol><h2 id="查询语法"><a href="#查询语法" class="headerlink" title="查询语法"></a>查询语法</h2><p><strong>全表查询</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure><p><strong>选择特定列</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_id ,c_id <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure><p><strong>列别名</strong></p><p>1）重命名一个列。<br>2）便于计算。<br>3）紧跟列名，也可以在列名和别名之间加入关键字‘AS’</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_id <span class="keyword">as</span> myid ,c_id <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure><h2 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h2><ul><li>求总行数（count）注：count(1) 等价于 count(*)</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure><ul><li>求分数的最大值（max）</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">max</span>(s_score) <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure><ul><li>求分数的最小值（min）</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">min</span>(s_score) <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure><ul><li>求分数的总和（sum）</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">sum</span>(s_score) <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure><ul><li>求分数的平均值（avg）</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">avg</span>(s_score) <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure><h2 id="LIMIT语句"><a href="#LIMIT语句" class="headerlink" title="LIMIT语句"></a>LIMIT语句</h2><p>典型的查询会返回多行数据。LIMIT子句用于限制返回的行数。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">limit</span> <span class="number">3</span>;</span><br></pre></td></tr></table></figure><h2 id="WHERE语句"><a href="#WHERE语句" class="headerlink" title="WHERE语句"></a>WHERE语句</h2><ol><li>使用WHERE 子句，将不满足条件的行过滤掉。</li><li>WHERE 子句紧随 FROM 子句。</li><li>案例实操</li></ol><p>查询出分数大于60的数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score &gt; <span class="number">60</span>;</span><br></pre></td></tr></table></figure><p><strong>比较运算符</strong></p><table><thead><tr><th>操作符</th><th>支持的数据类型</th><th>描述</th></tr></thead><tbody><tr><td>A=B</td><td>基本数据类型</td><td>如果A等于B则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;=&gt;B</td><td>基本数据类型</td><td>如果A和B都为NULL，则返回TRUE，其他的和等号（=）操作符的结果一致，如果任一为NULL则结果为NULL</td></tr><tr><td>A&lt;&gt;B, A!=B</td><td>基本数据类型</td><td>A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A小于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;=B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A小于等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&gt;B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A大于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&gt;=B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A大于等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A [NOT] BETWEEN B AND C</td><td>基本数据类型</td><td>如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果。</td></tr><tr><td>A IS NULL</td><td>所有数据类型</td><td>如果A等于NULL，则返回TRUE，反之返回FALSE</td></tr><tr><td>A IS NOT NULL</td><td>所有数据类型</td><td>如果A不等于NULL，则返回TRUE，反之返回FALSE</td></tr><tr><td>IN(数值1, 数值2)</td><td>所有数据类型</td><td>使用 IN运算显示列表中的值</td></tr><tr><td>A [NOT] LIKE B</td><td>STRING 类型</td><td>B是一个SQL下的简单正则表达式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。</td></tr><tr><td>A RLIKE B, A REGEXP B</td><td>STRING</td><td>类型 B是一个正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。</td></tr></tbody></table><ul><li>查询分数等于80的所有的数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score = <span class="number">80</span>;</span><br></pre></td></tr></table></figure><ul><li>查询分数在80到100的所有数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score <span class="keyword">between</span> <span class="number">80</span> <span class="keyword">and</span> <span class="number">100</span>;</span><br></pre></td></tr></table></figure><ul><li>查询成绩为空的所有数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score <span class="keyword">is</span> <span class="literal">null</span>;</span><br></pre></td></tr></table></figure><ul><li>查询成绩是80和90的数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score <span class="keyword">in</span>(<span class="number">80</span>,<span class="number">90</span>);</span><br></pre></td></tr></table></figure><h2 id="LIKE-和-RLIKE"><a href="#LIKE-和-RLIKE" class="headerlink" title="LIKE 和 RLIKE"></a>LIKE 和 RLIKE</h2><p>1、使用LIKE运算选择类似的值，选择条件可以包含字符或数字:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">% 代表零个或多个字符(任意个字符)。</span><br><span class="line">_ 代表一个字符。</span><br></pre></td></tr></table></figure><p>2、RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件。</p><p><strong>案例实操</strong></p><ol><li>查找以8开头的所有成绩</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score <span class="keyword">like</span> <span class="string">'8%'</span>;</span><br></pre></td></tr></table></figure><ol><li>查找第二个数值为9的所有成绩数据</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score <span class="keyword">like</span> <span class="string">'_9%'</span>;</span><br></pre></td></tr></table></figure><ol><li>查找s_id中含1的数据</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_id <span class="keyword">rlike</span> <span class="string">'[1]'</span>;  <span class="comment">#  like '%1%'</span></span><br></pre></td></tr></table></figure><h2 id="逻辑运算符"><a href="#逻辑运算符" class="headerlink" title="逻辑运算符"></a>逻辑运算符</h2><table><thead><tr><th>操作符</th><th>含义</th></tr></thead><tbody><tr><td>AND</td><td>逻辑并</td></tr><tr><td>OR</td><td>逻辑或</td></tr><tr><td>NOT</td><td>逻辑否</td></tr></tbody></table><ul><li>查询成绩大于80，并且s_id是01的数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score &gt;<span class="number">80</span> <span class="keyword">and</span> s_id = <span class="string">'01'</span>;</span><br></pre></td></tr></table></figure><ul><li>查询成绩大于80，或者s_id 是01的数</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score &gt; <span class="number">80</span> <span class="keyword">or</span> s_id = <span class="string">'01'</span>;</span><br></pre></td></tr></table></figure><ul><li>查询s_id 不是 01和02的学生</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_id <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">'01'</span>,<span class="string">'02'</span>);</span><br></pre></td></tr></table></figure><h2 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h2><h3 id="GROUP-BY-语句"><a href="#GROUP-BY-语句" class="headerlink" title="GROUP BY 语句"></a>GROUP BY 语句</h3><p>GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。<br>案例实操：</p><ul><li>计算每个学生的平均分数</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_id ,<span class="keyword">avg</span>(s_score) <span class="keyword">from</span> score <span class="keyword">group</span> <span class="keyword">by</span> s_id;</span><br></pre></td></tr></table></figure><ul><li>计算每个学生最高成绩</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_id ,<span class="keyword">max</span>(s_score) <span class="keyword">from</span> score <span class="keyword">group</span> <span class="keyword">by</span> s_id;</span><br></pre></td></tr></table></figure><h3 id="HAVING-语句"><a href="#HAVING-语句" class="headerlink" title="HAVING 语句"></a>HAVING 语句</h3><ol><li><p>having与where不同点</p><ol><li>where针对表中的列发挥作用，查询数据；having针对查询结果中的列发挥作用，筛选数据。</li><li>where后面不能写分组函数，而having后面可以使用分组函数。</li><li>having只用于group by分组统计语句。</li></ol></li><li><p>案例实操：</p><ul><li>求每个学生的平均分数</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_id ,<span class="keyword">avg</span>(s_score) <span class="keyword">from</span> score <span class="keyword">group</span> <span class="keyword">by</span> s_id;</span><br></pre></td></tr></table></figure><ul><li>求每个学生平均分数大于85的人</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_id ,<span class="keyword">avg</span>(s_score) avgscore <span class="keyword">from</span> score <span class="keyword">group</span> <span class="keyword">by</span> s_id <span class="keyword">having</span> avgscore &gt; <span class="number">85</span>;</span><br></pre></td></tr></table></figure></li></ol><h2 id="JOIN-语句"><a href="#JOIN-语句" class="headerlink" title="JOIN 语句"></a>JOIN 语句</h2><h3 id="等值-JOIN"><a href="#等值-JOIN" class="headerlink" title="等值 JOIN"></a>等值 JOIN</h3><p>Hive支持通常的SQL JOIN语句，但是只支持等值连接，不支持非等值连接。</p><p>案例操作: 查询分数对应的姓名</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s.s_id,s.s_score,stu.s_name,stu.s_birth  <span class="keyword">from</span> score s  <span class="keyword">join</span> student stu <span class="keyword">on</span> s.s_id = stu.s_id;</span><br></pre></td></tr></table></figure><h3 id="表的别名"><a href="#表的别名" class="headerlink" title="表的别名"></a>表的别名</h3><ul><li><p>好处</p><ul><li>使用别名可以简化查询。</li><li>使用表名前缀可以提高执行效率。</li></ul></li><li><p>案例实操</p><ul><li>合并老师与课程表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> techer t <span class="keyword">join</span> course c <span class="keyword">on</span> t.t_id = c.t_id;</span><br></pre></td></tr></table></figure></li></ul><h3 id="内连接"><a href="#内连接" class="headerlink" title="内连接"></a>内连接</h3><p>内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> techer t <span class="keyword">inner</span> <span class="keyword">join</span> course c <span class="keyword">on</span> t.t_id = c.t_id;</span><br></pre></td></tr></table></figure><h3 id="左外连接"><a href="#左外连接" class="headerlink" title="左外连接"></a>左外连接</h3><p>左外连接：JOIN操作符左边表中符合WHERE子句的所有记录将会被返回。<br>查询老师对应的课程</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> techer t <span class="keyword">left</span> <span class="keyword">join</span> course c <span class="keyword">on</span> t.t_id = c.t_id;</span><br></pre></td></tr></table></figure><h3 id="右外连接"><a href="#右外连接" class="headerlink" title="右外连接"></a>右外连接</h3><p>右外连接：JOIN操作符右边表中符合WHERE子句的所有记录将会被返回。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> teacher t <span class="keyword">right</span> <span class="keyword">join</span> course c <span class="keyword">on</span> t.t_id = c.t_id;</span><br></pre></td></tr></table></figure><h3 id="多表连接"><a href="#多表连接" class="headerlink" title="多表连接"></a>多表连接</h3><p>注意：连接 n个表，至少需要n-1个连接条件。例如：连接三个表，至少需要两个连接条件。</p><p>多表连接查询，查询老师对应的课程，以及对应的分数，对应的学生</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> teacher t</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> course c</span><br><span class="line"><span class="keyword">on</span> t.t_id = c.t_id</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> score s</span><br><span class="line"><span class="keyword">on</span> s.c_id = c.c_id</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> student stu</span><br><span class="line"><span class="keyword">on</span> s.s_id = stu.s_id;</span><br></pre></td></tr></table></figure><p>大多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表techer和表course进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表score;进行连接操作。</p><h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><h3 id="全局排序"><a href="#全局排序" class="headerlink" title="全局排序"></a>全局排序</h3><p>Order By：全局排序，只能有一个reduce</p><ol><li><p>使用 ORDER BY 子句排序<br>ASC（ascend）: 升序（默认）<br>DESC（descend）: 降序</p></li><li><p>ORDER BY 子句在SELECT语句的结尾。</p></li><li><p>案例实操</p><ol><li>查询学生的成绩，并按照分数降序排列</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> student s <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> score sco <span class="keyword">ON</span> s.s_id = sco.s_id <span class="keyword">ORDER</span> <span class="keyword">BY</span> sco.s_score <span class="keyword">DESC</span>;</span><br><span class="line">1</span><br></pre></td></tr></table></figure><p>​    2. 查询学生的成绩，并按照分数升序排列</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> student s <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> score sco <span class="keyword">ON</span> s.s_id = sco.s_id <span class="keyword">ORDER</span> <span class="keyword">BY</span> sco.s_score <span class="keyword">asc</span>;</span><br></pre></td></tr></table></figure></li></ol><h3 id="按照别名排序"><a href="#按照别名排序" class="headerlink" title="按照别名排序"></a>按照别名排序</h3><p>按照分数的平均值排序</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_id ,<span class="keyword">avg</span>(s_score) <span class="keyword">avg</span> <span class="keyword">from</span> score <span class="keyword">group</span> <span class="keyword">by</span> s_id <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">avg</span>;</span><br></pre></td></tr></table></figure><h3 id="多个列排序"><a href="#多个列排序" class="headerlink" title="多个列排序"></a>多个列排序</h3><p>按照学生id和平均成绩进行排序</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_id ,<span class="keyword">avg</span>(s_score) <span class="keyword">avg</span> <span class="keyword">from</span> score <span class="keyword">group</span> <span class="keyword">by</span> s_id <span class="keyword">order</span> <span class="keyword">by</span> s_id,<span class="keyword">avg</span>;</span><br></pre></td></tr></table></figure><h3 id="每个MapReduce内部排序（Sort-By）局部排序"><a href="#每个MapReduce内部排序（Sort-By）局部排序" class="headerlink" title="每个MapReduce内部排序（Sort By）局部排序"></a>每个MapReduce内部排序（Sort By）局部排序</h3><p>Sort By：每个MapReduce内部进行排序，对全局结果集来说不是排序。</p><ol><li><p>设置reduce个数</p> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">3</span>;</span><br></pre></td></tr></table></figure></li><li><p>查看设置reduce个数</p> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces;</span><br></pre></td></tr></table></figure></li><li><p>查询成绩按照成绩降序排列</p> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">sort</span> <span class="keyword">by</span> s_score;</span><br></pre></td></tr></table></figure></li><li><p>将查询结果导入到文件中（按照成绩降序排列）</p> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/export/servers/hivedatas/sort'</span> <span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">sort</span> <span class="keyword">by</span> s_score;</span><br></pre></td></tr></table></figure></li></ol><h3 id="分区排序（DISTRIBUTE-BY）"><a href="#分区排序（DISTRIBUTE-BY）" class="headerlink" title="分区排序（DISTRIBUTE BY）"></a>分区排序（DISTRIBUTE BY）</h3><p>Distribute By：类似MR中partition，进行分区，结合sort by使用。</p><p>注意，Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。</p><p>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</p><p>案例实操：先按照学生id进行分区，再按照学生成绩进行排序。</p><ol><li><p>设置reduce的个数，将我们对应的s_id划分到对应的reduce当中去</p> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">7</span>;</span><br></pre></td></tr></table></figure></li><li><p>通过distribute by 进行数据的分区</p> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/export/servers/hivedatas/sort'</span> <span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">distribute</span> <span class="keyword">by</span> s_id <span class="keyword">sort</span> <span class="keyword">by</span> s_score;</span><br></pre></td></tr></table></figure></li></ol><h3 id="CLUSTER-BY"><a href="#CLUSTER-BY" class="headerlink" title="CLUSTER BY"></a>CLUSTER BY</h3><p>当distribute by和sort by字段相同时，可以使用cluster by方式。</p><p>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是倒序排序，不能指定排序规则为ASC或者DESC。</p><p>以下两种写法等价</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score cluster <span class="keyword">by</span> s_id;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">distribute</span> <span class="keyword">by</span> s_id <span class="keyword">sort</span> <span class="keyword">by</span> s_id;</span><br></pre></td></tr></table></figure><h1 id="Shell参数"><a href="#Shell参数" class="headerlink" title="Shell参数"></a>Shell参数</h1><h2 id="Hive命令行"><a href="#Hive命令行" class="headerlink" title="Hive命令行"></a>Hive命令行</h2><h3 id="语法结构"><a href="#语法结构" class="headerlink" title="语法结构"></a>语法结构</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hive [-hiveconf x=y]* [&lt;-i filename&gt;]* [&lt;-f filename&gt;|&lt;-e query-string&gt;] [-S]</span><br></pre></td></tr></table></figure><h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><p>1、 -i 从文件初始化HQL。</p><p>2、 <code>-e从命令行执行指定的HQL</code></p><p>3、 <code>-f 执行HQL脚本</code></p><p>4、 -v 输出执行的HQL语句到控制台</p><p>5、 -p connect to Hive Server on port number</p><p>6、 -hiveconf x=y Use this to set hive/hadoop configuration variables. 设置hive运行时候的参数配置</p><h2 id="Hive参数配置方式"><a href="#Hive参数配置方式" class="headerlink" title="Hive参数配置方式"></a>Hive参数配置方式</h2><p>开发Hive应用时，不可避免地需要设定Hive的参数。设定Hive的参数可以调优HQL代码的执行效率，或帮助定位问题。</p><p><strong>对于一般参数，有以下三种设定方式：</strong></p><ul><li>配置文件</li><li>命令行参数</li><li>参数声明</li></ul><p><code>配置文件</code>：Hive的配置文件包括</p><ul><li><p>用户自定义配置文件：$HIVE_CONF_DIR/hive-site.xml</p></li><li><p>默认配置文件： $HIVE_CONF_DIR/hive-default.xml</p><p><strong>用户自定义配置会覆盖默认配置。</strong></p></li></ul><p>另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。</p><p>配置文件的设定对本机启动的所有Hive进程都有效。</p><p><code>命令行参数：</code>启动Hive（客户端或Server方式）时，可以在命令行添加-hiveconf param=value来设定参数，例如：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hive -hiveconf hive.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>这一设定对本次启动的Session（对于Server方式启动，则是所有请求的Sessions）有效。</p><p><code>参数声明</code>：可以在HQL中使用SET关键字设定参数，例如：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.reduce.tasks=<span class="number">100</span>;</span><br></pre></td></tr></table></figure><p>这一设定的作用域也是session级的。</p><p>上述三种设定方式的优先级依次递增。即参数声明覆盖命令行参数，命令行参数覆盖配置文件设定。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在Session建立以前已经完成了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇总结Hive的查询语法、Shell命令。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://iceWind-R.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://iceWind-R.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据_09(Hive基本操作)</title>
    <link href="http://icewind-r.github.io/2020/08/22/%E5%A4%A7%E6%95%B0%E6%8D%AE-09/"/>
    <id>http://icewind-r.github.io/2020/08/22/%E5%A4%A7%E6%95%B0%E6%8D%AE-09/</id>
    <published>2020-08-22T02:48:33.000Z</published>
    <updated>2020-08-24T08:20:26.655Z</updated>
    
    <content type="html"><![CDATA[<p>本篇总结Hive的交互方式和<strong>基本操作命令</strong>。</p><a id="more"></a><hr><h1 id="Hive-的交互方式"><a href="#Hive-的交互方式" class="headerlink" title="Hive 的交互方式"></a>Hive 的交互方式</h1><h2 id="第一种交互方式：-bin-hive"><a href="#第一种交互方式：-bin-hive" class="headerlink" title="第一种交互方式：./bin/hive"></a>第一种交互方式：./bin/hive</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create database if not exists mytest;</span><br></pre></td></tr></table></figure><h2 id="第二种交互方式：使用-sql-语句或者-sql-脚本进行交互"><a href="#第二种交互方式：使用-sql-语句或者-sql-脚本进行交互" class="headerlink" title="第二种交互方式：使用 sql 语句或者 sql 脚本进行交互"></a>第二种交互方式：使用 sql 语句或者 sql 脚本进行交互</h2><p>不进入hive的客户端直接执行hive的hql语句</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hive -e "create database if not exists mytest;"</span><br></pre></td></tr></table></figure><p>或者我们可以将我们的hql语句写成一个sql脚本执行，通过hive -f 来执行我们的sql脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hive -f /export/servers/hive.sql</span><br></pre></td></tr></table></figure><h1 id="Hive-的基本操作"><a href="#Hive-的基本操作" class="headerlink" title="Hive 的基本操作"></a>Hive 的基本操作</h1><h2 id="数据库操作"><a href="#数据库操作" class="headerlink" title="数据库操作"></a>数据库操作</h2><p><strong>创建数据库：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> myhive;</span><br></pre></td></tr></table></figure><p><strong>创建数据库并指定位置：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> myhive location <span class="string">'/myhive'</span>;</span><br></pre></td></tr></table></figure><p><strong>设置数据库键值对信息：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> foo <span class="keyword">with</span> dbproperties (<span class="string">'owner'</span>=<span class="string">'itcast'</span>,<span class="string">'date'</span>=<span class="string">'20190120'</span>);</span><br></pre></td></tr></table></figure><p><strong>查看数据库更多详细信息：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc database extended myhive;</span><br></pre></td></tr></table></figure><p><strong>删除数据库：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">database</span> myhive;</span><br></pre></td></tr></table></figure><p>​    强制删除数据库，包含数据库下面的表一起删除：（效果相当于 rm -rf，慎用！！！）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">database</span> myhive <span class="keyword">cascade</span>;</span><br></pre></td></tr></table></figure><h2 id="数据库表的操作"><a href="#数据库表的操作" class="headerlink" title="数据库表的操作"></a>数据库表的操作</h2><h3 id="创建表的语法"><a href="#创建表的语法" class="headerlink" title="创建表的语法"></a><strong>创建表的语法</strong></h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> [<span class="keyword">external</span>] <span class="keyword">table</span> [<span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span>] table_name (</span><br><span class="line">col_name data_type [<span class="keyword">comment</span> <span class="string">'字段描述信息'</span>]</span><br><span class="line">col_name data_type [<span class="keyword">comment</span> <span class="string">'字段描述信息'</span>])</span><br><span class="line">[<span class="keyword">comment</span> <span class="string">'表的描述信息'</span>]</span><br><span class="line">[partitioned <span class="keyword">by</span> (col_name data_type,...)]</span><br><span class="line">[clustered <span class="keyword">by</span> (col_name,col_name,...)]</span><br><span class="line">[sorted <span class="keyword">by</span> (col_name [<span class="keyword">asc</span>|<span class="keyword">desc</span>],...) <span class="keyword">into</span> num_buckets buckets]</span><br><span class="line">[<span class="keyword">row</span> <span class="keyword">format</span> row_format]</span><br><span class="line">[storted <span class="keyword">as</span> ....]</span><br><span class="line">[location <span class="string">'指定表的路径'</span>]</span><br></pre></td></tr></table></figure><p>说明：</p><blockquote><ol><li><p><a href="https://blog.csdn.net/chipeize/article/details/100364057" target="_blank" rel="noopener">create table</a></p><p>创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。</p></li><li><p><a href="https://blog.csdn.net/chipeize/article/details/100364057" target="_blank" rel="noopener">external</a></p><p>可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</p></li><li><p><a href="https://blog.csdn.net/chipeize/article/details/100364057" target="_blank" rel="noopener">comment</a></p><p>表示注释,默认不能使用中文</p></li><li><p><a href="https://blog.csdn.net/chipeize/article/details/100364057" target="_blank" rel="noopener">partitioned by</a></p><p>表示使用表分区,一个表可以拥有一个或者多个分区，每一个分区单独存在一个目录下 .</p></li><li><p><a href="https://blog.csdn.net/chipeize/article/details/100364057" target="_blank" rel="noopener">clustered by</a></p><p>对于每一个表分文件， Hive可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive也是 针对某一列进行桶的组织。</p></li><li><p><a href="https://blog.csdn.net/chipeize/article/details/100364057" target="_blank" rel="noopener">sorted by</a></p><p>指定排序字段和排序规则</p></li><li><p><a href="https://blog.csdn.net/chipeize/article/details/100364057" target="_blank" rel="noopener">row format</a></p><p>指定表文件字段分隔符</p></li><li><p><a href="https://blog.csdn.net/chipeize/article/details/100364057" target="_blank" rel="noopener">storted as</a></p><p>指定表文件的存储格式, 常用格式:SEQUENCEFILE, TEXTFILE, RCFILE,如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。如果数据需要压缩，使用 storted as SEQUENCEFILE。</p></li><li><p><a href="https://blog.csdn.net/chipeize/article/details/100364057" target="_blank" rel="noopener">location</a></p><p>指定表文件的存储路径</p></li></ol></blockquote><h3 id="内部表操作"><a href="#内部表操作" class="headerlink" title="内部表操作"></a>内部表操作</h3><p>创建表时，如果没有使用external关键字，则该表是内部表（管理表，managed table）。</p><p><strong>Hive 建表字段类型</strong></p><table><thead><tr><th align="left">分类</th><th align="left">类型</th><th align="left">描述</th><th align="left">字面量示例</th></tr></thead><tbody><tr><td align="left">原始类型</td><td align="left">BOOLEAN</td><td align="left">true/false</td><td align="left">TRUE</td></tr><tr><td align="left"></td><td align="left">TINYINT</td><td align="left">1字节的有符号整数, -128~127</td><td align="left">1Y</td></tr><tr><td align="left"></td><td align="left">SMALLINT</td><td align="left">2个字节的有符号整数，-32768~32767</td><td align="left">1S</td></tr><tr><td align="left"></td><td align="left">INT</td><td align="left">4个字节的带符号整数</td><td align="left">1</td></tr><tr><td align="left"></td><td align="left">BIGINT</td><td align="left">8字节带符号整数</td><td align="left">1L</td></tr><tr><td align="left"></td><td align="left">FLOAT</td><td align="left">4字节单精度浮点数</td><td align="left">1.0</td></tr><tr><td align="left"></td><td align="left">DOUBLE</td><td align="left">8字节双精度浮点数</td><td align="left">1.0</td></tr><tr><td align="left"></td><td align="left">DEICIMAL</td><td align="left">任意精度的带符号小数</td><td align="left">1.0</td></tr><tr><td align="left"></td><td align="left">STRING</td><td align="left">字符串，变长</td><td align="left">“a”,’b’</td></tr><tr><td align="left"></td><td align="left">VARCHAR</td><td align="left">变长字符串</td><td align="left">“a”,’b’</td></tr><tr><td align="left"></td><td align="left">CHAR</td><td align="left">固定长度字符串</td><td align="left">“a”,’b’</td></tr><tr><td align="left"></td><td align="left">BINARY</td><td align="left">字节数组</td><td align="left">无法表示</td></tr><tr><td align="left"></td><td align="left">TIMESTAMP</td><td align="left">时间戳，毫秒值精度</td><td align="left">122327493795</td></tr><tr><td align="left"></td><td align="left">DATE</td><td align="left">日期</td><td align="left">‘2016-03-29’</td></tr><tr><td align="left"></td><td align="left">INTERVAL</td><td align="left">时间频率间隔</td><td align="left"></td></tr><tr><td align="left">复杂类型</td><td align="left">ARRAY</td><td align="left">有序的的同类型的集合</td><td align="left">array(1,2)</td></tr><tr><td align="left"></td><td align="left">MAP</td><td align="left">key-value,key必须为原始类型，value可以任意类型</td><td align="left">map(‘a’,1,’b’,2)</td></tr><tr><td align="left"></td><td align="left">STRUCT</td><td align="left">字段集合,类型可以不同</td><td align="left">struct(‘1’,1,1.0), named_stract(‘col1’,’1’,’col2’,1,’clo3’,1.0)</td></tr><tr><td align="left"></td><td align="left">UNION</td><td align="left">在有限取值范围内的一个值</td><td align="left">create_union(1,’a’,63)</td></tr></tbody></table><p><strong>建表入门</strong>：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> myhive;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu(<span class="keyword">id</span> <span class="built_in">int</span>,<span class="keyword">name</span> <span class="keyword">string</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> stu <span class="keyword">values</span> (<span class="number">1</span>,<span class="string">"zhangsan"</span>);  <span class="comment">#插入数据</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> stu;</span><br></pre></td></tr></table></figure><p>创建表并指定字段之间的分隔符，默认为 <code>\001</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span>  <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> stu2(<span class="keyword">id</span> <span class="built_in">int</span> ,<span class="keyword">name</span> <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><p><strong>创建表并指定表文件的存放路径</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span>  <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> stu2(<span class="keyword">id</span> <span class="built_in">int</span> ,<span class="keyword">name</span> <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span> location <span class="string">'/user/stu2'</span>;</span><br></pre></td></tr></table></figure><p><strong>根据查询结果创建表</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过复制表结构和表内容创建新表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu3 <span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> stu2;</span><br></pre></td></tr></table></figure><p><strong>根据已经存在的表结构创建表</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu4 <span class="keyword">like</span> stu;</span><br></pre></td></tr></table></figure><p><strong>查询表的详细信息</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc formatted stu2;</span><br></pre></td></tr></table></figure><p><strong>删除表</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> stu;</span><br></pre></td></tr></table></figure><h3 id="外部表的操作"><a href="#外部表的操作" class="headerlink" title="外部表的操作"></a><strong>外部表的操作</strong></h3><p>外部表因为是指定其他的hdfs路径的数据加载到表当中来，所以hive表会认为自己不完全独占这份数据，所以删除hive表的时候，数据仍然存放在hdfs当中，不会删掉.</p><p><strong>内部表和外部表的使用场景</strong></p><p>每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。</p><h4 id="操作案例"><a href="#操作案例" class="headerlink" title="操作案例"></a>操作案例</h4><p>分别创建老师与学生外部表，并向表中加载数据。</p><p><strong>创建老师表</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> teacher(t_id <span class="keyword">string</span>,t_name <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><p><strong>创建学生表</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> student (s_id <span class="keyword">string</span>,s_name <span class="keyword">string</span>,s_birth <span class="keyword">string</span> , s_sex <span class="keyword">string</span> ) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><p><strong>加载数据</strong></p><ol><li><p>可以直接把指定结构的文件上传到hdfs文件系统的表目录下。</p><p>比如之前的teacher表有两个string字段，分隔符为 \t，我们可以建如下文件，teacher.txt，内容为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>zhangsan</span><br><span class="line"><span class="number">2</span>lisi</span><br><span class="line"><span class="number">3</span>wangwu           <span class="comment"># 分割符为 \t</span></span><br></pre></td></tr></table></figure><p>即可在hive命令下，使用select查询得到如上结果。</p></li><li><p>本地加载，可以加载本地的文件读入到hive数据仓库中。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/export/servers/hivedatas/student.csv'</span> <span class="keyword">into</span> <span class="keyword">table</span> student;</span><br><span class="line"></span><br><span class="line">加载本地路径下的csv文件到student表中。</span><br></pre></td></tr></table></figure><blockquote><p>我们可以在此验证外部表：通过drop删除表后，select不能查询，但是hdfs系统的数据文件仍在，通过之前的建表语句再次建表，就可查询成功。可见，外部表只是与真实数据的一种映射关系。</p></blockquote></li><li><p>加载数据并覆盖已有数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath '/export/servers/hivedatas/student.csv' overwrite into table student;</span><br></pre></td></tr></table></figure></li><li><p>从hdfs文件系统向表中加载数据（需要提前将数据上传到hdfs文件系统）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data inpath '/student.csv' into table student;</span><br></pre></td></tr></table></figure></li></ol><h3 id="分区表的操作"><a href="#分区表的操作" class="headerlink" title="分区表的操作"></a><strong>分区表的操作</strong></h3><p>​    在大数据中，最常用的一种思想就是分治，我们可以把大的文件切割划分成一个个的小的文件，这样每次操作一个小的文件就会很容易了，同样的道理，在hive当中也是支持这种思想的，就是我们可以把大的数据，按照每月，或者天进行切分成一个个的小的文件,存放在不同的文件夹中.</p><p><strong>创建分区表语法</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> score(s_id <span class="keyword">string</span>,c_id <span class="keyword">string</span>, s_score <span class="built_in">int</span>) partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><p><strong>创建一个表带多个分区</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table score2 (s_id string,c_id string, s_score int) partitioned by (year string,month string,day string) row format delimited fields terminated by &#39;\t&#39;;</span><br></pre></td></tr></table></figure><p><strong>加载数据到分区表中</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/export/servers/hivedatas/score.csv'</span> <span class="keyword">into</span> <span class="keyword">table</span> score <span class="keyword">partition</span> (<span class="keyword">month</span>=<span class="string">'201806'</span>);</span><br></pre></td></tr></table></figure><p><strong>加载数据到多分区表中</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/export/servers/hivedatas/score.csv'</span> <span class="keyword">into</span> <span class="keyword">table</span> score2 <span class="keyword">partition</span>(<span class="keyword">year</span>=<span class="string">'2018'</span>,<span class="keyword">month</span>=<span class="string">'06'</span>,<span class="keyword">day</span>=<span class="string">'01'</span>);</span><br></pre></td></tr></table></figure><p><strong>多分区表联合查询（使用union all）</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> <span class="keyword">month</span> = <span class="string">'201806'</span> <span class="keyword">union</span> <span class="keyword">all</span> <span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> <span class="keyword">month</span> = <span class="string">'201806'</span>;</span><br></pre></td></tr></table></figure><p><strong>查看分区</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span>  <span class="keyword">partitions</span>  score;</span><br></pre></td></tr></table></figure><p><strong>添加一个分区</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> score <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201805'</span>);</span><br></pre></td></tr></table></figure><p><strong>删除分区</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> score <span class="keyword">drop</span> <span class="keyword">partition</span>(<span class="keyword">month</span> = <span class="string">'201806'</span>);</span><br></pre></td></tr></table></figure><h3 id="分区表综合练习"><a href="#分区表综合练习" class="headerlink" title="分区表综合练习"></a>分区表综合练习</h3><p><strong>需求描述：</strong></p><p> 现在有一个文件score.csv文件，存放在集群的这个目录下/scoredatas/month=201806，这个文件每天都会生成，存放到对应的日期文件夹下面去，文件别人也需要公用，不能移动。需求，创建hive对应的表，并将数据加载到表中，进行数据统计分析，且删除表之后，数据不能删除</p><p><strong>数据准备：</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p /scoredatas/month=201806</span><br><span class="line">hdfs dfs -put score.csv /scoredatas/month=201806/</span><br></pre></td></tr></table></figure><p><strong>创建外部分区表，并指定文件数据存放目录</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create external table score4(s_id string, c_id string,s_score int) partitioned by (month string) row format delimited fields terminated by '\t' location '/scoredatas';</span><br></pre></td></tr></table></figure><p><strong>进行表的修复(建立表与数据文件之间的一个关系映射)</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">msck repair table score4;</span><br></pre></td></tr></table></figure><p>之后便可select查询该表验证结果。</p><h3 id="分桶表操作"><a href="#分桶表操作" class="headerlink" title="分桶表操作"></a>分桶表操作</h3><p>分桶，就是将数据按照指定的字段进行划分到多个文件当中去,分桶就是MapReduce中的分区.</p><p><strong>开启 Hive 的分桶功能</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.enforce.bucketing=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure><p><strong>设置 Reduce 个数</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">3</span>;</span><br></pre></td></tr></table></figure><p><strong>创建分桶表</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> course (c_id <span class="keyword">string</span>,c_name <span class="keyword">string</span>,t_id <span class="keyword">string</span>) clustered <span class="keyword">by</span>(c_id) <span class="keyword">into</span> <span class="number">3</span> buckets <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><p><strong>创建普通表</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> course_common (c_id <span class="keyword">string</span>,c_name <span class="keyword">string</span>,t_id <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><p><strong>普通表中加载数据</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/export/servers/hivedatas/course.csv'</span> <span class="keyword">into</span> <span class="keyword">table</span> course_common;</span><br></pre></td></tr></table></figure><p><strong>通过insert overwrite给桶表中加载数据</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> course <span class="keyword">select</span> * <span class="keyword">from</span> course_common cluster <span class="keyword">by</span>(c_id);</span><br></pre></td></tr></table></figure><h3 id="修改表结构"><a href="#修改表结构" class="headerlink" title="修改表结构"></a>修改表结构</h3><p><strong>重命名</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span>  <span class="keyword">table</span>  old_table_name  <span class="keyword">rename</span>  <span class="keyword">to</span>  new_table_name;</span><br></pre></td></tr></table></figure><p>把表score4修改成score5</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> score4 <span class="keyword">rename</span> <span class="keyword">to</span> score5;</span><br></pre></td></tr></table></figure><p><strong>增加/修改列信息:</strong></p><ul><li>查询表结构</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc score5;</span><br></pre></td></tr></table></figure><ul><li>添加列</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> score5 <span class="keyword">add</span> <span class="keyword">columns</span> (mycol <span class="keyword">string</span>, mysco <span class="built_in">int</span>);</span><br></pre></td></tr></table></figure><ul><li>更新列</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> score5 <span class="keyword">change</span> <span class="keyword">column</span> mysco mysconew <span class="built_in">int</span>;</span><br></pre></td></tr></table></figure><ul><li>删除表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> score5;</span><br></pre></td></tr></table></figure><h2 id="Hive表中加载数据"><a href="#Hive表中加载数据" class="headerlink" title="Hive表中加载数据"></a>Hive表中加载数据</h2><p><strong>直接向分区表中插入数据</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> score3 <span class="keyword">like</span> score;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> score3 <span class="keyword">partition</span>(<span class="keyword">month</span> =<span class="string">'201807'</span>) <span class="keyword">values</span> (<span class="string">'001'</span>,<span class="string">'002'</span>,<span class="string">'100'</span>);</span><br></pre></td></tr></table></figure><p><strong>通过load方式加载数据</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/export/servers/hivedatas/score.csv'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> score <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201806'</span>);</span><br></pre></td></tr></table></figure><p><strong>通过查询方式加载数据</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> score4 <span class="keyword">like</span> score;</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> score4 <span class="keyword">partition</span>(<span class="keyword">month</span> = <span class="string">'201806'</span>) <span class="keyword">select</span> s_id,c_id,s_score <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇总结Hive的交互方式和&lt;strong&gt;基本操作命令&lt;/strong&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://iceWind-R.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://iceWind-R.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据_08(Hive介绍和安装)</title>
    <link href="http://icewind-r.github.io/2020/08/21/%E5%A4%A7%E6%95%B0%E6%8D%AE-08/"/>
    <id>http://icewind-r.github.io/2020/08/21/%E5%A4%A7%E6%95%B0%E6%8D%AE-08/</id>
    <published>2020-08-21T13:52:07.000Z</published>
    <updated>2020-08-22T03:04:39.701Z</updated>
    
    <content type="html"><![CDATA[<p>本篇总结Hive（数据仓库）数据仓库和 Hive 的基本概念。</p><a id="more"></a><hr><h1 id="数据仓库"><a href="#数据仓库" class="headerlink" title="数据仓库"></a>数据仓库</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>英文名称为 Data Warehouse，可简写为 DW 或 DWH。数据仓库的目的是构建面相分析的集成化数据环境，为企业提供决策支持（Decision Support）。</p><p>​    数据仓库是存数据的，企业的各种数据往里面存，主要目的是为了分析有效数据，后续会基于它产出供分析挖掘的数据，或者数据应用需要的数据，如企业的分析性报告和各类报表等。</p><p>​    可以理解为：<strong>面向分析的存储系统。</strong></p><h2 id="主要特征"><a href="#主要特征" class="headerlink" title="主要特征"></a>主要特征</h2><p>数据仓库是面向主题的（Subject-Oriented）、集成的（Integrated）、非易失的（Non-Volatile）和时变的（Time-Variant）数据集合，用以支持管理决策。</p><h3 id="面向主题"><a href="#面向主题" class="headerlink" title="面向主题"></a>面向主题</h3><p>数据仓库是面向主题的,数据仓库通过一个个主题域将多个业务系统的数据加载到一起，为了各个主题（如：用户、订单、商品等）进行分析而建，操作型数据库是为了支撑各种业务而建立。</p><h3 id="集成性"><a href="#集成性" class="headerlink" title="集成性"></a>集成性</h3><p>数据仓库会将不同源数据库中的数据汇总到一起,数据仓库中的综合数据不能从原有的数据库系统直接得到。因此在数据进入数据仓库之前，必然要经过统一与整合，这一步是数据仓库建设中最关键、最复杂的一步(ETL)，要统一源数据中所有矛盾之处，如字段的同名异义、异名同义、单位不统一、字长不一致，等等。</p><h3 id="非易失性"><a href="#非易失性" class="headerlink" title="非易失性"></a>非易失性</h3><p>操作型数据库主要服务于日常的业务操作，使得数据库需要不断地对数据实时更新，以便迅速获得当前最新数据，不至于影响正常的业务运作。</p><p>​    在数据仓库中只要保存过去的业务数据，不需要每一笔业务都实时更新数据仓库，而是根据商业需要每隔一段时间把一批较新的数据导入数据仓库。 数据仓库的数据反映的是一段相当长的时间内历史数据的内容，是不同时点的数据库的集合，以及基于这些快照进行统计、综合和重组的导出数据。数据仓库中的数据一般仅执行查询操作，很少会有删除和更新。但是需定期加载和刷新数据。</p><h3 id="时变性"><a href="#时变性" class="headerlink" title="时变性"></a>时变性</h3><p>数据仓库包含各种粒度的历史数据。数据仓库中的数据可能与某个特定日期、星期、月份、季度或者年份有关。数据仓库的目的是通过分析企业过去一段时间业务的经营状况，挖掘其中隐藏的模式。虽然数据仓库的用户不能修改数据，但并不是说数据仓库的数据是永远不变的。分析的结果只能反映过去的情况，当业务变化后，挖掘出的模式会失去时效性。因此数据仓库的数据需要定时更新，以适应决策的需要。</p><h2 id="数据库与数据仓库的区别"><a href="#数据库与数据仓库的区别" class="headerlink" title="数据库与数据仓库的区别"></a>数据库与数据仓库的区别</h2><p>​    数据库与数据仓库的区别实际讲的是 <code>OLTP</code> 与 <code>OLAP</code> 的区别。</p><p>​    操作型处理，叫联机事务处理 OLTP（On-Line Transaction Processing，），也可以称面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性和并发支持的用户数等问题。传统的数据库系统作为数据管理的主要手段，主要用于操作型处理。</p><p>​    分析型处理，叫联机分析处理 OLAP（On-Line Analytical Processing）一般针对某些主题的历史数据进行分析，支持 管理决策。</p><p>首先要明白，数据仓库的出现，并不是要取代数据库。</p><ul><li>数据库是面向事务的设计，数据仓库是面向主题设计的。</li><li>数据库一般存储业务数据，数据仓库存储的一般是历史数据。</li><li>数据库设计是尽量避免冗余，一般针对某一业务应用进行设计，比如一张简单的User表，记录用户名、密码等简单数据即可，符合业务应用，但是不符合分析。数据仓库在设计是有意引入冗余，依照分析需求，分析维度、分析指标进行设计。</li><li>数据库是为捕获数据而设计，数据仓库是为分析数据而设计。</li></ul><p>​    <strong>数据仓库，是在数据库已经大量存在的情况下，为了进一步挖掘数据资源、为了决策需要而产生的，它决不是所谓的“大型数据库”。</strong></p><h2 id="数仓的分层架构"><a href="#数仓的分层架构" class="headerlink" title="数仓的分层架构"></a>数仓的分层架构</h2><p>按照数据流入流出的过程，数据仓库架构可分为三层——源数据、数据仓库、数据应用。</p><img src="/2020/08/21/%E5%A4%A7%E6%95%B0%E6%8D%AE-08/1.jpg" class><p>数据仓库的数据来源于不同的源数据，并提供多样的数据应用，数据自下而上流入数据仓库后向上层开放应用，而数据仓库只是中间集成化数据管理的一个平台。</p><ul><li><code>源数据层（ODS）</code>：此层数据无任何更改，直接沿用外围系统数据结构和数据，不对外开放；为临时存储层，是接口数据的临时存储区域，为后一步的数据处理做准备。</li><li><code>数据仓库层（DW）</code>：也称为细节层，DW层的数据应该是一致的、准确的、干净的数据，即对源系统数据进行了清洗（去除了杂质）后的数据。</li><li><code>数据应用层（DA或APP）</code>：前端应用直接读取的数据源；根据报表、专题分析需求而计算生成的数据。</li></ul><p>​    数据仓库从各数据源获取数据及在数据仓库内的数据转换和流动都可以认为是ETL（抽取Extra, 转化Transfer, 装载Load）的过程，ETL是数据仓库的流水线，也可以认为是数据仓库的血液，它维系着数据仓库中数据的新陈代谢，而数据仓库日常的管理和维护工作的大部分精力就是保持ETL的正常和稳定。</p><p><strong>为什么要对数据仓库分层？</strong></p><p>​    用空间换时间，通过大量的预处理来提升应用系统的用户体验（效率），因此数据仓库会存在大量冗余的数据；不分层的话，如果源业务系统的业务规则发生变化将会影响整个数据清洗过程，工作量巨大。</p><p>​    通过数据分层管理可以简化数据清洗的过程，因为把原来一步的工作分到了多个步骤去完成，相当于把一个复杂的工作拆成了多个简单的工作，把一个大的黑盒变成了一个白盒，每一层的处理逻辑都相对简单和容易理解，这样我们比较容易保证每一个步骤的正确性，当数据发生错误的时候，往往我们只需要局部调整某个步骤即可。</p><h2 id="数仓的元数据管理"><a href="#数仓的元数据管理" class="headerlink" title="数仓的元数据管理"></a>数仓的元数据管理</h2><p>​    元数据（Meta Date），主要记录数据仓库中模型的定义、各层级间的映射关系、监控数据仓库的数据状态及ETL的任务运行状态。一般会通过元数据资料库（Metadata Repository）来统一地存储和管理元数据，其主要目的是使数据仓库的设计、部署、操作和管理能达成协同和一致。</p><p>​    元数据是数据仓库管理系统的重要组成部分，元数据管理是企业级数据仓库中的关键组件，贯穿数据仓库构建的整个过程，直接影响着数据仓库的构建、使用和维护。</p><ul><li>构建数据仓库的主要步骤之一是ETL。这时元数据将发挥重要的作用，它定义了源数据系统到数据仓库的映射、数据转换的规则、数据仓库的逻辑结构、数据更新的规则、数据导入历史记录以及装载周期等相关内容。数据抽取和转换的专家以及数据仓库管理员正是通过元数据高效地构建数据仓库。</li><li>用户在使用数据仓库时，通过元数据访问数据，明确数据项的含义以及定制报表。</li><li>数据仓库的规模及其复杂性离不开正确的元数据管理，包括增加或移除外部数据源，改变数据清洗方法，控制出错的查询以及安排备份等。</li></ul><img src="/2020/08/21/%E5%A4%A7%E6%95%B0%E6%8D%AE-08/2.jpg" class><p>元数据可分为技术元数据和业务元数据。技术元数据为开发和管理数据仓库的IT 人员使用，它描述了与数据仓库开发、管理和维护相关的数据，包括数据源信息、数据转换描述、数据仓库模型、数据清洗与更新规则、数据映射和访问权限等。而业务元数据为管理层和业务分析人员服务，从业务角度描述数据，包括商务术语、数据仓库中有什么数据、数据的位置和数据的可用性等，帮助业务人员更好地理解数据仓库中哪些数据是可用的以及如何使用。</p><p>​    由上可见，元数据不仅定义了数据仓库中数据的模式、来源、抽取和转换规则等，而且是整个数据仓库系统运行的基础，元数据把数据仓库系统中各个松散的组件联系起来，组成了一个有机的整体。</p><h1 id="Hive-的基本概念"><a href="#Hive-的基本概念" class="headerlink" title="Hive 的基本概念"></a>Hive 的基本概念</h1><h2 id="Hive-简介"><a href="#Hive-简介" class="headerlink" title="Hive 简介"></a>Hive 简介</h2><p>​    Hive是基于Hadoop的一个数据仓库工具，可以将<strong>结构化的数据(有固定的字段，字段之间有固定的分隔符)</strong>文件映射为一张数据库表，并提供类SQL查询功能。</p><p>​    其本质是将SQL转换为MapReduce的任务进行运算，底层由HDFS来提供数据的存储，说白了hive可以理解为一个将SQL转换为MapReduce的任务的工具，甚至更进一步可以说hive就是一个MapReduce的客户端</p><p><strong>为什么使用 Hive</strong></p><ul><li>采用类SQL语法去操作数据，提供快速开发的能力。</li><li>避免了去写MapReduce，减少开发人员的学习成本。</li><li>功能扩展很方便。</li></ul><h2 id="Hive-架构"><a href="#Hive-架构" class="headerlink" title="Hive 架构"></a>Hive 架构</h2><img src="/2020/08/21/%E5%A4%A7%E6%95%B0%E6%8D%AE-08/3.jpg" class><ul><li><strong>用户接口：</strong> 包括CLI、JDBC/ODBC、WebGUI。其中，CLI(command line interface)为shell命令行；JDBC/ODBC是Hive的JAVA实现，与传统数据库JDBC类似；WebGUI是通过浏览器访问Hive。</li><li><strong>元数据存储：</strong> 通常是存储在关系数据库如mysql/derby中。Hive 将元数据存储在数据库中。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。</li><li><strong>解释器、编译器、优化器、执行器:</strong> 完成HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在HDFS 中，并在随后有MapReduce 调用执行。</li></ul><h2 id="Hive-与-Hadoop-的关系"><a href="#Hive-与-Hadoop-的关系" class="headerlink" title="Hive 与 Hadoop 的关系"></a>Hive 与 Hadoop 的关系</h2><p>Hive利用HDFS存储数据，利用MapReduce查询分析数据</p><img src="/2020/08/21/%E5%A4%A7%E6%95%B0%E6%8D%AE-08/4.jpg" class><h2 id="Hive与传统数据库对比"><a href="#Hive与传统数据库对比" class="headerlink" title="Hive与传统数据库对比"></a>Hive与传统数据库对比</h2><p>hive用于海量数据的离线数据分析</p><img src="/2020/08/21/%E5%A4%A7%E6%95%B0%E6%8D%AE-08/5.jpg" class><p><strong>总结：</strong>hive具有sql数据库的外表，但应用场景完全不同，hive只适合用来做批量数据统计分析</p><h1 id="Hive-的安装和配置"><a href="#Hive-的安装和配置" class="headerlink" title="Hive 的安装和配置"></a>Hive 的安装和配置</h1><h2 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h2><h3 id="下载Hive"><a href="#下载Hive" class="headerlink" title="下载Hive"></a>下载Hive</h3><ul><li><p>下载Hive安装包：这里我们选用hive的版本是 <strong>2.1.1</strong> 下载地址为：<a href="http://archive.apache.org/dist/hive/hive-2.1.1/apache-hive-2.1.1-bin.tar.gz" target="_blank" rel="noopener">http://archive.apache.org/dist/hive/hive-2.1.1/apache-hive-2.1.1-bin.tar.gz</a></p></li><li><p>解压到相应安装目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/softwares/</span><br><span class="line">tar -zxvf apache-hive-2.1.1-bin.tar.gz -C ../servers/</span><br></pre></td></tr></table></figure></li></ul><h3 id="安装MySQL"><a href="#安装MySQL" class="headerlink" title="安装MySQL"></a>安装MySQL</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">wget http://repo.mysql.com/mysql57-community-release-el7-9.noarch.rpm</span><br><span class="line">sudo rpm -ivh mysql57-community-release-el7-9.noarch.rpm</span><br><span class="line"></span><br><span class="line">yum install mysql mysql-server</span><br><span class="line"></span><br><span class="line">mysql_secure_installation</span><br><span class="line"></span><br><span class="line">grant all privileges on *.* to 'root'@'%' identified by '123456' with grant option;</span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure><h3 id="修改hive的配置文件"><a href="#修改hive的配置文件" class="headerlink" title="修改hive的配置文件"></a>修改hive的配置文件</h3><p><strong>修改hive-env.sh</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/apache-hive-2.1.1-bin/conf</span><br><span class="line">cp hive-env.sh.template hive-env.sh</span><br></pre></td></tr></table></figure><p>修改内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_HOME=/export/servers/hadoop-2.7.5</span><br><span class="line">export HIVE_CONF_DIR=/export/servers/apache-hive-2.1.1-bin/conf</span><br></pre></td></tr></table></figure><p><strong>修改hive-site.xml</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/apache-hive-2.1.1-bin/conf</span><br><span class="line">vim hive-site.xml</span><br></pre></td></tr></table></figure><p>修改内容</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://node03:3306/hive?createDatabaseIfNotExist=true<span class="symbol">&amp;amp;</span>useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>datanucleus.schema.autoCreateAll<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>node03<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="添加mysql的连接驱动包到hive的lib目录下"><a href="#添加mysql的连接驱动包到hive的lib目录下" class="headerlink" title="添加mysql的连接驱动包到hive的lib目录下"></a>添加mysql的连接驱动包到hive的lib目录下</h3><p>将mysql 的 驱动 mysql-connector-java-5.1.18-bin.jar 添加 hive的lib目录下即可。</p><h3 id="配置hive的环境变量"><a href="#配置hive的环境变量" class="headerlink" title="配置hive的环境变量"></a>配置hive的环境变量</h3><p> 在 /etc/profile 中添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_HOME=/export/servers/apache-hive-2.1.1-bin    (安装目录)</span><br><span class="line">export PATH=:$HIVE_HOME/bin:$PATH</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇总结Hive（数据仓库）数据仓库和 Hive 的基本概念。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://iceWind-R.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://iceWind-R.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据_07(Yarn)</title>
    <link href="http://icewind-r.github.io/2020/08/20/%E5%A4%A7%E6%95%B0%E6%8D%AE-07/"/>
    <id>http://icewind-r.github.io/2020/08/20/%E5%A4%A7%E6%95%B0%E6%8D%AE-07/</id>
    <published>2020-08-19T23:58:59.000Z</published>
    <updated>2020-08-21T13:53:14.437Z</updated>
    
    <content type="html"><![CDATA[<p>本篇总结<strong>Yarn资源调度</strong>的基础知识。</p><a id="more"></a><hr><h1 id="Yarn介绍"><a href="#Yarn介绍" class="headerlink" title="Yarn介绍"></a>Yarn介绍</h1><p>Apache Hadoop YARN （Yet Another Resource Negotiator，另一种资源协调者）是一 种新的 Hadoop 资源管理器，它是一个通用资源管理系统和调度平台，可为上层应用提供统 一的资源管理和调度，它的引入为集群在利用率、资源统一管理和数据共享等方面带来了巨大好处。</p><p>Yarn核心出发点是为了分离资源管理与作业监控，实现分离的做法是拥有一个全局的资源管理（ResourceManager，RM），以及每个应用程序对应一个的应用管理器（ApplicationMaster，AM）</p><p>总结一句话就是：<strong>Yarn就是为了调度资源，管理任务的</strong> 。</p><p>其调度分为两个层次：</p><ul><li>一级调度管理：计算资源管理（CPU，内存，网络IO，磁盘）</li><li>二级调度管理：任务内部的计算模型管理（AppMaster的任务精细化管理）</li></ul><h1 id="Yarn的主要组件介绍和作用"><a href="#Yarn的主要组件介绍和作用" class="headerlink" title="Yarn的主要组件介绍和作用"></a>Yarn的主要组件介绍和作用</h1><p>Yarn总体上是Master/Slave结构，主要由ResourceManager、NodeManager、ApplicationMaster和Container等几个组件构成。</p><img src="/2020/08/20/%E5%A4%A7%E6%95%B0%E6%8D%AE-07/1.gif" class><h2 id="1-ResourceManager"><a href="#1-ResourceManager" class="headerlink" title="1. ResourceManager"></a>1. ResourceManager</h2><p>每个Hadoop集群只会有一个ResourceManager（如果是HA的话会存在两个，但是有且只有一个处于active状态），它负责管理整个集群的计算资源，并将这些资源分别给应用程序。ResourceManager 内部主要有<strong>两个组件</strong>：</p><ol><li><strong>调度器</strong>（Scheduler）:负责资源的 分配。</li><li><strong>应用程序管理器</strong>  ApplicationsManager (AsM):这个组件用于管理整个集群应用程序的application masters，负责接收应用程序的提交；为application master启动提供资源；监控应用程序的运行进度以及在应用程序出现故障时重启它。</li></ol><h2 id="2-NodeManager"><a href="#2-NodeManager" class="headerlink" title="2. NodeManager"></a>2. NodeManager</h2><p>NodeManager是YARN中每个节点上的代理，它管理Hadoop集群中单个计算节点，根据相关的设置来启动容器的。NodeManager会定期向ResourceManager发送心跳信息来更新其健康状态。同时其也会监督Container的生命周期管理，监控每个Container的资源使用（内存、CPU等）情况，追踪节点健康状况，管理日志和不同应用程序用到的附属服务（auxiliary service）。</p><h2 id="3-ApplicationMaster"><a href="#3-ApplicationMaster" class="headerlink" title="3. ApplicationMaster"></a>3. ApplicationMaster</h2><p>ApplicationMaster是应用程序级别的，每个ApplicationMaster管理运行在YARN上的应用程序。YARN 将 ApplicationMaster看做是第三方组件，ApplicationMaster负责和ResourceManager scheduler协商资源，并且和NodeManager通信来运行相应的task。ResourceManager 为 ApplicationMaster 分配容器，这些容器将会用来运行task。ApplicationMaster 也会追踪应用程序的状态，监控容器的运行进度。当容器运行完成， ApplicationMaster 将会向 ResourceManager 注销这个容器；如果是整个作业运行完成，其也会向 ResourceManager 注销自己，这样这些资源就可以分配给其他的应用程序使用了。</p><h2 id="4-Container"><a href="#4-Container" class="headerlink" title="4. Container"></a>4. Container</h2><p>是Yarn中的资源抽象。Container是与特定节点绑定的，其包含了内存、CPU磁盘等逻辑资源。不过在现在的容器实现中，这些资源只包括了内存和CPU。容器是由 ResourceManager scheduler 服务动态分配的资源构成。容器授予 ApplicationMaster 使用特定主机的特定数量资源的权限。ApplicationMaster 也是在容器中运行的，其在应用程序分配的第一个容器中运行。</p><p>作业的完整运行如下所示：</p><img src="/2020/08/20/%E5%A4%A7%E6%95%B0%E6%8D%AE-07/2.jpeg" class>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇总结&lt;strong&gt;Yarn资源调度&lt;/strong&gt;的基础知识。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://iceWind-R.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="http://iceWind-R.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>大数据_06(MapReduce 的 Shuffle 详解（分区、排序、规约、分组）)</title>
    <link href="http://icewind-r.github.io/2020/08/13/%E5%A4%A7%E6%95%B0%E6%8D%AE-06/"/>
    <id>http://icewind-r.github.io/2020/08/13/%E5%A4%A7%E6%95%B0%E6%8D%AE-06/</id>
    <published>2020-08-13T02:25:03.000Z</published>
    <updated>2020-08-20T00:05:27.823Z</updated>
    
    <content type="html"><![CDATA[<p>本篇总结MapReduce<strong>分区、排序、规约</strong>的基础知识。</p><a id="more"></a><hr><h1 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h1><p>在 MapReduce 中, 通过我们指定分区, 会将同一个分区的数据发送到同一个 Reduce 当中进行处理</p><p>​    例如: 为了数据的统计, 可以把一批类似的数据发送到同一个 Reduce 当中, 在同一个 Reduce 当中统计相同类型的数据, 就可以实现类似的数据分区和统计等</p><p>​    其实就是相同类型的数据, 有共性的数据, 送到一起去处理</p><p>​    Reduce 当中默认的分区只有一个</p><h2 id="Mapper"><a href="#Mapper" class="headerlink" title="Mapper"></a>Mapper</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> partition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Counter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">* K1：行偏移量 LongWritable</span></span><br><span class="line"><span class="comment">* V1:行文本数据 Text</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">* K2:行文本数据 Text</span></span><br><span class="line"><span class="comment">* V2:NullWritable</span></span><br><span class="line"><span class="comment">* */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PartitionMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>,<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">// map 方法：将 K1 V1 转为 K2 V2</span></span><br><span class="line">    <span class="comment">// K2 就是 V1</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 方法1：定义计数器</span></span><br><span class="line">        <span class="comment">// 参数： 计数器类型，  计数器变量</span></span><br><span class="line">        Counter counter = context.getCounter(<span class="string">"MY_COUNTER"</span>, <span class="string">"partition_counter"</span>);</span><br><span class="line">        <span class="comment">//每次执行map方法，计数器被执行，1L 表示每次执行 +1</span></span><br><span class="line">        counter.increment(<span class="number">1L</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        context.write(value,NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Reducer"><a href="#Reducer" class="headerlink" title="Reducer"></a>Reducer</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PartitionerReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>,<span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;NullWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        context.write(key, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Partitioner"><a href="#Partitioner" class="headerlink" title="Partitioner"></a>Partitioner</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> partition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * 定义分区规则</span></span><br><span class="line"><span class="comment">    * 返回对应分区编号</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text text, NullWritable nullWritable, <span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 拆分行文本数据(K2),获取中奖字段值</span></span><br><span class="line">        String[] split = text.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">        String numStr = split[<span class="number">5</span>];</span><br><span class="line">        <span class="comment">// 判断中奖字段值和15的关系，然后返回对应的分区编号</span></span><br><span class="line">        <span class="keyword">if</span> (Integer.parseInt(numStr) &gt; <span class="number">15</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="JobMain"><a href="#JobMain" class="headerlink" title="JobMain"></a>JobMain</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> partition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobMain</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 创建job任务对象</span></span><br><span class="line">        Job job = Job.getInstance(<span class="keyword">super</span>.getConf(), <span class="string">"PartitionMapReduce"</span>);</span><br><span class="line">        <span class="comment">// 对job任务进行配置(8个步骤)</span></span><br><span class="line">        <span class="comment">// 1、设置输入类和输入的路径</span></span><br><span class="line">        job.setInputFormatClass(TextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        TextInputFormat.addInputPath(job,<span class="keyword">new</span> Path(<span class="string">"hdfs://bigdata1:8020/input"</span>));</span><br><span class="line">        TextInputFormat.addInputPath(job,<span class="keyword">new</span> Path(<span class="string">"file:///D:\\input"</span>));</span><br><span class="line">        <span class="comment">//2、设置mapper类和数据类型</span></span><br><span class="line">        job.setMapperClass(PartitionMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">//3、指定分区类</span></span><br><span class="line">        job.setPartitionerClass(MyPartitioner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">//4、5、6</span></span><br><span class="line">        <span class="comment">//7、指定reducer类和数据类型（K3 和 V3）</span></span><br><span class="line">        job.setReducerClass(PartitionerReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">// 设置reduceTask的个数</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//8、指定输出类和输出类型</span></span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        TextOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"hdfs://bigdata1:8020/out/partition_out"</span>));</span><br><span class="line">        TextOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"file:///D:\\out\\partition_out"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">// 启动job任务</span></span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(configuration, <span class="keyword">new</span> JobMain(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h2><p>其中在 mapper 和 reducer 里添加了计数器的相关使用，得到结果如下。</p><img src="/2020/08/13/%E5%A4%A7%E6%95%B0%E6%8D%AE-06/1.png" class><img src="/2020/08/13/%E5%A4%A7%E6%95%B0%E6%8D%AE-06/2.png" class><h1 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h1><ul><li>序列化 (Serialization) 是指把结构化对象转化为字节流</li><li>反序列化 (Deserialization) 是序列化的逆过程. 把字节流转为结构化对象. 当要在进程间传递对象或持久化对象的时候, 就需要序列化对象成字节流, 反之当要将接收到或从磁盘读取的字节流转换为对象, 就要进行反序列化</li><li>Java 的序列化 (Serializable) 是一个重量级序列化框架, 一个对象被序列化后, 会附带很多额外的信息 (各种校验信息, header, 继承体系等）, 不便于在网络中高效传输. 所以, Hadoop 自己开发了一套序列化机制(Writable), 精简高效. 不用像 Java 对象类一样传输多层的父子关系, 需要哪个属性就传输哪个属性值, 大大的减少网络传输的开销</li><li>Writable 是 Hadoop 的序列化格式, Hadoop 定义了这样一个 Writable 接口. 一个类要支持可序列化只需实现这个接口即可</li><li>另外 Writable 有一个子接口是 WritableComparable, WritableComparable 是既可实现序列化, 也可以对key进行比较, 我们这里可以通过自定义 Key 实现 WritableComparable 来实现我们的排序功能</li></ul><p>数据格式如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a   1</span><br><span class="line">a   9</span><br><span class="line">b   3</span><br><span class="line">a   7</span><br><span class="line">b   8</span><br><span class="line">b   10</span><br><span class="line">a   5</span><br></pre></td></tr></table></figure><p>要求:</p><ul><li>第一列按照字典顺序进行排列</li><li>第一列相同的时候, 第二列按照升序进行排列</li></ul><p>解决思路:</p><ul><li>将 Map 端输出的 <code>&lt;key,value&gt;</code> 中的 key 和 value 组合成一个新的 key (newKey), value值不变</li><li>这里就变成 <code>&lt;(key,value),value&gt;</code>, 在针对 newKey 排序的时候, 如果 key 相同, 就再对value进行排序</li></ul><h2 id="自定义类型和比较器"><a href="#自定义类型和比较器" class="headerlink" title="自定义类型和比较器"></a>自定义类型和比较器</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.cpz.mapreduce.sort;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> com.sun.xml.internal.fastinfoset.algorithm.BuiltInEncodingAlgorithm;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SortBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">SortBean</span>&gt; </span>&#123;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">private</span> String word;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> num;</span><br><span class="line"> </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span>  word + <span class="string">"\t"</span> + num ;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getWord</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> word;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setWord</span><span class="params">(String word)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.word = word;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getNum</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> num;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setNum</span><span class="params">(<span class="keyword">int</span> num)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.num = num;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 实现比较器，指定排序的规则</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    规则：</span></span><br><span class="line"><span class="comment">        第一列（word）按照字典顺序进行排列     //字典顺序：aac aad abc (按照ASCII码相减)</span></span><br><span class="line"><span class="comment">        第一列相同的私事后，第二列（num）按照升序进行排列</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(SortBean sortBean)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// // 先对第一列排序：word排序</span></span><br><span class="line">        <span class="keyword">int</span> result = <span class="keyword">this</span>.word.compareTo(sortBean.word);</span><br><span class="line">        <span class="comment">// 如果第一列相同，则按照第二列进行排序</span></span><br><span class="line">        <span class="keyword">if</span> (result == <span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">this</span>.num - sortBean.num;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 实现序列化</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dataOutput.writeUTF(word);</span><br><span class="line">        dataOutput.writeInt(num);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 实现反序列化</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.word = dataInput.readUTF();</span><br><span class="line">        <span class="keyword">this</span>.num = dataInput.readInt();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Mapper-1"><a href="#Mapper-1" class="headerlink" title="Mapper"></a>Mapper</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SortMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>,<span class="title">Text</span>,<span class="title">SortBean</span>,<span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 将行文本数据（V1)拆分，并将数据封装到 SortBean 对象，就可以得到K2</span></span><br><span class="line">        String[] split = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line"> </span><br><span class="line">        SortBean sortBean = <span class="keyword">new</span> SortBean();</span><br><span class="line">        sortBean.setWord(split[<span class="number">0</span>]);</span><br><span class="line">        sortBean.setNum(Integer.parseInt(split[<span class="number">1</span>]));</span><br><span class="line"> </span><br><span class="line">        <span class="comment">// 将K2 V2 写入上下文中</span></span><br><span class="line">        context.write(sortBean,NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Reducer-1"><a href="#Reducer-1" class="headerlink" title="Reducer"></a>Reducer</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SortReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">SortBean</span>,<span class="title">NullWritable</span>,<span class="title">SortBean</span>,<span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(SortBean key, Iterable&lt;NullWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        context.write(key,NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="JobMain-1"><a href="#JobMain-1" class="headerlink" title="JobMain"></a>JobMain</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobMain</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"> </span><br><span class="line">        Job job = Job.getInstance(<span class="keyword">super</span>.getConf(), <span class="string">"mapreduce_sort"</span>);</span><br><span class="line"> </span><br><span class="line">        job.setInputFormatClass(TextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        TextInputFormat.setInputPaths(job,<span class="keyword">new</span> Path(<span class="string">"d:\\mapreduce\\sort_in"</span>));</span><br><span class="line"> </span><br><span class="line">        job.setMapperClass(SortMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputKeyClass(SortBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"> </span><br><span class="line">        <span class="comment">// 排序：不需要配置，只需要定义排序规则（SortBean中的排序规则）</span></span><br><span class="line"> </span><br><span class="line">        job.setReducerClass(SortReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputKeyClass(SortBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"> </span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        TextOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(<span class="string">"d:\\mapreduce\\sort_out"</span>));</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">boolean</span> bl = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">return</span> bl ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(configuration, <span class="keyword">new</span> JobMain(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="规约"><a href="#规约" class="headerlink" title="规约"></a>规约</h1><p><strong>概念：</strong></p><p>​    每一个 map 都可能会产生大量的本地输出，Combiner 的作用就是对 map 端的输出先做一次合并，以减少在 map 和 reduce 节点之间的数据传输量，以提高网络IO 性能，是 MapReduce 的一种优化手段之一</p><ul><li>combiner 是 MR 程序中 Mapper 和 Reducer 之外的一种组件</li><li>combiner 组件的父类就是 Reducer</li><li>combiner 和 reducer 的区别在于运行的位置<ul><li>Combiner 是在每一个 maptask 所在的节点运行</li><li>Reducer 是接收全局所有 Mapper 的输出结果</li></ul></li><li>combiner 的意义就是对每一个 maptask 的输出进行局部汇总，以减小网络传输量</li></ul><p><strong>实现步骤：</strong></p><ol><li>自定义一个 combiner 继承 Reducer，重写 reduce 方法（与之前步骤几乎一致）</li><li>在 job 中设置 <code>job.setCombinerClass(CustomCombiner.class)</code></li></ol><p>Combiner 能够应用的前提是不能影响最终的业务逻辑，而且，combiner 的输出 kv 应该跟 reducer 的输入 kv 类型要对应起来</p><h2 id="Mapper-2"><a href="#Mapper-2" class="headerlink" title="Mapper"></a>Mapper</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CombinerMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>,<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">LongWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        Text text = <span class="keyword">new</span> Text();</span><br><span class="line">        LongWritable longWritable = <span class="keyword">new</span> LongWritable();</span><br><span class="line">        String[] split = value.toString().split(<span class="string">","</span>);</span><br><span class="line">        <span class="keyword">for</span> (String s : split) &#123;</span><br><span class="line">            text.set(s);</span><br><span class="line">            longWritable.set(<span class="number">1</span>);</span><br><span class="line">            context.write(text,longWritable);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="MyCombiner"><a href="#MyCombiner" class="headerlink" title="MyCombiner"></a>MyCombiner</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyCombiner</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>,<span class="title">LongWritable</span>,<span class="title">Text</span>,<span class="title">LongWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;LongWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (LongWritable value : values) &#123;</span><br><span class="line">            count += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        context.write(key,<span class="keyword">new</span> LongWritable(count));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Reducer-2"><a href="#Reducer-2" class="headerlink" title="Reducer"></a>Reducer</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CombinerReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>,<span class="title">LongWritable</span>,<span class="title">Text</span>,<span class="title">LongWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;LongWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (LongWritable value : values) &#123;</span><br><span class="line">            count += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        context.write(key,<span class="keyword">new</span> LongWritable(count));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="JobMain-2"><a href="#JobMain-2" class="headerlink" title="JobMain"></a>JobMain</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobMain</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"> </span><br><span class="line">        Job job = Job.getInstance(<span class="keyword">super</span>.getConf(), <span class="string">"mapreduce_combiner"</span>);</span><br><span class="line"> </span><br><span class="line">        job.setInputFormatClass(TextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        TextInputFormat.setInputPaths(job,<span class="keyword">new</span> Path(<span class="string">"d:\\mapreduce\\combiner_in"</span>));</span><br><span class="line"> </span><br><span class="line">        job.setMapperClass(CombinerMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(LongWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"> </span><br><span class="line">        job.setCombinerClass(MyCombiner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"> </span><br><span class="line">        job.setReducerClass(CombinerReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(LongWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"> </span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        TextOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(<span class="string">"d:\\mapreduce\\combiner_out"</span>));</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">boolean</span> bl = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">return</span> bl ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(configuration, <span class="keyword">new</span> JobMain(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇总结MapReduce&lt;strong&gt;分区、排序、规约&lt;/strong&gt;的基础知识。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://iceWind-R.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="http://iceWind-R.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>大数据_05(MapReduce基础和入门案例(单词统计))</title>
    <link href="http://icewind-r.github.io/2020/08/11/%E5%A4%A7%E6%95%B0%E6%8D%AE-05/"/>
    <id>http://icewind-r.github.io/2020/08/11/%E5%A4%A7%E6%95%B0%E6%8D%AE-05/</id>
    <published>2020-08-11T09:30:52.000Z</published>
    <updated>2020-10-23T03:59:12.504Z</updated>
    
    <content type="html"><![CDATA[<p>本篇总结MapReduce的入门基础知识。</p><a id="more"></a><hr><h1 id="MapReduce介绍"><a href="#MapReduce介绍" class="headerlink" title="MapReduce介绍"></a>MapReduce介绍</h1><p>MapReduce的核心思想是“<strong>分而治之</strong>”，适用于大量复杂的任务处理场景。</p><ul><li><p>Map负责“分”，即把复杂的任务分解为若干“简单的任务”来并行处理。可以进行拆分的前提是这些小任务可以并行计算，彼此间几乎没有依赖关系。</p></li><li><p>Reduce负责“合”，即对Map阶段的结果进行全局汇总。</p></li><li><p>MapReduce运行在Yarn集群，是一个“主从” 结构。</p><ol><li>ResourceManager</li><li>NodeManager</li></ol><p>这两个阶段合起来正是MapReduce思想的体现。</p></li></ul><img src="/2020/08/11/%E5%A4%A7%E6%95%B0%E6%8D%AE-05/1.png" class><h2 id="MapReduce设计构思"><a href="#MapReduce设计构思" class="headerlink" title="MapReduce设计构思"></a>MapReduce设计构思</h2><p>MapReduce是一个分布式运算程序的编程框架，核心功能是将<strong>用户编写的业务逻辑代码</strong>和<strong>自带默认组件</strong>整合成一个完整的分布式运算程序，并发运行在Hadoop集群上。</p><p>​    MapReduce设计并提供了统一的计算框架，为程序员隐藏了绝大多数系统层面的处理细节。为程序员提供一个抽象和高层的编程接口和框架。程序员仅需要关心其应用层的具体计算问题，仅需编写少量的处理应用本身计算问题的程序代码。如何具体完成这个并行计算任务所相关的诸多系统层细节被隐藏起来,交给计算框架去处理：</p><p>​    Map和Reduce为程序员提供了一个清晰的操作接口抽象描述。MapReduce中定义了如下的Map和Reduce两个抽象的编程接口，由用户去编程实现.Map和Reduce,MapReduce处理的数据类型是&lt;key,value&gt;键值对。</p><ul><li>Map: <code>(k1; v1) → [(k2; v2)]</code></li><li>Reduce: <code>(k2; [v2]) → [(k3; v3)]</code></li></ul><p>一个完整的mapreduce程序在分布式运行时有三类实例进程：</p><ol><li><code>MRAppMaster</code> 负责整个程序的过程调度及状态协调</li><li><code>MapTask</code> 负责map阶段的整个数据处理流程</li><li><code>ReduceTask</code> 负责reduce阶段的整个数据处理流程</li></ol><img src="/2020/08/11/%E5%A4%A7%E6%95%B0%E6%8D%AE-05/2.png" class><h1 id="MapReduce编程规范"><a href="#MapReduce编程规范" class="headerlink" title="MapReduce编程规范"></a>MapReduce编程规范</h1><p>MapReduce的开发一共有8个步骤：</p><h2 id="Map阶段2个步骤"><a href="#Map阶段2个步骤" class="headerlink" title="Map阶段2个步骤"></a>Map阶段2个步骤</h2><ul><li>设置InputFormat类，将数据切分为 key-value(<strong>K1</strong>和<strong>V1</strong>)对，输入到第二步</li><li>自定义Map逻辑，将第一步的结果转换成另一种的key-value(<strong>K2</strong>和<strong>V2</strong>)对，输出结果</li></ul><blockquote><p>( 1 )用户自定义的Mapper要继承自己的父类</p><p>( 2 ) Mapper的输入数据是KV对的形式(KV的类型可自定义)</p><p>( 3 ) Mapper中的业务逻辑写在map()方法中</p><p>( 4 ) Mapper的输出数据是KV对的形式(KV的类型可自定义)</p><p>( 5 ) map()方法(MapTask进程）对每一个&lt;K,V&gt;调用一次</p></blockquote><h2 id="Shuffle阶段4个步骤"><a href="#Shuffle阶段4个步骤" class="headerlink" title="Shuffle阶段4个步骤"></a>Shuffle阶段4个步骤</h2><ul><li>对输出的key-value对进行<strong>分区</strong></li><li>对不同分区的数据按照相同的Key<strong>排序</strong></li><li>（可选）对分组过的数据初步<strong>规约</strong>，降低数据的网络拷贝</li><li>对数据进行<strong>分组</strong>，相同Key的Value放入一个集合中</li></ul><h2 id="Reduce阶段2个步骤"><a href="#Reduce阶段2个步骤" class="headerlink" title="Reduce阶段2个步骤"></a>Reduce阶段2个步骤</h2><ul><li>对多个Map任务的结果进行排序以及合并，编写Reduce函数实现自己的逻辑，对输入的Key-Value进行处理，转为新的Key-Value（K3 和 V3）输出</li><li>设置OutputFormat处理并保存Reduce输出的 Key-Value数据</li></ul><blockquote><p>( 1 )用户自定义的Reducer要继承自己的父类<br>( 2 ) Reducer的输入数据类型对应Mapper的输出数据类型，也是KV( 3 )Reducer的业务逻辑写在reduce()方法中<br>( 4 ) ReduceTask进程对每一组相同k的&lt;k,v&gt;组调用一次reduce()方法</p></blockquote><h2 id="Driver阶段"><a href="#Driver阶段" class="headerlink" title="Driver阶段"></a>Driver阶段</h2><p>相当于YARN集群的客户端，用于提交我们整个程序到YARN集群，提交的是封装了MapReduce程序相关运行参数的job对象。</p><h1 id="入门案例：单词统计"><a href="#入门案例：单词统计" class="headerlink" title="入门案例：单词统计"></a>入门案例：单词统计</h1><h2 id="数据格式准备"><a href="#数据格式准备" class="headerlink" title="数据格式准备"></a>数据格式准备</h2><h3 id="创建一个新的文件"><a href="#创建一个新的文件" class="headerlink" title="创建一个新的文件"></a>创建一个新的文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd &#x2F;export&#x2F;servers</span><br><span class="line">vim wordcount.txt</span><br></pre></td></tr></table></figure><h3 id="向其中放入以下内容并保存"><a href="#向其中放入以下内容并保存" class="headerlink" title="向其中放入以下内容并保存"></a>向其中放入以下内容并保存</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hello,world,hadoop</span><br><span class="line">hive,sqoop,flume,hello</span><br><span class="line">kitty,tom,jerry,world</span><br><span class="line">hadoop</span><br></pre></td></tr></table></figure><h3 id="上传到-HDFS"><a href="#上传到-HDFS" class="headerlink" title="上传到 HDFS"></a>上传到 HDFS</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir &#x2F;wordcount&#x2F;</span><br><span class="line">hdfs dfs -put wordcount.txt &#x2F;wordcount&#x2F;</span><br></pre></td></tr></table></figure><h2 id="具体程序代码实现"><a href="#具体程序代码实现" class="headerlink" title="具体程序代码实现"></a>具体程序代码实现</h2><h3 id="WordCountMapper-java"><a href="#WordCountMapper-java" class="headerlink" title="WordCountMapper.java"></a>WordCountMapper.java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">* 其中 Mapper类的四个泛型解释：</span></span><br><span class="line"><span class="comment">* KETIN：K1的类型</span></span><br><span class="line"><span class="comment">* VALUEIN:V1的类型</span></span><br><span class="line"><span class="comment">* KEYOUT:K2的类型</span></span><br><span class="line"><span class="comment">* VALUEOUT:V2的类型</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">* 使用已经给出定义好的类型，基本类型的封装，操作序列化起来更加方便</span></span><br><span class="line"><span class="comment">* &lt;Long, String, Long, String&gt;  - &gt; &lt;LongWritable, Text, Text, LongWritable&gt;</span></span><br><span class="line"><span class="comment">* */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">/* map方法就是将 K1 和 V1 转换为 K2 和 V2</span></span><br><span class="line"><span class="comment">    * 参数：</span></span><br><span class="line"><span class="comment">    *   key  : K1 行偏移量</span></span><br><span class="line"><span class="comment">    *   value: V1 每一行的文本数据</span></span><br><span class="line"><span class="comment">    *   context : 上下文对象</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 如何将 K1 和 V1 转换为 K2 和 V2</span></span><br><span class="line"><span class="comment">    * K1            V1</span></span><br><span class="line"><span class="comment">    * 0         hello,world,hadoop</span></span><br><span class="line"><span class="comment">    * 18        hdfs,hello,hadoop</span></span><br><span class="line"><span class="comment">    * ---------------------------</span></span><br><span class="line"><span class="comment">    * K2           V2</span></span><br><span class="line"><span class="comment">    * hello         1</span></span><br><span class="line"><span class="comment">    * world         1</span></span><br><span class="line"><span class="comment">    * hadoop        1</span></span><br><span class="line"><span class="comment">    * hdfs          1</span></span><br><span class="line"><span class="comment">    * hello         1</span></span><br><span class="line"><span class="comment">    * hadoop        1</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        Text text = <span class="keyword">new</span> Text();</span><br><span class="line">        LongWritable longWritable = <span class="keyword">new</span> LongWritable();</span><br><span class="line">        <span class="comment">// 将一行的文本数据进行拆分</span></span><br><span class="line">        String[] split = value.toString().split(<span class="string">","</span>);</span><br><span class="line">        <span class="comment">// 遍历数组，组装K2 和 V2</span></span><br><span class="line">        <span class="keyword">for</span> (String word : split) &#123;</span><br><span class="line">            text.set(word);</span><br><span class="line">            longWritable.set(<span class="number">1</span>);</span><br><span class="line">            <span class="comment">// 将 K2 和 V2 写入上下文</span></span><br><span class="line">            context.write(text, longWritable);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="WordCountReducer-java"><a href="#WordCountReducer-java" class="headerlink" title="WordCountReducer.java"></a>WordCountReducer.java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">* 四个泛型解释：</span></span><br><span class="line"><span class="comment">* KEYIN:K2类型</span></span><br><span class="line"><span class="comment">* VALUEIN:V2类型</span></span><br><span class="line"><span class="comment">* KEYOUT：K3类型</span></span><br><span class="line"><span class="comment">* VALUEOUT：V3类型</span></span><br><span class="line"><span class="comment">* */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * reduce方法作用：将新的 K2 和 V2 转换为 K3 和 V3，并写入上下文</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 参数：</span></span><br><span class="line"><span class="comment">    *   key：新 K2</span></span><br><span class="line"><span class="comment">    *   values：集合 新V2</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 如何将新的 K2 和 V2 转换为 K3 和 V3</span></span><br><span class="line"><span class="comment">    * 新 K2          V2</span></span><br><span class="line"><span class="comment">    *   world       &lt;1,1,1&gt;</span></span><br><span class="line"><span class="comment">    *   hello       &lt;1,1&gt;</span></span><br><span class="line"><span class="comment">    *   hadoop      &lt;1&gt;</span></span><br><span class="line"><span class="comment">    * ---------------------</span></span><br><span class="line"><span class="comment">    *   K3          V3</span></span><br><span class="line"><span class="comment">    *   hello       2</span></span><br><span class="line"><span class="comment">    *   world       3</span></span><br><span class="line"><span class="comment">    *   hadoop      1</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;LongWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> count = <span class="number">0</span>;</span><br><span class="line">        LongWritable longWritable = <span class="keyword">new</span> LongWritable();</span><br><span class="line">        <span class="comment">// 遍历values集合，将集合中的数字相加得到 V3</span></span><br><span class="line">        <span class="keyword">for</span> (LongWritable value : values) &#123;</span><br><span class="line">            count += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 将 K3 和 V3 写入上下文中</span></span><br><span class="line">        longWritable.set(count);</span><br><span class="line">        context.write(key, longWritable);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="JobMain-java"><a href="#JobMain-java" class="headerlink" title="JobMain.java"></a>JobMain.java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobMain</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 该方法用于指定一个Job任务</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 创建一个job任务对象</span></span><br><span class="line">        <span class="comment">// 第一个参数是一个configuration，下面的main方法调用时已经传入，存在Configured类中，通过getConf()方法获取</span></span><br><span class="line">        Job job = Job.getInstance(<span class="keyword">super</span>.getConf(), <span class="string">"wordCount"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//job.setJarByClass(JobMain.class); // 如果打包出错，则需要该行代码</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置job任务对象 (8个步骤)</span></span><br><span class="line">        <span class="comment">//1、指定文件的读取方式和读取路径</span></span><br><span class="line">        job.setInputFormatClass(TextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        TextInputFormat.addInputPath(job, <span class="keyword">new</span> Path(<span class="string">"hdfs://bigdata1:8020/wordcount"</span>));</span><br><span class="line">        <span class="comment">// 本地执行方法，执行JobMain主函数即可，提前准备好文件，该路径下的wordcount.txt</span></span><br><span class="line">        <span class="comment">//TextInputFormat.addInputPath(job, new Path("file:///D:\\mapReduce\\input"));</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//2、指定Map阶段的处理方式 和 数据类型</span></span><br><span class="line">        job.setMapperClass(WordCountMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">//设置Map阶段K2的类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">// 设置Map阶段 V2 的类型</span></span><br><span class="line">        job.setMapOutputValueClass(LongWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 第3，4，5，6采用默认的方式</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7、指定Reduce阶段的处理方式和数据类型</span></span><br><span class="line">        job.setReducerClass(WordCountReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(LongWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//8、设置输出类型</span></span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">// 设置输出路径，并判断该目录是否存在，存在则删除</span></span><br><span class="line">        Path path = <span class="keyword">new</span> Path(<span class="string">"hdfs://bigdata1:8020/wordCount_out"</span>);</span><br><span class="line">        TextOutputFormat.setOutputPath(job,path);</span><br><span class="line">        <span class="comment">//TextOutputFormat.setOutputPath(job,new Path("file:///D:\\mapReduce\\output"));</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 判断目标目录是否存在</span></span><br><span class="line">        FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://bigdata1:8020/"</span>), <span class="keyword">new</span> Configuration());</span><br><span class="line">        <span class="keyword">if</span>(fileSystem.exists(path))&#123;</span><br><span class="line">            fileSystem.delete(path, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 等待任务结束</span></span><br><span class="line">        <span class="keyword">boolean</span> bl = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">return</span> bl? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">// 启动 job 任务，实际就是调用上面的run方法</span></span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(configuration, <span class="keyword">new</span> JobMain(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="运行方式"><a href="#运行方式" class="headerlink" title="运行方式"></a>运行方式</h2><p>写好程序，有两种运行方式，一种是本地运行，在JobMain函数中注释的部分即是。然后运行主函数即可，这种方法一般用于本地的测试。</p><p>另一种是<strong>集群运行</strong>模式</p><ul><li><p>将MapReduce程序提交给Yarn集群，分发到很多的结点上并执行</p></li><li><p>处理的数据和输出结果应该位于HDFS文件系统</p></li><li><p>提交代码的实现步骤：将程序打成jar包并上传到虚拟机服务器上，使用hdfs命令执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar original-mapreduce-1.0-SNAPSHOT.jar(jar包名) JobMain(主函数名，函数右键copy reference)</span><br></pre></td></tr></table></figure><blockquote><p>打成jar包方式：Maven项目里的Lifecycle下的package功能，得到的包位于target目录下，会有两个jar包，体积一大一小，本质并无不同，都可使用上传。</p></blockquote></li></ul><p>本程序将结果输出到HDFS的/wordCount_out目录下，可以 <code>http://bigdata1:50070/explorer.html#/</code>下查看。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇总结MapReduce的入门基础知识。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://iceWind-R.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="http://iceWind-R.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>大数据_04(HDFS_API操作)</title>
    <link href="http://icewind-r.github.io/2020/08/10/%E5%A4%A7%E6%95%B0%E6%8D%AE-04/"/>
    <id>http://icewind-r.github.io/2020/08/10/%E5%A4%A7%E6%95%B0%E6%8D%AE-04/</id>
    <published>2020-08-10T10:11:20.000Z</published>
    <updated>2020-10-15T06:39:21.883Z</updated>
    
    <content type="html"><![CDATA[<p>本篇总结HDFS在windows操作系统Java环境下的API操作。</p><a id="more"></a><hr><h1 id="HDFS的Java-api操作"><a href="#HDFS的Java-api操作" class="headerlink" title="HDFS的Java_api操作"></a>HDFS的Java_api操作</h1><h2 id="配置Windows下的-Hadoop环境"><a href="#配置Windows下的-Hadoop环境" class="headerlink" title="配置Windows下的 Hadoop环境"></a>配置Windows下的 Hadoop环境</h2><p>在Windows系统需要配置Hadoop运行环境，相当于Windows是一个Hadoop客户端。</p><p>不配置而直接运行代码会出现以下问题：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Could not locate executable null\bin\winutils.exe <span class="keyword">in</span> the hadoop binaries</span><br></pre></td></tr></table></figure><p><strong>原因：</strong> 缺少winutils.exe</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Unable to load native-hadoop library <span class="keyword">for</span> your platform..using <span class="built_in">builtin</span>-Java classes <span class="built_in">where</span> applicable</span><br></pre></td></tr></table></figure><p><strong>原因：</strong> 缺少hadoop.dll</p><h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><p><strong>1、</strong>首先下载Hadoop在Windows上的工具包，下载地址：<a href="https://github.com/steveloughran/winutils" target="_blank" rel="noopener">https://github.com/steveloughran/winutils</a> 。</p><p><strong>2、</strong>得到后，将其解压到一个无中文无空格的目录下，并配置环境变量。</p><img src="/2020/08/10/%E5%A4%A7%E6%95%B0%E6%8D%AE-04/1.png" class><p>Path下添加：<code>%HADOOP_HOME%\bin</code></p><p><strong>3、</strong>将下载的包内的 <code>hadoop.dll</code> 拷贝一份到 C:\Windows\System32 目录下。</p><p><strong>4、</strong> 重启电脑，完成。</p><h2 id="导入Maven依赖"><a href="#导入Maven依赖" class="headerlink" title="导入Maven依赖"></a>导入Maven依赖</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-mapreduce-client-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">encoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">encoding</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">minimizeJar</span>&gt;</span>true<span class="tag">&lt;/<span class="name">minimizeJar</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="使用URL方式访问数据（了解）"><a href="#使用URL方式访问数据（了解）" class="headerlink" title="使用URL方式访问数据（了解）"></a>使用URL方式访问数据（了解）</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">urlHDFS</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 注册URL</span></span><br><span class="line">    URL.setURLStreamHandlerFactory(<span class="keyword">new</span> FsUrlStreamHandlerFactory());</span><br><span class="line">    <span class="comment">//获取hdfs文件的输入流</span></span><br><span class="line">    InputStream inputStream = <span class="keyword">new</span> URL(<span class="string">"hdfs://bigdata1:8020/a.txt"</span>).openStream();</span><br><span class="line">    <span class="comment">// 获取本地文件的输出流</span></span><br><span class="line">    FileOutputStream fileOutputStream = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"D:\\hello.txt"</span>));</span><br><span class="line">    <span class="comment">// 实现文件的拷贝</span></span><br><span class="line">    IOUtils.copy(inputStream, fileOutputStream);</span><br><span class="line">    <span class="comment">// 关流</span></span><br><span class="line">    IOUtils.closeQuietly(inputStream);</span><br><span class="line">    IOUtils.closeQuietly(fileOutputStream);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="使用文件系统方式访问数据（掌握）"><a href="#使用文件系统方式访问数据（掌握）" class="headerlink" title="使用文件系统方式访问数据（掌握）"></a>使用文件系统方式访问数据（掌握）</h2><h3 id="涉及的主要类"><a href="#涉及的主要类" class="headerlink" title="涉及的主要类"></a>涉及的主要类</h3><p>在Java中操作HDFS，主要涉及以下Class：</p><ul><li><p><code>Configuration</code>：该类的对象封装了客户端或者服务器的配置</p></li><li><p><code>FileSystem</code>：该类的对象是一个文件系统对象，可以用该对象的一些方法来对文件进行操作，通过FileSystem的静态方法 get 获得该对象</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FileSystem fs = FileSystem.get(conf) # 就是Configuration类的对象</span><br></pre></td></tr></table></figure><ul><li><code>get</code> 方法从 <code>conf</code> 中的一个参数 <code>fs.defaultFS</code> 的配置值判断具体是什么类型的文件系统</li><li>如果我们的代码中没有指定<code>fs.defaultFS</code>，并且工程ClassPath下也没有给定相应的配置，<code>conf</code>中的默认值来自于Hadoop的Jar包中的<code>core-default.xml</code></li><li>默认值为<code>file:///</code>，则获取的不是一个DistributedFileSystem的实例，而是一个本地文件系统的客户端对象</li></ul></li></ul><h3 id="获取FileSystem的四种方式"><a href="#获取FileSystem的四种方式" class="headerlink" title="获取FileSystem的四种方式"></a>获取FileSystem的四种方式</h3><ul><li><p>第一种</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getFileSystem1</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 创建一个Configuration对象，封装</span></span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 设置文件系统类型</span></span><br><span class="line">    configuration.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://bigdata1:8020"</span>);</span><br><span class="line">    <span class="comment">// 获取指定的文件系统</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(configuration);</span><br><span class="line">    <span class="comment">// 输出 DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_373282607_1, ugi=11655 (auth:SIMPLE)]]</span></span><br><span class="line">    System.out.println(fileSystem);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>第二种</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getFileSystem2</span><span class="params">()</span> <span class="keyword">throws</span> IOException, URISyntaxException </span>&#123;</span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://bigdata1:8020"</span>), <span class="keyword">new</span> Configuration());</span><br><span class="line">    System.out.println(fileSystem);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>第三种</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getFileSystem3</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 创建一个Configuration对象</span></span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 设置文件系统类型</span></span><br><span class="line">    configuration.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://bigdata1:8020"</span>);</span><br><span class="line">    <span class="comment">// 获取指定的文件系统</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.newInstance(configuration);</span><br><span class="line">    <span class="comment">// 输出 DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_373282607_1, ugi=11655 (auth:SIMPLE)]]</span></span><br><span class="line">    System.out.println(fileSystem);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>第四种</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getFileSystem4</span><span class="params">()</span> <span class="keyword">throws</span> IOException, URISyntaxException </span>&#123;</span><br><span class="line">    FileSystem fileSystem = FileSystem.newInstance(<span class="keyword">new</span> URI(<span class="string">"hdfs://bigdata1:8020"</span>), <span class="keyword">new</span> Configuration());</span><br><span class="line">    System.out.println(fileSystem);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><blockquote><p>注意：1、3比较相似，2、4比较相似，主要是get方法和 newInstance方法的使用</p></blockquote><p>这里对Configuration参数对象的加载机制作出解释：</p><ol><li>首先构造时会加载jar包的默认配置，如：xxx-default.xml</li><li>再加载用户配置的文件（必须放在resources资源目录下），如自定义的hdfs-site.xml</li><li>最后可以手动设置，覆盖之前的相同有关配置：configuration.set(“dfs.blocksize”, “64m”);</li></ol><h3 id="遍历HDFS所有文件信息"><a href="#遍历HDFS所有文件信息" class="headerlink" title="遍历HDFS所有文件信息"></a>遍历HDFS所有文件信息</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listFiles</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 获取FileSystem实例</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://bigdata1:8020"</span>), <span class="keyword">new</span> Configuration());</span><br><span class="line">    <span class="comment">// 调用方法listFiles 获取一个目录下的文件信息，为一个迭代器对象</span></span><br><span class="line">    <span class="comment">// 第一个参数：指定目录</span></span><br><span class="line">    <span class="comment">// 第二个参数，是否迭代获取</span></span><br><span class="line">    RemoteIterator&lt;LocatedFileStatus&gt; iterator = fileSystem.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>), <span class="keyword">true</span>);</span><br><span class="line">    <span class="comment">//遍历迭代器，获取文件的详细信息</span></span><br><span class="line">    <span class="keyword">while</span> (iterator.hasNext())&#123;</span><br><span class="line">        LocatedFileStatus fileStatus = iterator.next();</span><br><span class="line">        <span class="comment">// 获取文件的绝对路径："hdfs://bigdata1:8020/xxx"</span></span><br><span class="line">        System.out.println(fileStatus.getPath() + <span class="string">"  ---  "</span> + fileStatus.getPath().getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//文件的Block信息</span></span><br><span class="line">        BlockLocation[] blockLocations = fileStatus.getBlockLocations();</span><br><span class="line">        System.out.println(<span class="string">"Block数："</span> + blockLocations.length);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="HDFS创建文件夹"><a href="#HDFS创建文件夹" class="headerlink" title="HDFS创建文件夹"></a>HDFS创建文件夹</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">mkdirs</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException </span>&#123;</span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://bigdata1:8020/a.txt"</span>), <span class="keyword">new</span> Configuration());</span><br><span class="line">    <span class="comment">// 创建文件夹</span></span><br><span class="line">    <span class="keyword">boolean</span> bl = fileSystem.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/aaa/bbb/ccc"</span>));</span><br><span class="line">    <span class="comment">//创建文件</span></span><br><span class="line">    fileSystem.create(<span class="keyword">new</span> Path(<span class="string">"/aaa/aaa.txt"</span>));</span><br><span class="line">    <span class="comment">// 两个创建方法都为递归创建</span></span><br><span class="line">    System.out.println(bl);</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="文件的下载"><a href="#文件的下载" class="headerlink" title="文件的下载"></a>文件的下载</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">downloadFile</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 获取FileSystem</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://bigdata1:8020/a.txt"</span>), <span class="keyword">new</span> Configuration());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取hdfs的输入流 </span></span><br><span class="line">    FSDataInputStream inputStream = fileSystem.open(<span class="keyword">new</span> Path(<span class="string">"/a.txt"</span>));</span><br><span class="line">    <span class="comment">// 获取本地路径的输出流 </span></span><br><span class="line">    FileOutputStream outputStream = <span class="keyword">new</span> FileOutputStream(<span class="string">"D://a.txt"</span>);</span><br><span class="line">    <span class="comment">// 文件的拷贝 </span></span><br><span class="line">    IOUtils.copy(inputStream, outputStream);</span><br><span class="line">    <span class="comment">// 关闭流 </span></span><br><span class="line">    IOUtils.closeQuietly(inputStream); </span><br><span class="line">    IOUtils.closeQuietly(outputStream); </span><br><span class="line">    fileSystem.close(); </span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 实现文件的下载 2</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">downloadFile2</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException </span>&#123;</span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://bigdata1:8020/a.txt"</span>), <span class="keyword">new</span> Configuration());</span><br><span class="line">    fileSystem.copyToLocalFile(<span class="keyword">new</span> Path(<span class="string">"/a.txt"</span>), <span class="keyword">new</span> Path(<span class="string">"D://a.txt"</span>));</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="文件的上传"><a href="#文件的上传" class="headerlink" title="文件的上传"></a>文件的上传</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">uploadFile</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException </span>&#123;</span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://bigdata1:8020/a.txt"</span>), <span class="keyword">new</span> Configuration());</span><br><span class="line">    fileSystem.copyFromLocalFile(<span class="keyword">new</span> Path(<span class="string">"D://b.txt"</span>),<span class="keyword">new</span> Path(<span class="string">"/"</span>));</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="HDFS的权限访问控制"><a href="#HDFS的权限访问控制" class="headerlink" title="HDFS的权限访问控制"></a>HDFS的权限访问控制</h3><p>首先进入Hadoop的安装目录下的/etc/hadoop/hdfs-site.xml，修改permission为true，代表启动权限。启动后通过命令行的权限修改才能生效，修改配置文件需要重启才能生效。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -chmod 000 /a.txt</span><br></pre></td></tr></table></figure><p>数字代表权限等级，当开启权限控制时，文件会有其对应的Owner，不是相应的Owner仍然无法访问资源。这时我们可以在get方法内指定伪装用户对资源进行访问：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://bigdata1:8020/a.txt"</span>), <span class="keyword">new</span> Configuration(), <span class="string">"root"</span>);</span><br></pre></td></tr></table></figure><h3 id="小文件合并"><a href="#小文件合并" class="headerlink" title="小文件合并"></a>小文件合并</h3><p>由于Hadoop擅长存储大文件，因为大文件的元数据信息比较少。如果集群中有大量的小文件，则需要维护大量的元数据，增大内存压力。所以有必要将小文件合并成大文件一起处理。</p><p>在HDFS的Shell命令下，可以用如下命令讲很多HDFS文件合并成一个大文件下载到本地</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers</span><br><span class="line">hdfs dfs -getmerge /config/*.xml ./hello.xml # 表示合并文件，下载到当前目录下的hello.xml</span><br></pre></td></tr></table></figure><p>同样也可以在上传时将小文件合并到一个大文件里面去</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">mergeFile</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 获取FileSystem</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://bigdata1:8020/a.txt"</span>), <span class="keyword">new</span> Configuration(), <span class="string">"root"</span>);</span><br><span class="line">    <span class="comment">// 获取hdfs大文件的输出流，创建一个承载所有内容的大文件</span></span><br><span class="line">    FSDataOutputStream outputStream = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">"/big.txt"</span>));</span><br><span class="line">    <span class="comment">// 获取一个本地文件系统</span></span><br><span class="line">    LocalFileSystem localFileSystem = FileSystem.getLocal(<span class="keyword">new</span> Configuration());</span><br><span class="line">    <span class="comment">// 获取本地文件夹下所有文件的详情,input是提前准备的文件夹，里面有一些小文件</span></span><br><span class="line">    FileStatus[] fileStatuses = localFileSystem.listStatus(<span class="keyword">new</span> Path(<span class="string">"D:\\input"</span>));</span><br><span class="line">    <span class="comment">// 遍历每个文件，获得每个文件的输入流</span></span><br><span class="line">    <span class="keyword">for</span> (FileStatus fileStatus : fileStatuses) &#123;</span><br><span class="line">        FSDataInputStream inputStream = localFileSystem.open(fileStatus.getPath());</span><br><span class="line">        <span class="comment">// 将小文件的数据复制到大文件</span></span><br><span class="line">        IOUtils.copy(inputStream, outputStream);</span><br><span class="line">        IOUtils.closeQuietly(inputStream);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 关闭流</span></span><br><span class="line">    IOUtils.closeQuietly(outputStream);</span><br><span class="line">    localFileSystem.close();</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="HDFS的高可用机制"><a href="#HDFS的高可用机制" class="headerlink" title="HDFS的高可用机制"></a>HDFS的高可用机制</h1><p>在Hadoop2.X之前，Namenode是HDFS集群中可能发生单点故障的节点，每个HDFS集群只有一个namenode，一旦这个节点不可用，则整个HDFS集群将处于不可用状态。<br>HDFS高可用（HA）方案就是为了解决上述问题而产生的，在HA HDFS集群中会同时运行两个Namenode，一个作为活动的Namenode（Active），一个作为备份的Namenode（Standby）。备份的Namenode的命名空间与活动的Namenode是实时同步的，所以当活动的Namenode发生故障而停止服务时，备份Namenode可以立即切换为活动状态，而不影响HDFS集群服务。</p><p>详情：<a href="https://blog.csdn.net/u012736748/article/details/79534019" target="_blank" rel="noopener">https://blog.csdn.net/u012736748/article/details/79534019</a></p><h1 id="Hadoop联邦机制"><a href="#Hadoop联邦机制" class="headerlink" title="Hadoop联邦机制"></a>Hadoop联邦机制</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇总结HDFS在windows操作系统Java环境下的API操作。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://iceWind-R.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="http://iceWind-R.github.io/tags/Hadoop/"/>
    
  </entry>
  
</feed>
