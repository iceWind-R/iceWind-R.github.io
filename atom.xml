<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Thorine</title>
  
  <subtitle>凡是过往，皆为序章</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://icewind-r.github.io/"/>
  <updated>2020-09-13T11:55:49.097Z</updated>
  <id>http://icewind-r.github.io/</id>
  
  <author>
    <name>大雪初晴丶</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>我的考研必看</title>
    <link href="http://icewind-r.github.io/2020/09/13/%E6%88%91%E7%9A%84%E8%80%83%E7%A0%94%E5%BF%85%E7%9C%8B/"/>
    <id>http://icewind-r.github.io/2020/09/13/%E6%88%91%E7%9A%84%E8%80%83%E7%A0%94%E5%BF%85%E7%9C%8B/</id>
    <published>2020-09-13T11:45:17.000Z</published>
    <updated>2020-09-13T11:55:49.097Z</updated>
    
    <content type="html"><![CDATA[<p>为什么考研？自己适合考研吗？这两个问题，想要考研的你，真的思考过吗？</p><hr><p>给自己定的初步目标：<strong>中下游211本专业</strong></p><p>说到需求，当然就是你考研是想要达到什么目的。我举几个例子，假如说你打算将来考选调生，那么985院校就是目标，专业视具体要求而定；假如你想回老家考个公务员，那选择一个匹配岗位的学校和专业即可；假如你想进入某些知名企业，那么应该调研清楚它的目标院校是什么档次，然后作出选择。</p><p>先说竞争对手方面的因素，这一方面主要从目标院校历年的考生和录取情况来评估。</p><p>1.初试科目是什么</p><p>2.专业课参考书是什么</p><p>3.最近三年的分数线（这一定要看地区，不要盲目对比，二区的380和北京上海的380完全不是一回事）</p><p>4.最近三年的报录比</p><p>5.报考学生的生源大概水平</p><p>6.复试什么形式，在总分中占比多少</p><p>7.最后实际录取学生的总分及单科分数分布（和其他院校对比，要对比同地区的学校公共课的分数，专业课不具可比性）</p><p>8.最后录取学生的本科背景</p><p>9.录取学生中二战以以上的学生比例</p><p>10.录取的跨校跨专业学生的比例</p><p>以上信息都不是能百度的，需要自己去搜集，像5、8、9、10这些问题不需要多么精准，知道大概水平即可。至于怎么搜集，最直接的就是找上一届录取的前辈。<strong>（最近很多人私信我去哪里找已经录取的前辈，大家可以下载一个APP叫做经验超市，这个APP是由北大的几位考研学长开发的，可以去上面看看有没有学长学姐，上面的学长学姐都是经过身份认证的。）</strong></p><p>自身因素方面：</p><p>1.高中学习状态及高考成绩（高中是否尽力学习，高考是正常发挥还是失常或者超常发挥。这一点可以衡量潜在的学习能力，毕竟高考和考研都是类似的应试）</p><p>2.本科院校及专业（正常录取还是填志愿失误造成滑档）</p><p>3.本科成绩（结合自己学习努力程度来看）</p><p>4.个人学习习惯和自制力（十几年的读书经历，想必自己心中应该有数。在考研这种靠自己自律的环境下，能够发生重大改变的人少之又少，不要高估自己的决心和毅力）</p><p>5.考研时间（自己有多少时间准备，本科课程能不能应付，学校的琐事会不会消耗太多时间）</p><p>6.自身心态（自己心态建设做的怎么样，是不是过于焦虑和敏感，是不是能够承压）</p><p>7.公共课基础如何</p><p>8.是否跨专业，专业课是否零基础</p><p><strong>人生终究是回归均值的一个过程，不要以为自己能够轻易的突破。考研之前请认真思考，把握好自己的定位、需求、成本和风险承担能力。每个人的天赋，成长过程，家庭环境都是不同的，即便要向命运挑战，也要结合自己的实际情况。</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;为什么考研？自己适合考研吗？这两个问题，想要考研的你，真的思考过吗？&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;给自己定的初步目标：&lt;strong&gt;中下游211本专业&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;说到需求，当然就是你考研是想要达到什么目的。我举几个例子，假如说你打算将来考选调生，那么98
      
    
    </summary>
    
    
      <category term="考研" scheme="http://iceWind-R.github.io/categories/%E8%80%83%E7%A0%94/"/>
    
    
      <category term="idea" scheme="http://iceWind-R.github.io/tags/idea/"/>
    
  </entry>
  
  <entry>
    <title>C++文本操作</title>
    <link href="http://icewind-r.github.io/2020/09/09/C-%E6%96%87%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    <id>http://icewind-r.github.io/2020/09/09/C-%E6%96%87%E6%9C%AC%E6%93%8D%E4%BD%9C/</id>
    <published>2020-09-09T01:09:40.000Z</published>
    <updated>2020-09-10T01:07:47.533Z</updated>
    
    <content type="html"><![CDATA[<p>本篇总结 C++ 的文本操作，即C++文件和流的概念。</p><a id="more"></a><hr><h1 id="文件流"><a href="#文件流" class="headerlink" title="文件流"></a>文件流</h1><p>到目前为止，我们使用了iostream标准库，它提供了cin 和 cout 方法分别用于从<strong>标准输入读取流</strong>和向<strong>标准输入写入流</strong>。</p><img src="/2020/09/09/C-%E6%96%87%E6%9C%AC%E6%93%8D%E4%BD%9C/1.gif" class><p>如何从<strong>标准输入读取流</strong>和向<strong>标准输入写入流</strong>？这就需要用到 C++ 中另一个标准库fstream，它定义了三个新的数据类型：</p><table><thead><tr><th>数据类型</th><th>描述</th></tr></thead><tbody><tr><td>ofstream</td><td>该数据类型表示输出文件流，用于创建文件并向文件写入信息</td></tr><tr><td>ifstream</td><td>该数据类型表示输入文件流，用于从文件读取信息</td></tr><tr><td>fstream</td><td>该数据类型通常表示文件流，且同时具有 ofstream 和 ifstream 两种功能，这意味着它可以创建文件，向文件写入信息，从文件读取信息。</td></tr></tbody></table><p>要在C++中进行文件处理，必须在 C++ 代码中包含头文件 <code>&lt;iostream&gt;</code> 和 <code>&lt;fstream&gt;</code></p><h2 id="打开文件"><a href="#打开文件" class="headerlink" title="打开文件"></a>打开文件</h2><p>在从文件读取信息或者向文件写入信息之前，必须先打开文件。<strong>ofstream</strong> 和 <strong>fstream</strong>对象都可以用来打开文件进行<strong>写</strong>操作；如果只需要打开文件进行<strong>读</strong>操作，则使用 <strong>ifstream</strong> 对象。</p><p>下面是open()函数的标准语法。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">open</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *fileName, ios::openmode mode)</span></span>;</span><br></pre></td></tr></table></figure><p>在这里，open()成员函数的第一参数指定要打开的文件名字，第二个参数指定打开文件的模式。</p><table><thead><tr><th>模式标志</th><th>描述</th></tr></thead><tbody><tr><td>ios::app</td><td>追加模式，所有写入都追加到文件末尾</td></tr><tr><td>ios::ate</td><td>文件打开后将文件指针位置定位到文件末尾</td></tr><tr><td>ios::in</td><td>打开文件用于读取</td></tr><tr><td>ios::out</td><td>打开文件用于写入</td></tr><tr><td>ios::trunc</td><td>如果该文件已经存在，其内容将在打开之前被截断，即把文件长度设为0.</td></tr></tbody></table><p>可以把以上两种或两种以上的模式结合使用，以“或”运算（“|”）的方式。例如：如果想要以写入模式打开文件，并希望截断文件，以防文件已经存在，那么可以使用下面的语法：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ofstream outFile;</span><br><span class="line">outFile.<span class="built_in">open</span>( <span class="string">"file.dat"</span>, ios::out | ios::trunc );</span><br></pre></td></tr></table></figure><p>类似的，如果想要打开一个文件用于读写，可以使用下面的语法：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ifstream afile;</span><br><span class="line">afile.<span class="built_in">open</span>(<span class="string">"file.dat"</span>, ios::out | ios::in)</span><br></pre></td></tr></table></figure><p>很多程序中，可能会碰到ofstream out(“Hello.txt”), ifstream in(“…”),fstream foi(“…”)这样的的使用，并没有显式的去调用open（）函数就进行文件的操作，直接调用了其默认的打开方式，因为在stream类的构造函数中调用了open()函数,并拥有同样的构造函数，所以在这里可以直接使用流对象进行文件的操作，默认方式如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ofstream <span class="title">out</span><span class="params">(<span class="string">"..."</span>, ios::out)</span></span>;</span><br><span class="line"><span class="function">ifstream <span class="title">in</span><span class="params">(<span class="string">"..."</span>, ios::in)</span></span>;</span><br><span class="line"><span class="function">fstream <span class="title">foi</span><span class="params">(<span class="string">"..."</span>, ios::in|ios::out)</span></span>;</span><br></pre></td></tr></table></figure><h2 id="关闭文件"><a href="#关闭文件" class="headerlink" title="关闭文件"></a>关闭文件</h2><p>使用close() 函数关闭打开的文件，它是 fstream，ifstream 和 ofstream 对象的一个成员。</p><h2 id="写入文件"><a href="#写入文件" class="headerlink" title="写入文件"></a>写入文件</h2><p>我们使用 流插入运算符（ &lt;&lt; ）向文件写入信息。</p><h2 id="读取文件"><a href="#读取文件" class="headerlink" title="读取文件"></a>读取文件</h2><p>我们使用 流提取运算符（ &gt;&gt; ）从文件读取信息。</p><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p>下面给出一个例子，读取hello.txt文件中的字符串，写入out.txt中：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fstream&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt; </span></span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>; </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    <span class="function">ifstream <span class="title">myfile</span><span class="params">(<span class="string">"hello.txt"</span>)</span></span>; </span><br><span class="line">    <span class="function">ofstream <span class="title">outfile</span><span class="params">(<span class="string">"out.txt"</span>, ios::app)</span></span>; </span><br><span class="line">    <span class="built_in">string</span> temp; </span><br><span class="line">    <span class="keyword">if</span> (!myfile.is_open()) </span><br><span class="line">    &#123; </span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"未成功打开文件"</span> &lt;&lt; <span class="built_in">endl</span>; </span><br><span class="line">    &#125; </span><br><span class="line">    <span class="keyword">while</span>(getline(myfile,temp)) </span><br><span class="line">    &#123; </span><br><span class="line">        outfile &lt;&lt; temp; </span><br><span class="line">        outfile &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125; </span><br><span class="line">    myfile.<span class="built_in">close</span>(); </span><br><span class="line">    outfile.<span class="built_in">close</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中getline(stream, string)函数的功能：按行从输入流中读入字符，存到string变量</p><ul><li><p>直到出现以下情况为止：</p></li><li><p>读入了文件结束标志</p></li><li><p>读到一个新行</p></li><li><p>达到字符串的最大长度</p></li></ul><p>如果getline()没有读入字符，将返回false，可用于判断文件是否结束。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> v, w, weight;</span><br><span class="line">ifstream infile;   <span class="comment">//输入流</span></span><br><span class="line"> </span><br><span class="line">infile.<span class="built_in">open</span>(<span class="string">"data.txt"</span>, ios::in); </span><br><span class="line"><span class="keyword">if</span>(!infile.is_open ())</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"Open file failure"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">while</span> (!infile.eof())            <span class="comment">// 到达文件末尾时返回true</span></span><br><span class="line">&#123;</span><br><span class="line">    infile &gt;&gt; v &gt;&gt; w &gt;&gt; weight;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; v &lt;&lt; <span class="string">"\t"</span> &lt;&lt; w &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"> infile.<span class="built_in">close</span>();   <span class="comment">//关闭文件</span></span><br></pre></td></tr></table></figure><blockquote><p>上述代码的功能是读取data.txt文件的数据，注意，此时要求data.txt文件中的数据是三个一行，每个数据用空格或换行符隔开。</p></blockquote><h2 id="文件指针"><a href="#文件指针" class="headerlink" title="文件指针"></a>文件指针</h2><p> 文件指针位置在c++中的用法：</p><table><thead><tr><th>标准</th><th>描述</th></tr></thead><tbody><tr><td>ios::beg</td><td>文件头</td></tr><tr><td>ios::end</td><td>文件尾</td></tr><tr><td>ios::cur</td><td>当前位置</td></tr></tbody></table><p>例如：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">file.seekg(<span class="number">0</span>,ios::beg);   <span class="comment">//让文件指针定位到文件开头 </span></span><br><span class="line">file.seekg(<span class="number">0</span>,ios::<span class="built_in">end</span>);   <span class="comment">//让文件指针定位到文件末尾 </span></span><br><span class="line">file.seekg(<span class="number">10</span>,ios::cur);   <span class="comment">//让文件指针从当前位置向文件末方向移动10个字节 </span></span><br><span class="line">file.seekg(<span class="number">-10</span>,ios::cur);   <span class="comment">//让文件指针从当前位置向文件开始方向移动10个字节 </span></span><br><span class="line">file.seekg(<span class="number">10</span>,ios::beg);   <span class="comment">//让文件指针定位到离文件开头10个字节的位置</span></span><br></pre></td></tr></table></figure><p><strong>注意：移动的单位是字节，而不是行</strong>。</p><h2 id="状态标志符的验证-Verification-of-state-flags"><a href="#状态标志符的验证-Verification-of-state-flags" class="headerlink" title="状态标志符的验证(Verification of state flags)"></a>状态标志符的验证(Verification of state flags)</h2><p>除了eof()以外，还有一些验证流的状态的成员函数（所有都返回bool型返回值）：</p><ul><li><p><strong>bad()</strong></p><p>如果在读写过程中出错，返回 true 。例如：当我们要对一个不是打开为写状态的文件进行写入时，或者我们要写入的设备没有剩余空间的时候。</p></li><li><p><strong>fail()</strong></p><p>除了与bad() 同样的情况下会返回 true 以外，加上格式错误时也返回true ，例如当想要读入一个整数，而获得了一个字母的时候。</p></li><li><p><strong>eof()</strong></p><p>如果读文件到达文件末尾，返回true。</p></li><li><p><strong>good()</strong></p><p>这是最通用的：如果调用以上任何一个函数返回true 的话，此函数返回 false 。</p></li></ul><p>要想重置以上成员函数所检查的状态标志，你可以使用成员函数clear()，没有参数。</p><h1 id="string流"><a href="#string流" class="headerlink" title="string流"></a>string流</h1><p>string头文件定义了三个类型来支持内存IO，istringstream向string读取数据，ostringstream从string写数据，stringstream既可从string读取数据也可向string写数据，就像string是一个IO流一样。</p><h2 id="istringstream的用法"><a href="#istringstream的用法" class="headerlink" title="istringstream的用法"></a>istringstream的用法</h2><p>例子如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sstream&gt;    //使用istringstream所需要的头文件 </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt; </span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    <span class="built_in">string</span> str = <span class="string">"Hello world! I am Lee"</span>; </span><br><span class="line">    <span class="function"><span class="built_in">istringstream</span> <span class="title">is</span><span class="params">(str)</span></span>;    <span class="comment">//将is绑定到str</span></span><br><span class="line">    <span class="built_in">string</span> s; </span><br><span class="line">    <span class="keyword">while</span> (is &gt;&gt; s) </span><br><span class="line">    &#123; </span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; s &lt;&lt; <span class="built_in">endl</span>; </span><br><span class="line">    &#125; </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这相当于把一个句子拆分成单词，联系到前文提到的从文件中读取string的方法，如果读取到的string对象为一个句子，包含很多单词，那么我们就可以运用这种方法把string对象拆分开来。</p><p>结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Hello</span><br><span class="line">world!</span><br><span class="line">I</span><br><span class="line">am</span><br><span class="line">Lee</span><br></pre></td></tr></table></figure><h2 id="ostringstream的用法"><a href="#ostringstream的用法" class="headerlink" title="ostringstream的用法"></a>ostringstream的用法</h2><p>​    ostringstream同样是由一个string对象构造而来，ostringstream类向一个string插入字符。 </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sstream&gt;    //使用ostringstream所需要的头文件 </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;  </span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span>    </span></span><br><span class="line"><span class="function"></span>&#123;  </span><br><span class="line">    <span class="built_in">ostringstream</span> ostr;  </span><br><span class="line">   <span class="comment">// ostr.str("abc");//如果构造的时候设置了字符串参数,那么增长操作的时候不会从结尾开始增加,而是修改原有数据,超出的部分增长  </span></span><br><span class="line">    ostr.<span class="built_in">put</span>(<span class="string">'d'</span>);  </span><br><span class="line">    ostr.<span class="built_in">put</span>(<span class="string">'e'</span>);  </span><br><span class="line">    ostr&lt;&lt;<span class="string">"fg"</span>;    </span><br><span class="line">    <span class="built_in">string</span> gstr = ostr.str();  </span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;gstr &lt;&lt; <span class="built_in">endl</span>; </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在上例代码中，我们通过put()或者左移操作符可以不断向ostr插入单个字符或者是字符串，通过str()函数返回增长过后的完整字符串数据，但值 得注意的一点是，当构造的时候对象内已经存在字符串数据的时候，那么增长操作的时候不会从结尾开始增加,而是修改原有数据,超出的部分增长。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇总结 C++ 的文本操作，即C++文件和流的概念。&lt;/p&gt;
    
    </summary>
    
    
      <category term="C++" scheme="http://iceWind-R.github.io/categories/C/"/>
    
    
      <category term="文本操作" scheme="http://iceWind-R.github.io/tags/%E6%96%87%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>停歇_在此靠岸</title>
    <link href="http://icewind-r.github.io/2020/09/04/%E5%81%9C%E6%AD%87-%E5%9C%A8%E6%AD%A4%E9%9D%A0%E5%B2%B8/"/>
    <id>http://icewind-r.github.io/2020/09/04/%E5%81%9C%E6%AD%87-%E5%9C%A8%E6%AD%A4%E9%9D%A0%E5%B2%B8/</id>
    <published>2020-09-04T04:53:07.000Z</published>
    <updated>2020-09-04T05:00:38.740Z</updated>
    
    <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=522353195&auto=1&height=66"></iframe>]]></content>
    
    <summary type="html">
    
      
      
        &lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=&quot;330&quot; height=&quot;86&quot; src=&quot;//music.163.com/outchain/player?type=2&amp;id=
      
    
    </summary>
    
    
      <category term="随笔" scheme="http://iceWind-R.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="idea" scheme="http://iceWind-R.github.io/tags/idea/"/>
    
  </entry>
  
  <entry>
    <title>大数据_11(函数,数据压缩存储,调优)</title>
    <link href="http://icewind-r.github.io/2020/08/29/%E5%A4%A7%E6%95%B0%E6%8D%AE-11/"/>
    <id>http://icewind-r.github.io/2020/08/29/%E5%A4%A7%E6%95%B0%E6%8D%AE-11/</id>
    <published>2020-08-29T01:32:29.000Z</published>
    <updated>2020-08-29T02:01:53.487Z</updated>
    
    <content type="html"><![CDATA[<p>本篇总结Hive的函数、数据压缩和数据存储格式 以及 Hive调优等。</p><a id="more"></a><hr><h1 id="Hive函数"><a href="#Hive函数" class="headerlink" title="Hive函数"></a>Hive函数</h1><h2 id="内置函数"><a href="#内置函数" class="headerlink" title="内置函数"></a>内置函数</h2><p>内容较多，见<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF" target="_blank" rel="noopener">《Hive官方文档》</a>。</p><ol><li><p>查看系统自带的函数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> show <span class="built_in">functions</span>;</span></span><br></pre></td></tr></table></figure></li><li><p>显示自带的函数的用法</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> desc <span class="keyword">function</span> upper;</span></span><br></pre></td></tr></table></figure></li><li><p>详细显示自带的函数的用法</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> desc <span class="keyword">function</span> extended upper;</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="常用内置函数"><a href="#常用内置函数" class="headerlink" title="常用内置函数"></a>常用内置函数</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">字符串连接函数： concat </span></span><br><span class="line">  select concat('abc','def’,'gh');</span><br><span class="line"><span class="meta">#</span><span class="bash">带分隔符字符串连接函数： concat_ws </span></span><br><span class="line">  select concat_ws(',','abc','def','gh');</span><br><span class="line"><span class="meta">#</span><span class="bash">cast类型转换</span></span><br><span class="line">  select cast(1.5 as int);</span><br><span class="line"><span class="meta">#</span><span class="bash">get_json_object(json 解析函数，用来处理json，必须是json格式)</span></span><br><span class="line">   select get_json_object('&#123;"name":"jack","age":"20"&#125;','$.name');</span><br><span class="line"><span class="meta">#</span><span class="bash">URL解析函数</span></span><br><span class="line">   select parse_url('http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1', 'HOST');</span><br><span class="line"><span class="meta">#</span><span class="bash">explode：把map集合中每个键值对或数组中的每个元素都单独生成一行的形式</span></span><br></pre></td></tr></table></figure><h2 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><ol><li>Hive 自带了一些函数，比如：max/min等，当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数(UDF).</li><li>根据用户自定义函数类别分为以下三种：<ol><li>UDF（User-Defined-Function）<ul><li>一进一出</li></ul></li><li>UDAF（User-Defined Aggregation Function）<ul><li>聚集函数，多进一出</li><li>类似于：<code>count</code>/<code>max</code>/<code>min</code></li></ul></li><li>UDTF（User-Defined Table-Generating Functions）<ul><li>一进多出</li><li>如 <code>lateral</code> <code>view</code> <code>explore()</code></li></ul></li></ol></li><li>编程步骤：<ol><li>继承org.apache.hadoop.hive.ql.UDF</li><li>需要实现evaluate函数；evaluate函数支持重载；</li></ol></li><li>注意事项<ol><li>UDF必须要有返回类型，可以返回null，但是返回类型不能为void；</li><li>UDF中常用Text/LongWritable等类型，不推荐使用java类型；</li></ol></li></ol><h3 id="UDF-开发实例"><a href="#UDF-开发实例" class="headerlink" title="UDF 开发实例"></a>UDF 开发实例</h3><p>需求，建立的自己的my_upper方法，将输入的字符串第一个字符大写。</p><h4 id="1、创建maven工程"><a href="#1、创建maven工程" class="headerlink" title="1、创建maven工程"></a>1、创建maven工程</h4><p>其中的pom.xml文件如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-exec --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">encoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">encoding</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><p>注意：这一步踩了坑，hive-exec坐标引用后爆红（org\pentaho\pentaho-aggdesigner-algorithm\5.1.5-jhyde.jar有红线），<strong>解决方法：</strong></p><ul><li><p>进入<a href="https://mvnrepository.com/artifact/org.pentaho/pentaho-aggdesigner-algorithm/5.1.5-jhyde下载相应jar包" target="_blank" rel="noopener">https://mvnrepository.com/artifact/org.pentaho/pentaho-aggdesigner-algorithm/5.1.5-jhyde下载相应jar包</a></p></li><li><p>放入本地maven仓库中（我的目录为D:\Server\Tools\maven_repository\org\pentaho\pentaho-aggdesigner-algorithm\5.1.5-jhyde）</p></li><li><p>重启IDEA，并在pom.xml中导入以下依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.pentaho<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>pentaho-aggdesigner-algorithm<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.5-jhyde<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>完成。</p></li></ul><h4 id="2、编写程序代码"><a href="#2、编写程序代码" class="headerlink" title="2、编写程序代码"></a>2、编写程序代码</h4><p><strong>开发 Java 类集成 UDF</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyUDF</span>  <span class="keyword">extends</span> <span class="title">UDF</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Text <span class="title">evaluate</span><span class="params">(<span class="keyword">final</span> Text str)</span></span>&#123;</span><br><span class="line">        String tmp_str = str.toString();</span><br><span class="line">        <span class="keyword">if</span>(str != <span class="keyword">null</span> &amp;&amp; !tmp_str.equals(<span class="string">""</span>))&#123;</span><br><span class="line">          String str_ret = tmp_str.substring(<span class="number">0</span>, <span class="number">1</span>).toUpperCase() + tmp_str.substring(<span class="number">1</span>);</span><br><span class="line">          <span class="keyword">return</span>  <span class="keyword">new</span> Text(str_ret);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span>  <span class="keyword">new</span> Text(<span class="string">""</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="3、项目打包"><a href="#3、项目打包" class="headerlink" title="3、项目打包"></a>3、项目打包</h4><p>利用maven 的package命令打成jar包，并上传到集群（bigdata3）的hive的lib目录下。</p><h4 id="4、添加jar包到hive中"><a href="#4、添加jar包到hive中" class="headerlink" title="4、添加jar包到hive中"></a>4、添加jar包到hive中</h4><p>重命名我们的jar包名称</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/apache-hive-2.7.5-bin/lib</span><br><span class="line">mv hive-1.0-SNAPSHOT.jar my_upper.jar</span><br></pre></td></tr></table></figure><p>hive的客户端添加我们的jar包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> add jar /<span class="built_in">export</span>/servers/apache-hive-2.7.5-bin/lib/my_upper.jar;</span></span><br></pre></td></tr></table></figure><h4 id="5、设置函数与我们的自定义函数关联"><a href="#5、设置函数与我们的自定义函数关联" class="headerlink" title="5、设置函数与我们的自定义函数关联"></a>5、设置函数与我们的自定义函数关联</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create temporary function my_upper as 'udf.MyUDF';</span><br></pre></td></tr></table></figure><h4 id="6、使用自定义函数"><a href="#6、使用自定义函数" class="headerlink" title="6、使用自定义函数"></a>6、使用自定义函数</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> my_upper(<span class="string">'hello world!'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果 Hello world!</span></span><br></pre></td></tr></table></figure><h1 id="数据压缩、数据存储格式"><a href="#数据压缩、数据存储格式" class="headerlink" title="数据压缩、数据存储格式"></a>数据压缩、数据存储格式</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇总结Hive的函数、数据压缩和数据存储格式 以及 Hive调优等。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://iceWind-R.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://iceWind-R.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据_10(查询、Shell参数)</title>
    <link href="http://icewind-r.github.io/2020/08/24/%E5%A4%A7%E6%95%B0%E6%8D%AE-10/"/>
    <id>http://icewind-r.github.io/2020/08/24/%E5%A4%A7%E6%95%B0%E6%8D%AE-10/</id>
    <published>2020-08-24T08:14:15.000Z</published>
    <updated>2020-08-29T01:35:36.068Z</updated>
    
    <content type="html"><![CDATA[<p>本篇总结Hive的查询语法、Shell命令。</p><a id="more"></a><hr><h1 id="Hive-查询语法"><a href="#Hive-查询语法" class="headerlink" title="Hive 查询语法"></a>Hive 查询语法</h1><h2 id="SELECT"><a href="#SELECT" class="headerlink" title="SELECT"></a>SELECT</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> [<span class="keyword">ALL</span> | <span class="keyword">DISTINCT</span>] select_expr, select_expr, ...</span><br><span class="line"><span class="keyword">FROM</span> table_reference</span><br><span class="line">[<span class="keyword">WHERE</span> where_condition]</span><br><span class="line">[<span class="keyword">GROUP</span> <span class="keyword">BY</span> col_list [<span class="keyword">HAVING</span> condition]]</span><br><span class="line">[CLUSTER <span class="keyword">BY</span> col_list</span><br><span class="line">| [<span class="keyword">DISTRIBUTE</span> <span class="keyword">BY</span> col_list] [<span class="keyword">SORT</span> <span class="keyword">BY</span>| <span class="keyword">ORDER</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">]</span><br><span class="line">[<span class="keyword">LIMIT</span> <span class="built_in">number</span>]</span><br></pre></td></tr></table></figure><ol><li>order by 会对输入做全局排序，因此只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。</li><li>sort by不是全局排序，其在数据进入reducer前完成排序。因此，如果用sort by进行排序，并且设置mapred.reduce.tasks&gt;1，则sort by只保证每个reducer的输出有序，不保证全局有序。</li><li>distribute by(字段)根据指定的字段将数据分到不同的reducer，且分发算法是hash散列。</li><li>cluster by(字段) 除了具有distribute by的功能外，还会对该字段进行排序.</li><li>因此，如果distribute 和sort字段是同一个时，此时，<code>cluster by = distribute by + sort by</code></li></ol><h2 id="查询语法"><a href="#查询语法" class="headerlink" title="查询语法"></a>查询语法</h2><p><strong>全表查询</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure><p><strong>选择特定列</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_id ,c_id <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure><p><strong>列别名</strong></p><p>1）重命名一个列。<br>2）便于计算。<br>3）紧跟列名，也可以在列名和别名之间加入关键字‘AS’</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_id <span class="keyword">as</span> myid ,c_id <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure><h2 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h2><ul><li>求总行数（count）注：count(1) 等价于 count(*)</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure><ul><li>求分数的最大值（max）</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">max</span>(s_score) <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure><ul><li>求分数的最小值（min）</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">min</span>(s_score) <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure><ul><li>求分数的总和（sum）</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">sum</span>(s_score) <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure><ul><li>求分数的平均值（avg）</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">avg</span>(s_score) <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure><h2 id="LIMIT语句"><a href="#LIMIT语句" class="headerlink" title="LIMIT语句"></a>LIMIT语句</h2><p>典型的查询会返回多行数据。LIMIT子句用于限制返回的行数。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">limit</span> <span class="number">3</span>;</span><br></pre></td></tr></table></figure><h2 id="WHERE语句"><a href="#WHERE语句" class="headerlink" title="WHERE语句"></a>WHERE语句</h2><ol><li>使用WHERE 子句，将不满足条件的行过滤掉。</li><li>WHERE 子句紧随 FROM 子句。</li><li>案例实操</li></ol><p>查询出分数大于60的数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score &gt; <span class="number">60</span>;</span><br></pre></td></tr></table></figure><p><strong>比较运算符</strong></p><table><thead><tr><th>操作符</th><th>支持的数据类型</th><th>描述</th></tr></thead><tbody><tr><td>A=B</td><td>基本数据类型</td><td>如果A等于B则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;=&gt;B</td><td>基本数据类型</td><td>如果A和B都为NULL，则返回TRUE，其他的和等号（=）操作符的结果一致，如果任一为NULL则结果为NULL</td></tr><tr><td>A&lt;&gt;B, A!=B</td><td>基本数据类型</td><td>A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A小于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;=B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A小于等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&gt;B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A大于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&gt;=B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A大于等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A [NOT] BETWEEN B AND C</td><td>基本数据类型</td><td>如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果。</td></tr><tr><td>A IS NULL</td><td>所有数据类型</td><td>如果A等于NULL，则返回TRUE，反之返回FALSE</td></tr><tr><td>A IS NOT NULL</td><td>所有数据类型</td><td>如果A不等于NULL，则返回TRUE，反之返回FALSE</td></tr><tr><td>IN(数值1, 数值2)</td><td>所有数据类型</td><td>使用 IN运算显示列表中的值</td></tr><tr><td>A [NOT] LIKE B</td><td>STRING 类型</td><td>B是一个SQL下的简单正则表达式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。</td></tr><tr><td>A RLIKE B, A REGEXP B</td><td>STRING</td><td>类型 B是一个正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。</td></tr></tbody></table><ul><li>查询分数等于80的所有的数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score = <span class="number">80</span>;</span><br></pre></td></tr></table></figure><ul><li>查询分数在80到100的所有数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score <span class="keyword">between</span> <span class="number">80</span> <span class="keyword">and</span> <span class="number">100</span>;</span><br></pre></td></tr></table></figure><ul><li>查询成绩为空的所有数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score <span class="keyword">is</span> <span class="literal">null</span>;</span><br></pre></td></tr></table></figure><ul><li>查询成绩是80和90的数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score <span class="keyword">in</span>(<span class="number">80</span>,<span class="number">90</span>);</span><br></pre></td></tr></table></figure><h2 id="LIKE-和-RLIKE"><a href="#LIKE-和-RLIKE" class="headerlink" title="LIKE 和 RLIKE"></a>LIKE 和 RLIKE</h2><p>1、使用LIKE运算选择类似的值，选择条件可以包含字符或数字:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">% 代表零个或多个字符(任意个字符)。</span><br><span class="line">_ 代表一个字符。</span><br></pre></td></tr></table></figure><p>2、RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件。</p><p><strong>案例实操</strong></p><ol><li>查找以8开头的所有成绩</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score <span class="keyword">like</span> <span class="string">'8%'</span>;</span><br></pre></td></tr></table></figure><ol><li>查找第二个数值为9的所有成绩数据</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score <span class="keyword">like</span> <span class="string">'_9%'</span>;</span><br></pre></td></tr></table></figure><ol><li>查找s_id中含1的数据</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_id <span class="keyword">rlike</span> <span class="string">'[1]'</span>;  <span class="comment">#  like '%1%'</span></span><br></pre></td></tr></table></figure><h2 id="逻辑运算符"><a href="#逻辑运算符" class="headerlink" title="逻辑运算符"></a>逻辑运算符</h2><table><thead><tr><th>操作符</th><th>含义</th></tr></thead><tbody><tr><td>AND</td><td>逻辑并</td></tr><tr><td>OR</td><td>逻辑或</td></tr><tr><td>NOT</td><td>逻辑否</td></tr></tbody></table><ul><li>查询成绩大于80，并且s_id是01的数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score &gt;<span class="number">80</span> <span class="keyword">and</span> s_id = <span class="string">'01'</span>;</span><br></pre></td></tr></table></figure><ul><li>查询成绩大于80，或者s_id 是01的数</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score &gt; <span class="number">80</span> <span class="keyword">or</span> s_id = <span class="string">'01'</span>;</span><br></pre></td></tr></table></figure><ul><li>查询s_id 不是 01和02的学生</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_id <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">'01'</span>,<span class="string">'02'</span>);</span><br></pre></td></tr></table></figure><h2 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h2><h3 id="GROUP-BY-语句"><a href="#GROUP-BY-语句" class="headerlink" title="GROUP BY 语句"></a>GROUP BY 语句</h3><p>GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。<br>案例实操：</p><ul><li>计算每个学生的平均分数</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_id ,<span class="keyword">avg</span>(s_score) <span class="keyword">from</span> score <span class="keyword">group</span> <span class="keyword">by</span> s_id;</span><br></pre></td></tr></table></figure><ul><li>计算每个学生最高成绩</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_id ,<span class="keyword">max</span>(s_score) <span class="keyword">from</span> score <span class="keyword">group</span> <span class="keyword">by</span> s_id;</span><br></pre></td></tr></table></figure><h3 id="HAVING-语句"><a href="#HAVING-语句" class="headerlink" title="HAVING 语句"></a>HAVING 语句</h3><ol><li><p>having与where不同点</p><ol><li>where针对表中的列发挥作用，查询数据；having针对查询结果中的列发挥作用，筛选数据。</li><li>where后面不能写分组函数，而having后面可以使用分组函数。</li><li>having只用于group by分组统计语句。</li></ol></li><li><p>案例实操：</p><ul><li>求每个学生的平均分数</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_id ,<span class="keyword">avg</span>(s_score) <span class="keyword">from</span> score <span class="keyword">group</span> <span class="keyword">by</span> s_id;</span><br></pre></td></tr></table></figure><ul><li>求每个学生平均分数大于85的人</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_id ,<span class="keyword">avg</span>(s_score) avgscore <span class="keyword">from</span> score <span class="keyword">group</span> <span class="keyword">by</span> s_id <span class="keyword">having</span> avgscore &gt; <span class="number">85</span>;</span><br></pre></td></tr></table></figure></li></ol><h2 id="JOIN-语句"><a href="#JOIN-语句" class="headerlink" title="JOIN 语句"></a>JOIN 语句</h2><h3 id="等值-JOIN"><a href="#等值-JOIN" class="headerlink" title="等值 JOIN"></a>等值 JOIN</h3><p>Hive支持通常的SQL JOIN语句，但是只支持等值连接，不支持非等值连接。</p><p>案例操作: 查询分数对应的姓名</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s.s_id,s.s_score,stu.s_name,stu.s_birth  <span class="keyword">from</span> score s  <span class="keyword">join</span> student stu <span class="keyword">on</span> s.s_id = stu.s_id;</span><br></pre></td></tr></table></figure><h3 id="表的别名"><a href="#表的别名" class="headerlink" title="表的别名"></a>表的别名</h3><ul><li><p>好处</p><ul><li>使用别名可以简化查询。</li><li>使用表名前缀可以提高执行效率。</li></ul></li><li><p>案例实操</p><ul><li>合并老师与课程表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> techer t <span class="keyword">join</span> course c <span class="keyword">on</span> t.t_id = c.t_id;</span><br></pre></td></tr></table></figure></li></ul><h3 id="内连接"><a href="#内连接" class="headerlink" title="内连接"></a>内连接</h3><p>内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> techer t <span class="keyword">inner</span> <span class="keyword">join</span> course c <span class="keyword">on</span> t.t_id = c.t_id;</span><br></pre></td></tr></table></figure><h3 id="左外连接"><a href="#左外连接" class="headerlink" title="左外连接"></a>左外连接</h3><p>左外连接：JOIN操作符左边表中符合WHERE子句的所有记录将会被返回。<br>查询老师对应的课程</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> techer t <span class="keyword">left</span> <span class="keyword">join</span> course c <span class="keyword">on</span> t.t_id = c.t_id;</span><br></pre></td></tr></table></figure><h3 id="右外连接"><a href="#右外连接" class="headerlink" title="右外连接"></a>右外连接</h3><p>右外连接：JOIN操作符右边表中符合WHERE子句的所有记录将会被返回。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> teacher t <span class="keyword">right</span> <span class="keyword">join</span> course c <span class="keyword">on</span> t.t_id = c.t_id;</span><br></pre></td></tr></table></figure><h3 id="多表连接"><a href="#多表连接" class="headerlink" title="多表连接"></a>多表连接</h3><p>注意：连接 n个表，至少需要n-1个连接条件。例如：连接三个表，至少需要两个连接条件。</p><p>多表连接查询，查询老师对应的课程，以及对应的分数，对应的学生</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> teacher t</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> course c</span><br><span class="line"><span class="keyword">on</span> t.t_id = c.t_id</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> score s</span><br><span class="line"><span class="keyword">on</span> s.c_id = c.c_id</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> student stu</span><br><span class="line"><span class="keyword">on</span> s.s_id = stu.s_id;</span><br></pre></td></tr></table></figure><p>大多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表techer和表course进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表score;进行连接操作。</p><h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><h3 id="全局排序"><a href="#全局排序" class="headerlink" title="全局排序"></a>全局排序</h3><p>Order By：全局排序，只能有一个reduce</p><ol><li><p>使用 ORDER BY 子句排序<br>ASC（ascend）: 升序（默认）<br>DESC（descend）: 降序</p></li><li><p>ORDER BY 子句在SELECT语句的结尾。</p></li><li><p>案例实操</p><ol><li>查询学生的成绩，并按照分数降序排列</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> student s <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> score sco <span class="keyword">ON</span> s.s_id = sco.s_id <span class="keyword">ORDER</span> <span class="keyword">BY</span> sco.s_score <span class="keyword">DESC</span>;</span><br><span class="line">1</span><br></pre></td></tr></table></figure><p>​    2. 查询学生的成绩，并按照分数升序排列</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> student s <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> score sco <span class="keyword">ON</span> s.s_id = sco.s_id <span class="keyword">ORDER</span> <span class="keyword">BY</span> sco.s_score <span class="keyword">asc</span>;</span><br></pre></td></tr></table></figure></li></ol><h3 id="按照别名排序"><a href="#按照别名排序" class="headerlink" title="按照别名排序"></a>按照别名排序</h3><p>按照分数的平均值排序</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_id ,<span class="keyword">avg</span>(s_score) <span class="keyword">avg</span> <span class="keyword">from</span> score <span class="keyword">group</span> <span class="keyword">by</span> s_id <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">avg</span>;</span><br></pre></td></tr></table></figure><h3 id="多个列排序"><a href="#多个列排序" class="headerlink" title="多个列排序"></a>多个列排序</h3><p>按照学生id和平均成绩进行排序</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_id ,<span class="keyword">avg</span>(s_score) <span class="keyword">avg</span> <span class="keyword">from</span> score <span class="keyword">group</span> <span class="keyword">by</span> s_id <span class="keyword">order</span> <span class="keyword">by</span> s_id,<span class="keyword">avg</span>;</span><br></pre></td></tr></table></figure><h3 id="每个MapReduce内部排序（Sort-By）局部排序"><a href="#每个MapReduce内部排序（Sort-By）局部排序" class="headerlink" title="每个MapReduce内部排序（Sort By）局部排序"></a>每个MapReduce内部排序（Sort By）局部排序</h3><p>Sort By：每个MapReduce内部进行排序，对全局结果集来说不是排序。</p><ol><li><p>设置reduce个数</p> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">3</span>;</span><br></pre></td></tr></table></figure></li><li><p>查看设置reduce个数</p> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces;</span><br></pre></td></tr></table></figure></li><li><p>查询成绩按照成绩降序排列</p> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">sort</span> <span class="keyword">by</span> s_score;</span><br></pre></td></tr></table></figure></li><li><p>将查询结果导入到文件中（按照成绩降序排列）</p> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/export/servers/hivedatas/sort'</span> <span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">sort</span> <span class="keyword">by</span> s_score;</span><br></pre></td></tr></table></figure></li></ol><h3 id="分区排序（DISTRIBUTE-BY）"><a href="#分区排序（DISTRIBUTE-BY）" class="headerlink" title="分区排序（DISTRIBUTE BY）"></a>分区排序（DISTRIBUTE BY）</h3><p>Distribute By：类似MR中partition，进行分区，结合sort by使用。</p><p>注意，Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。</p><p>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</p><p>案例实操：先按照学生id进行分区，再按照学生成绩进行排序。</p><ol><li><p>设置reduce的个数，将我们对应的s_id划分到对应的reduce当中去</p> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">7</span>;</span><br></pre></td></tr></table></figure></li><li><p>通过distribute by 进行数据的分区</p> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/export/servers/hivedatas/sort'</span> <span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">distribute</span> <span class="keyword">by</span> s_id <span class="keyword">sort</span> <span class="keyword">by</span> s_score;</span><br></pre></td></tr></table></figure></li></ol><h3 id="CLUSTER-BY"><a href="#CLUSTER-BY" class="headerlink" title="CLUSTER BY"></a>CLUSTER BY</h3><p>当distribute by和sort by字段相同时，可以使用cluster by方式。</p><p>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是倒序排序，不能指定排序规则为ASC或者DESC。</p><p>以下两种写法等价</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score cluster <span class="keyword">by</span> s_id;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">distribute</span> <span class="keyword">by</span> s_id <span class="keyword">sort</span> <span class="keyword">by</span> s_id;</span><br></pre></td></tr></table></figure><h1 id="Shell参数"><a href="#Shell参数" class="headerlink" title="Shell参数"></a>Shell参数</h1><h2 id="Hive命令行"><a href="#Hive命令行" class="headerlink" title="Hive命令行"></a>Hive命令行</h2><h3 id="语法结构"><a href="#语法结构" class="headerlink" title="语法结构"></a>语法结构</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hive [-hiveconf x=y]* [&lt;-i filename&gt;]* [&lt;-f filename&gt;|&lt;-e query-string&gt;] [-S]</span><br></pre></td></tr></table></figure><h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><p>1、 -i 从文件初始化HQL。</p><p>2、 <code>-e从命令行执行指定的HQL</code></p><p>3、 <code>-f 执行HQL脚本</code></p><p>4、 -v 输出执行的HQL语句到控制台</p><p>5、 -p connect to Hive Server on port number</p><p>6、 -hiveconf x=y Use this to set hive/hadoop configuration variables. 设置hive运行时候的参数配置</p><h2 id="Hive参数配置方式"><a href="#Hive参数配置方式" class="headerlink" title="Hive参数配置方式"></a>Hive参数配置方式</h2><p>开发Hive应用时，不可避免地需要设定Hive的参数。设定Hive的参数可以调优HQL代码的执行效率，或帮助定位问题。</p><p><strong>对于一般参数，有以下三种设定方式：</strong></p><ul><li>配置文件</li><li>命令行参数</li><li>参数声明</li></ul><p><code>配置文件</code>：Hive的配置文件包括</p><ul><li><p>用户自定义配置文件：$HIVE_CONF_DIR/hive-site.xml</p></li><li><p>默认配置文件： $HIVE_CONF_DIR/hive-default.xml</p><p><strong>用户自定义配置会覆盖默认配置。</strong></p></li></ul><p>另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。</p><p>配置文件的设定对本机启动的所有Hive进程都有效。</p><p><code>命令行参数：</code>启动Hive（客户端或Server方式）时，可以在命令行添加-hiveconf param=value来设定参数，例如：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hive -hiveconf hive.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>这一设定对本次启动的Session（对于Server方式启动，则是所有请求的Sessions）有效。</p><p><code>参数声明</code>：可以在HQL中使用SET关键字设定参数，例如：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.reduce.tasks=<span class="number">100</span>;</span><br></pre></td></tr></table></figure><p>这一设定的作用域也是session级的。</p><p>上述三种设定方式的优先级依次递增。即参数声明覆盖命令行参数，命令行参数覆盖配置文件设定。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在Session建立以前已经完成了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇总结Hive的查询语法、Shell命令。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://iceWind-R.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://iceWind-R.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据_09(Hive基本操作)</title>
    <link href="http://icewind-r.github.io/2020/08/22/%E5%A4%A7%E6%95%B0%E6%8D%AE-09/"/>
    <id>http://icewind-r.github.io/2020/08/22/%E5%A4%A7%E6%95%B0%E6%8D%AE-09/</id>
    <published>2020-08-22T02:48:33.000Z</published>
    <updated>2020-08-24T08:20:26.655Z</updated>
    
    <content type="html"><![CDATA[<p>本篇总结Hive的交互方式和<strong>基本操作命令</strong>。</p><a id="more"></a><hr><h1 id="Hive-的交互方式"><a href="#Hive-的交互方式" class="headerlink" title="Hive 的交互方式"></a>Hive 的交互方式</h1><h2 id="第一种交互方式：-bin-hive"><a href="#第一种交互方式：-bin-hive" class="headerlink" title="第一种交互方式：./bin/hive"></a>第一种交互方式：./bin/hive</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create database if not exists mytest;</span><br></pre></td></tr></table></figure><h2 id="第二种交互方式：使用-sql-语句或者-sql-脚本进行交互"><a href="#第二种交互方式：使用-sql-语句或者-sql-脚本进行交互" class="headerlink" title="第二种交互方式：使用 sql 语句或者 sql 脚本进行交互"></a>第二种交互方式：使用 sql 语句或者 sql 脚本进行交互</h2><p>不进入hive的客户端直接执行hive的hql语句</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hive -e "create database if not exists mytest;"</span><br></pre></td></tr></table></figure><p>或者我们可以将我们的hql语句写成一个sql脚本执行，通过hive -f 来执行我们的sql脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hive -f /export/servers/hive.sql</span><br></pre></td></tr></table></figure><h1 id="Hive-的基本操作"><a href="#Hive-的基本操作" class="headerlink" title="Hive 的基本操作"></a>Hive 的基本操作</h1><h2 id="数据库操作"><a href="#数据库操作" class="headerlink" title="数据库操作"></a>数据库操作</h2><p><strong>创建数据库：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> myhive;</span><br></pre></td></tr></table></figure><p><strong>创建数据库并指定位置：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> myhive location <span class="string">'/myhive'</span>;</span><br></pre></td></tr></table></figure><p><strong>设置数据库键值对信息：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> foo <span class="keyword">with</span> dbproperties (<span class="string">'owner'</span>=<span class="string">'itcast'</span>,<span class="string">'date'</span>=<span class="string">'20190120'</span>);</span><br></pre></td></tr></table></figure><p><strong>查看数据库更多详细信息：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc database extended myhive;</span><br></pre></td></tr></table></figure><p><strong>删除数据库：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">database</span> myhive;</span><br></pre></td></tr></table></figure><p>​    强制删除数据库，包含数据库下面的表一起删除：（效果相当于 rm -rf，慎用！！！）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">database</span> myhive <span class="keyword">cascade</span>;</span><br></pre></td></tr></table></figure><h2 id="数据库表的操作"><a href="#数据库表的操作" class="headerlink" title="数据库表的操作"></a>数据库表的操作</h2><h3 id="创建表的语法"><a href="#创建表的语法" class="headerlink" title="创建表的语法"></a><strong>创建表的语法</strong></h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> [<span class="keyword">external</span>] <span class="keyword">table</span> [<span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span>] table_name (</span><br><span class="line">col_name data_type [<span class="keyword">comment</span> <span class="string">'字段描述信息'</span>]</span><br><span class="line">col_name data_type [<span class="keyword">comment</span> <span class="string">'字段描述信息'</span>])</span><br><span class="line">[<span class="keyword">comment</span> <span class="string">'表的描述信息'</span>]</span><br><span class="line">[partitioned <span class="keyword">by</span> (col_name data_type,...)]</span><br><span class="line">[clustered <span class="keyword">by</span> (col_name,col_name,...)]</span><br><span class="line">[sorted <span class="keyword">by</span> (col_name [<span class="keyword">asc</span>|<span class="keyword">desc</span>],...) <span class="keyword">into</span> num_buckets buckets]</span><br><span class="line">[<span class="keyword">row</span> <span class="keyword">format</span> row_format]</span><br><span class="line">[storted <span class="keyword">as</span> ....]</span><br><span class="line">[location <span class="string">'指定表的路径'</span>]</span><br></pre></td></tr></table></figure><p>说明：</p><blockquote><ol><li><p><a href="https://blog.csdn.net/chipeize/article/details/100364057" target="_blank" rel="noopener">create table</a></p><p>创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。</p></li><li><p><a href="https://blog.csdn.net/chipeize/article/details/100364057" target="_blank" rel="noopener">external</a></p><p>可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</p></li><li><p><a href="https://blog.csdn.net/chipeize/article/details/100364057" target="_blank" rel="noopener">comment</a></p><p>表示注释,默认不能使用中文</p></li><li><p><a href="https://blog.csdn.net/chipeize/article/details/100364057" target="_blank" rel="noopener">partitioned by</a></p><p>表示使用表分区,一个表可以拥有一个或者多个分区，每一个分区单独存在一个目录下 .</p></li><li><p><a href="https://blog.csdn.net/chipeize/article/details/100364057" target="_blank" rel="noopener">clustered by</a></p><p>对于每一个表分文件， Hive可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive也是 针对某一列进行桶的组织。</p></li><li><p><a href="https://blog.csdn.net/chipeize/article/details/100364057" target="_blank" rel="noopener">sorted by</a></p><p>指定排序字段和排序规则</p></li><li><p><a href="https://blog.csdn.net/chipeize/article/details/100364057" target="_blank" rel="noopener">row format</a></p><p>指定表文件字段分隔符</p></li><li><p><a href="https://blog.csdn.net/chipeize/article/details/100364057" target="_blank" rel="noopener">storted as</a></p><p>指定表文件的存储格式, 常用格式:SEQUENCEFILE, TEXTFILE, RCFILE,如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。如果数据需要压缩，使用 storted as SEQUENCEFILE。</p></li><li><p><a href="https://blog.csdn.net/chipeize/article/details/100364057" target="_blank" rel="noopener">location</a></p><p>指定表文件的存储路径</p></li></ol></blockquote><h3 id="内部表操作"><a href="#内部表操作" class="headerlink" title="内部表操作"></a>内部表操作</h3><p>创建表时，如果没有使用external关键字，则该表是内部表（管理表，managed table）。</p><p><strong>Hive 建表字段类型</strong></p><table><thead><tr><th align="left">分类</th><th align="left">类型</th><th align="left">描述</th><th align="left">字面量示例</th></tr></thead><tbody><tr><td align="left">原始类型</td><td align="left">BOOLEAN</td><td align="left">true/false</td><td align="left">TRUE</td></tr><tr><td align="left"></td><td align="left">TINYINT</td><td align="left">1字节的有符号整数, -128~127</td><td align="left">1Y</td></tr><tr><td align="left"></td><td align="left">SMALLINT</td><td align="left">2个字节的有符号整数，-32768~32767</td><td align="left">1S</td></tr><tr><td align="left"></td><td align="left">INT</td><td align="left">4个字节的带符号整数</td><td align="left">1</td></tr><tr><td align="left"></td><td align="left">BIGINT</td><td align="left">8字节带符号整数</td><td align="left">1L</td></tr><tr><td align="left"></td><td align="left">FLOAT</td><td align="left">4字节单精度浮点数</td><td align="left">1.0</td></tr><tr><td align="left"></td><td align="left">DOUBLE</td><td align="left">8字节双精度浮点数</td><td align="left">1.0</td></tr><tr><td align="left"></td><td align="left">DEICIMAL</td><td align="left">任意精度的带符号小数</td><td align="left">1.0</td></tr><tr><td align="left"></td><td align="left">STRING</td><td align="left">字符串，变长</td><td align="left">“a”,’b’</td></tr><tr><td align="left"></td><td align="left">VARCHAR</td><td align="left">变长字符串</td><td align="left">“a”,’b’</td></tr><tr><td align="left"></td><td align="left">CHAR</td><td align="left">固定长度字符串</td><td align="left">“a”,’b’</td></tr><tr><td align="left"></td><td align="left">BINARY</td><td align="left">字节数组</td><td align="left">无法表示</td></tr><tr><td align="left"></td><td align="left">TIMESTAMP</td><td align="left">时间戳，毫秒值精度</td><td align="left">122327493795</td></tr><tr><td align="left"></td><td align="left">DATE</td><td align="left">日期</td><td align="left">‘2016-03-29’</td></tr><tr><td align="left"></td><td align="left">INTERVAL</td><td align="left">时间频率间隔</td><td align="left"></td></tr><tr><td align="left">复杂类型</td><td align="left">ARRAY</td><td align="left">有序的的同类型的集合</td><td align="left">array(1,2)</td></tr><tr><td align="left"></td><td align="left">MAP</td><td align="left">key-value,key必须为原始类型，value可以任意类型</td><td align="left">map(‘a’,1,’b’,2)</td></tr><tr><td align="left"></td><td align="left">STRUCT</td><td align="left">字段集合,类型可以不同</td><td align="left">struct(‘1’,1,1.0), named_stract(‘col1’,’1’,’col2’,1,’clo3’,1.0)</td></tr><tr><td align="left"></td><td align="left">UNION</td><td align="left">在有限取值范围内的一个值</td><td align="left">create_union(1,’a’,63)</td></tr></tbody></table><p><strong>建表入门</strong>：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> myhive;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu(<span class="keyword">id</span> <span class="built_in">int</span>,<span class="keyword">name</span> <span class="keyword">string</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> stu <span class="keyword">values</span> (<span class="number">1</span>,<span class="string">"zhangsan"</span>);  <span class="comment">#插入数据</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> stu;</span><br></pre></td></tr></table></figure><p>创建表并指定字段之间的分隔符，默认为 <code>\001</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span>  <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> stu2(<span class="keyword">id</span> <span class="built_in">int</span> ,<span class="keyword">name</span> <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><p><strong>创建表并指定表文件的存放路径</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span>  <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> stu2(<span class="keyword">id</span> <span class="built_in">int</span> ,<span class="keyword">name</span> <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span> location <span class="string">'/user/stu2'</span>;</span><br></pre></td></tr></table></figure><p><strong>根据查询结果创建表</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过复制表结构和表内容创建新表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu3 <span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> stu2;</span><br></pre></td></tr></table></figure><p><strong>根据已经存在的表结构创建表</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu4 <span class="keyword">like</span> stu;</span><br></pre></td></tr></table></figure><p><strong>查询表的详细信息</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc formatted stu2;</span><br></pre></td></tr></table></figure><p><strong>删除表</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> stu;</span><br></pre></td></tr></table></figure><h3 id="外部表的操作"><a href="#外部表的操作" class="headerlink" title="外部表的操作"></a><strong>外部表的操作</strong></h3><p>外部表因为是指定其他的hdfs路径的数据加载到表当中来，所以hive表会认为自己不完全独占这份数据，所以删除hive表的时候，数据仍然存放在hdfs当中，不会删掉.</p><p><strong>内部表和外部表的使用场景</strong></p><p>每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。</p><h4 id="操作案例"><a href="#操作案例" class="headerlink" title="操作案例"></a>操作案例</h4><p>分别创建老师与学生外部表，并向表中加载数据。</p><p><strong>创建老师表</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> teacher(t_id <span class="keyword">string</span>,t_name <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><p><strong>创建学生表</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> student (s_id <span class="keyword">string</span>,s_name <span class="keyword">string</span>,s_birth <span class="keyword">string</span> , s_sex <span class="keyword">string</span> ) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><p><strong>加载数据</strong></p><ol><li><p>可以直接把指定结构的文件上传到hdfs文件系统的表目录下。</p><p>比如之前的teacher表有两个string字段，分隔符为 \t，我们可以建如下文件，teacher.txt，内容为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>zhangsan</span><br><span class="line"><span class="number">2</span>lisi</span><br><span class="line"><span class="number">3</span>wangwu           <span class="comment"># 分割符为 \t</span></span><br></pre></td></tr></table></figure><p>即可在hive命令下，使用select查询得到如上结果。</p></li><li><p>本地加载，可以加载本地的文件读入到hive数据仓库中。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/export/servers/hivedatas/student.csv'</span> <span class="keyword">into</span> <span class="keyword">table</span> student;</span><br><span class="line"></span><br><span class="line">加载本地路径下的csv文件到student表中。</span><br></pre></td></tr></table></figure><blockquote><p>我们可以在此验证外部表：通过drop删除表后，select不能查询，但是hdfs系统的数据文件仍在，通过之前的建表语句再次建表，就可查询成功。可见，外部表只是与真实数据的一种映射关系。</p></blockquote></li><li><p>加载数据并覆盖已有数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath '/export/servers/hivedatas/student.csv' overwrite into table student;</span><br></pre></td></tr></table></figure></li><li><p>从hdfs文件系统向表中加载数据（需要提前将数据上传到hdfs文件系统）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data inpath '/student.csv' into table student;</span><br></pre></td></tr></table></figure></li></ol><h3 id="分区表的操作"><a href="#分区表的操作" class="headerlink" title="分区表的操作"></a><strong>分区表的操作</strong></h3><p>​    在大数据中，最常用的一种思想就是分治，我们可以把大的文件切割划分成一个个的小的文件，这样每次操作一个小的文件就会很容易了，同样的道理，在hive当中也是支持这种思想的，就是我们可以把大的数据，按照每月，或者天进行切分成一个个的小的文件,存放在不同的文件夹中.</p><p><strong>创建分区表语法</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> score(s_id <span class="keyword">string</span>,c_id <span class="keyword">string</span>, s_score <span class="built_in">int</span>) partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><p><strong>创建一个表带多个分区</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table score2 (s_id string,c_id string, s_score int) partitioned by (year string,month string,day string) row format delimited fields terminated by &#39;\t&#39;;</span><br></pre></td></tr></table></figure><p><strong>加载数据到分区表中</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/export/servers/hivedatas/score.csv'</span> <span class="keyword">into</span> <span class="keyword">table</span> score <span class="keyword">partition</span> (<span class="keyword">month</span>=<span class="string">'201806'</span>);</span><br></pre></td></tr></table></figure><p><strong>加载数据到多分区表中</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/export/servers/hivedatas/score.csv'</span> <span class="keyword">into</span> <span class="keyword">table</span> score2 <span class="keyword">partition</span>(<span class="keyword">year</span>=<span class="string">'2018'</span>,<span class="keyword">month</span>=<span class="string">'06'</span>,<span class="keyword">day</span>=<span class="string">'01'</span>);</span><br></pre></td></tr></table></figure><p><strong>多分区表联合查询（使用union all）</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> <span class="keyword">month</span> = <span class="string">'201806'</span> <span class="keyword">union</span> <span class="keyword">all</span> <span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> <span class="keyword">month</span> = <span class="string">'201806'</span>;</span><br></pre></td></tr></table></figure><p><strong>查看分区</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span>  <span class="keyword">partitions</span>  score;</span><br></pre></td></tr></table></figure><p><strong>添加一个分区</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> score <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201805'</span>);</span><br></pre></td></tr></table></figure><p><strong>删除分区</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> score <span class="keyword">drop</span> <span class="keyword">partition</span>(<span class="keyword">month</span> = <span class="string">'201806'</span>);</span><br></pre></td></tr></table></figure><h3 id="分区表综合练习"><a href="#分区表综合练习" class="headerlink" title="分区表综合练习"></a>分区表综合练习</h3><p><strong>需求描述：</strong></p><p> 现在有一个文件score.csv文件，存放在集群的这个目录下/scoredatas/month=201806，这个文件每天都会生成，存放到对应的日期文件夹下面去，文件别人也需要公用，不能移动。需求，创建hive对应的表，并将数据加载到表中，进行数据统计分析，且删除表之后，数据不能删除</p><p><strong>数据准备：</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p /scoredatas/month=201806</span><br><span class="line">hdfs dfs -put score.csv /scoredatas/month=201806/</span><br></pre></td></tr></table></figure><p><strong>创建外部分区表，并指定文件数据存放目录</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create external table score4(s_id string, c_id string,s_score int) partitioned by (month string) row format delimited fields terminated by '\t' location '/scoredatas';</span><br></pre></td></tr></table></figure><p><strong>进行表的修复(建立表与数据文件之间的一个关系映射)</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">msck repair table score4;</span><br></pre></td></tr></table></figure><p>之后便可select查询该表验证结果。</p><h3 id="分桶表操作"><a href="#分桶表操作" class="headerlink" title="分桶表操作"></a>分桶表操作</h3><p>分桶，就是将数据按照指定的字段进行划分到多个文件当中去,分桶就是MapReduce中的分区.</p><p><strong>开启 Hive 的分桶功能</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.enforce.bucketing=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure><p><strong>设置 Reduce 个数</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">3</span>;</span><br></pre></td></tr></table></figure><p><strong>创建分桶表</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> course (c_id <span class="keyword">string</span>,c_name <span class="keyword">string</span>,t_id <span class="keyword">string</span>) clustered <span class="keyword">by</span>(c_id) <span class="keyword">into</span> <span class="number">3</span> buckets <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><p><strong>创建普通表</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> course_common (c_id <span class="keyword">string</span>,c_name <span class="keyword">string</span>,t_id <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><p><strong>普通表中加载数据</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/export/servers/hivedatas/course.csv'</span> <span class="keyword">into</span> <span class="keyword">table</span> course_common;</span><br></pre></td></tr></table></figure><p><strong>通过insert overwrite给桶表中加载数据</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> course <span class="keyword">select</span> * <span class="keyword">from</span> course_common cluster <span class="keyword">by</span>(c_id);</span><br></pre></td></tr></table></figure><h3 id="修改表结构"><a href="#修改表结构" class="headerlink" title="修改表结构"></a>修改表结构</h3><p><strong>重命名</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span>  <span class="keyword">table</span>  old_table_name  <span class="keyword">rename</span>  <span class="keyword">to</span>  new_table_name;</span><br></pre></td></tr></table></figure><p>把表score4修改成score5</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> score4 <span class="keyword">rename</span> <span class="keyword">to</span> score5;</span><br></pre></td></tr></table></figure><p><strong>增加/修改列信息:</strong></p><ul><li>查询表结构</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc score5;</span><br></pre></td></tr></table></figure><ul><li>添加列</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> score5 <span class="keyword">add</span> <span class="keyword">columns</span> (mycol <span class="keyword">string</span>, mysco <span class="built_in">int</span>);</span><br></pre></td></tr></table></figure><ul><li>更新列</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> score5 <span class="keyword">change</span> <span class="keyword">column</span> mysco mysconew <span class="built_in">int</span>;</span><br></pre></td></tr></table></figure><ul><li>删除表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> score5;</span><br></pre></td></tr></table></figure><h2 id="Hive表中加载数据"><a href="#Hive表中加载数据" class="headerlink" title="Hive表中加载数据"></a>Hive表中加载数据</h2><p><strong>直接向分区表中插入数据</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> score3 <span class="keyword">like</span> score;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> score3 <span class="keyword">partition</span>(<span class="keyword">month</span> =<span class="string">'201807'</span>) <span class="keyword">values</span> (<span class="string">'001'</span>,<span class="string">'002'</span>,<span class="string">'100'</span>);</span><br></pre></td></tr></table></figure><p><strong>通过load方式加载数据</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/export/servers/hivedatas/score.csv'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> score <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201806'</span>);</span><br></pre></td></tr></table></figure><p><strong>通过查询方式加载数据</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> score4 <span class="keyword">like</span> score;</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> score4 <span class="keyword">partition</span>(<span class="keyword">month</span> = <span class="string">'201806'</span>) <span class="keyword">select</span> s_id,c_id,s_score <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇总结Hive的交互方式和&lt;strong&gt;基本操作命令&lt;/strong&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://iceWind-R.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://iceWind-R.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据_08(Hive介绍和安装)</title>
    <link href="http://icewind-r.github.io/2020/08/21/%E5%A4%A7%E6%95%B0%E6%8D%AE-08/"/>
    <id>http://icewind-r.github.io/2020/08/21/%E5%A4%A7%E6%95%B0%E6%8D%AE-08/</id>
    <published>2020-08-21T13:52:07.000Z</published>
    <updated>2020-08-22T03:04:39.701Z</updated>
    
    <content type="html"><![CDATA[<p>本篇总结Hive（数据仓库）数据仓库和 Hive 的基本概念。</p><a id="more"></a><hr><h1 id="数据仓库"><a href="#数据仓库" class="headerlink" title="数据仓库"></a>数据仓库</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>英文名称为 Data Warehouse，可简写为 DW 或 DWH。数据仓库的目的是构建面相分析的集成化数据环境，为企业提供决策支持（Decision Support）。</p><p>​    数据仓库是存数据的，企业的各种数据往里面存，主要目的是为了分析有效数据，后续会基于它产出供分析挖掘的数据，或者数据应用需要的数据，如企业的分析性报告和各类报表等。</p><p>​    可以理解为：<strong>面向分析的存储系统。</strong></p><h2 id="主要特征"><a href="#主要特征" class="headerlink" title="主要特征"></a>主要特征</h2><p>数据仓库是面向主题的（Subject-Oriented）、集成的（Integrated）、非易失的（Non-Volatile）和时变的（Time-Variant）数据集合，用以支持管理决策。</p><h3 id="面向主题"><a href="#面向主题" class="headerlink" title="面向主题"></a>面向主题</h3><p>数据仓库是面向主题的,数据仓库通过一个个主题域将多个业务系统的数据加载到一起，为了各个主题（如：用户、订单、商品等）进行分析而建，操作型数据库是为了支撑各种业务而建立。</p><h3 id="集成性"><a href="#集成性" class="headerlink" title="集成性"></a>集成性</h3><p>数据仓库会将不同源数据库中的数据汇总到一起,数据仓库中的综合数据不能从原有的数据库系统直接得到。因此在数据进入数据仓库之前，必然要经过统一与整合，这一步是数据仓库建设中最关键、最复杂的一步(ETL)，要统一源数据中所有矛盾之处，如字段的同名异义、异名同义、单位不统一、字长不一致，等等。</p><h3 id="非易失性"><a href="#非易失性" class="headerlink" title="非易失性"></a>非易失性</h3><p>操作型数据库主要服务于日常的业务操作，使得数据库需要不断地对数据实时更新，以便迅速获得当前最新数据，不至于影响正常的业务运作。</p><p>​    在数据仓库中只要保存过去的业务数据，不需要每一笔业务都实时更新数据仓库，而是根据商业需要每隔一段时间把一批较新的数据导入数据仓库。 数据仓库的数据反映的是一段相当长的时间内历史数据的内容，是不同时点的数据库的集合，以及基于这些快照进行统计、综合和重组的导出数据。数据仓库中的数据一般仅执行查询操作，很少会有删除和更新。但是需定期加载和刷新数据。</p><h3 id="时变性"><a href="#时变性" class="headerlink" title="时变性"></a>时变性</h3><p>数据仓库包含各种粒度的历史数据。数据仓库中的数据可能与某个特定日期、星期、月份、季度或者年份有关。数据仓库的目的是通过分析企业过去一段时间业务的经营状况，挖掘其中隐藏的模式。虽然数据仓库的用户不能修改数据，但并不是说数据仓库的数据是永远不变的。分析的结果只能反映过去的情况，当业务变化后，挖掘出的模式会失去时效性。因此数据仓库的数据需要定时更新，以适应决策的需要。</p><h2 id="数据库与数据仓库的区别"><a href="#数据库与数据仓库的区别" class="headerlink" title="数据库与数据仓库的区别"></a>数据库与数据仓库的区别</h2><p>​    数据库与数据仓库的区别实际讲的是 <code>OLTP</code> 与 <code>OLAP</code> 的区别。</p><p>​    操作型处理，叫联机事务处理 OLTP（On-Line Transaction Processing，），也可以称面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性和并发支持的用户数等问题。传统的数据库系统作为数据管理的主要手段，主要用于操作型处理。</p><p>​    分析型处理，叫联机分析处理 OLAP（On-Line Analytical Processing）一般针对某些主题的历史数据进行分析，支持 管理决策。</p><p>首先要明白，数据仓库的出现，并不是要取代数据库。</p><ul><li>数据库是面向事务的设计，数据仓库是面向主题设计的。</li><li>数据库一般存储业务数据，数据仓库存储的一般是历史数据。</li><li>数据库设计是尽量避免冗余，一般针对某一业务应用进行设计，比如一张简单的User表，记录用户名、密码等简单数据即可，符合业务应用，但是不符合分析。数据仓库在设计是有意引入冗余，依照分析需求，分析维度、分析指标进行设计。</li><li>数据库是为捕获数据而设计，数据仓库是为分析数据而设计。</li></ul><p>​    <strong>数据仓库，是在数据库已经大量存在的情况下，为了进一步挖掘数据资源、为了决策需要而产生的，它决不是所谓的“大型数据库”。</strong></p><h2 id="数仓的分层架构"><a href="#数仓的分层架构" class="headerlink" title="数仓的分层架构"></a>数仓的分层架构</h2><p>按照数据流入流出的过程，数据仓库架构可分为三层——源数据、数据仓库、数据应用。</p><img src="/2020/08/21/%E5%A4%A7%E6%95%B0%E6%8D%AE-08/1.jpg" class><p>数据仓库的数据来源于不同的源数据，并提供多样的数据应用，数据自下而上流入数据仓库后向上层开放应用，而数据仓库只是中间集成化数据管理的一个平台。</p><ul><li><code>源数据层（ODS）</code>：此层数据无任何更改，直接沿用外围系统数据结构和数据，不对外开放；为临时存储层，是接口数据的临时存储区域，为后一步的数据处理做准备。</li><li><code>数据仓库层（DW）</code>：也称为细节层，DW层的数据应该是一致的、准确的、干净的数据，即对源系统数据进行了清洗（去除了杂质）后的数据。</li><li><code>数据应用层（DA或APP）</code>：前端应用直接读取的数据源；根据报表、专题分析需求而计算生成的数据。</li></ul><p>​    数据仓库从各数据源获取数据及在数据仓库内的数据转换和流动都可以认为是ETL（抽取Extra, 转化Transfer, 装载Load）的过程，ETL是数据仓库的流水线，也可以认为是数据仓库的血液，它维系着数据仓库中数据的新陈代谢，而数据仓库日常的管理和维护工作的大部分精力就是保持ETL的正常和稳定。</p><p><strong>为什么要对数据仓库分层？</strong></p><p>​    用空间换时间，通过大量的预处理来提升应用系统的用户体验（效率），因此数据仓库会存在大量冗余的数据；不分层的话，如果源业务系统的业务规则发生变化将会影响整个数据清洗过程，工作量巨大。</p><p>​    通过数据分层管理可以简化数据清洗的过程，因为把原来一步的工作分到了多个步骤去完成，相当于把一个复杂的工作拆成了多个简单的工作，把一个大的黑盒变成了一个白盒，每一层的处理逻辑都相对简单和容易理解，这样我们比较容易保证每一个步骤的正确性，当数据发生错误的时候，往往我们只需要局部调整某个步骤即可。</p><h2 id="数仓的元数据管理"><a href="#数仓的元数据管理" class="headerlink" title="数仓的元数据管理"></a>数仓的元数据管理</h2><p>​    元数据（Meta Date），主要记录数据仓库中模型的定义、各层级间的映射关系、监控数据仓库的数据状态及ETL的任务运行状态。一般会通过元数据资料库（Metadata Repository）来统一地存储和管理元数据，其主要目的是使数据仓库的设计、部署、操作和管理能达成协同和一致。</p><p>​    元数据是数据仓库管理系统的重要组成部分，元数据管理是企业级数据仓库中的关键组件，贯穿数据仓库构建的整个过程，直接影响着数据仓库的构建、使用和维护。</p><ul><li>构建数据仓库的主要步骤之一是ETL。这时元数据将发挥重要的作用，它定义了源数据系统到数据仓库的映射、数据转换的规则、数据仓库的逻辑结构、数据更新的规则、数据导入历史记录以及装载周期等相关内容。数据抽取和转换的专家以及数据仓库管理员正是通过元数据高效地构建数据仓库。</li><li>用户在使用数据仓库时，通过元数据访问数据，明确数据项的含义以及定制报表。</li><li>数据仓库的规模及其复杂性离不开正确的元数据管理，包括增加或移除外部数据源，改变数据清洗方法，控制出错的查询以及安排备份等。</li></ul><img src="/2020/08/21/%E5%A4%A7%E6%95%B0%E6%8D%AE-08/2.jpg" class><p>元数据可分为技术元数据和业务元数据。技术元数据为开发和管理数据仓库的IT 人员使用，它描述了与数据仓库开发、管理和维护相关的数据，包括数据源信息、数据转换描述、数据仓库模型、数据清洗与更新规则、数据映射和访问权限等。而业务元数据为管理层和业务分析人员服务，从业务角度描述数据，包括商务术语、数据仓库中有什么数据、数据的位置和数据的可用性等，帮助业务人员更好地理解数据仓库中哪些数据是可用的以及如何使用。</p><p>​    由上可见，元数据不仅定义了数据仓库中数据的模式、来源、抽取和转换规则等，而且是整个数据仓库系统运行的基础，元数据把数据仓库系统中各个松散的组件联系起来，组成了一个有机的整体。</p><h1 id="Hive-的基本概念"><a href="#Hive-的基本概念" class="headerlink" title="Hive 的基本概念"></a>Hive 的基本概念</h1><h2 id="Hive-简介"><a href="#Hive-简介" class="headerlink" title="Hive 简介"></a>Hive 简介</h2><p>​    Hive是基于Hadoop的一个数据仓库工具，可以将<strong>结构化的数据(有固定的字段，字段之间有固定的分隔符)</strong>文件映射为一张数据库表，并提供类SQL查询功能。</p><p>​    其本质是将SQL转换为MapReduce的任务进行运算，底层由HDFS来提供数据的存储，说白了hive可以理解为一个将SQL转换为MapReduce的任务的工具，甚至更进一步可以说hive就是一个MapReduce的客户端</p><p><strong>为什么使用 Hive</strong></p><ul><li>采用类SQL语法去操作数据，提供快速开发的能力。</li><li>避免了去写MapReduce，减少开发人员的学习成本。</li><li>功能扩展很方便。</li></ul><h2 id="Hive-架构"><a href="#Hive-架构" class="headerlink" title="Hive 架构"></a>Hive 架构</h2><img src="/2020/08/21/%E5%A4%A7%E6%95%B0%E6%8D%AE-08/3.jpg" class><ul><li><strong>用户接口：</strong> 包括CLI、JDBC/ODBC、WebGUI。其中，CLI(command line interface)为shell命令行；JDBC/ODBC是Hive的JAVA实现，与传统数据库JDBC类似；WebGUI是通过浏览器访问Hive。</li><li><strong>元数据存储：</strong> 通常是存储在关系数据库如mysql/derby中。Hive 将元数据存储在数据库中。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。</li><li><strong>解释器、编译器、优化器、执行器:</strong> 完成HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在HDFS 中，并在随后有MapReduce 调用执行。</li></ul><h2 id="Hive-与-Hadoop-的关系"><a href="#Hive-与-Hadoop-的关系" class="headerlink" title="Hive 与 Hadoop 的关系"></a>Hive 与 Hadoop 的关系</h2><p>Hive利用HDFS存储数据，利用MapReduce查询分析数据</p><img src="/2020/08/21/%E5%A4%A7%E6%95%B0%E6%8D%AE-08/4.jpg" class><h2 id="Hive与传统数据库对比"><a href="#Hive与传统数据库对比" class="headerlink" title="Hive与传统数据库对比"></a>Hive与传统数据库对比</h2><p>hive用于海量数据的离线数据分析</p><img src="/2020/08/21/%E5%A4%A7%E6%95%B0%E6%8D%AE-08/5.jpg" class><p><strong>总结：</strong>hive具有sql数据库的外表，但应用场景完全不同，hive只适合用来做批量数据统计分析</p><h1 id="Hive-的安装和配置"><a href="#Hive-的安装和配置" class="headerlink" title="Hive 的安装和配置"></a>Hive 的安装和配置</h1><h2 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h2><h3 id="下载Hive"><a href="#下载Hive" class="headerlink" title="下载Hive"></a>下载Hive</h3><ul><li><p>下载Hive安装包：这里我们选用hive的版本是 <strong>2.1.1</strong> 下载地址为：<a href="http://archive.apache.org/dist/hive/hive-2.1.1/apache-hive-2.1.1-bin.tar.gz" target="_blank" rel="noopener">http://archive.apache.org/dist/hive/hive-2.1.1/apache-hive-2.1.1-bin.tar.gz</a></p></li><li><p>解压到相应安装目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/softwares/</span><br><span class="line">tar -zxvf apache-hive-2.1.1-bin.tar.gz -C ../servers/</span><br></pre></td></tr></table></figure></li></ul><h3 id="安装MySQL"><a href="#安装MySQL" class="headerlink" title="安装MySQL"></a>安装MySQL</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">wget http://repo.mysql.com/mysql57-community-release-el7-9.noarch.rpm</span><br><span class="line">sudo rpm -ivh mysql57-community-release-el7-9.noarch.rpm</span><br><span class="line"></span><br><span class="line">yum install mysql mysql-server</span><br><span class="line"></span><br><span class="line">mysql_secure_installation</span><br><span class="line"></span><br><span class="line">grant all privileges on *.* to 'root'@'%' identified by '123456' with grant option;</span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure><h3 id="修改hive的配置文件"><a href="#修改hive的配置文件" class="headerlink" title="修改hive的配置文件"></a>修改hive的配置文件</h3><p><strong>修改hive-env.sh</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/apache-hive-2.1.1-bin/conf</span><br><span class="line">cp hive-env.sh.template hive-env.sh</span><br></pre></td></tr></table></figure><p>修改内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_HOME=/export/servers/hadoop-2.7.5</span><br><span class="line">export HIVE_CONF_DIR=/export/servers/apache-hive-2.1.1-bin/conf</span><br></pre></td></tr></table></figure><p><strong>修改hive-site.xml</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/apache-hive-2.1.1-bin/conf</span><br><span class="line">vim hive-site.xml</span><br></pre></td></tr></table></figure><p>修改内容</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://node03:3306/hive?createDatabaseIfNotExist=true<span class="symbol">&amp;amp;</span>useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>datanucleus.schema.autoCreateAll<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>node03<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="添加mysql的连接驱动包到hive的lib目录下"><a href="#添加mysql的连接驱动包到hive的lib目录下" class="headerlink" title="添加mysql的连接驱动包到hive的lib目录下"></a>添加mysql的连接驱动包到hive的lib目录下</h3><p>将mysql 的 驱动 mysql-connector-java-5.1.18-bin.jar 添加 hive的lib目录下即可。</p><h3 id="配置hive的环境变量"><a href="#配置hive的环境变量" class="headerlink" title="配置hive的环境变量"></a>配置hive的环境变量</h3><p> 在 /etc/profile 中添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_HOME=/export/servers/apache-hive-2.1.1-bin    (安装目录)</span><br><span class="line">export PATH=:$HIVE_HOME/bin:$PATH</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇总结Hive（数据仓库）数据仓库和 Hive 的基本概念。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://iceWind-R.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://iceWind-R.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据_07(Yarn)</title>
    <link href="http://icewind-r.github.io/2020/08/20/%E5%A4%A7%E6%95%B0%E6%8D%AE-07/"/>
    <id>http://icewind-r.github.io/2020/08/20/%E5%A4%A7%E6%95%B0%E6%8D%AE-07/</id>
    <published>2020-08-19T23:58:59.000Z</published>
    <updated>2020-08-21T13:53:14.437Z</updated>
    
    <content type="html"><![CDATA[<p>本篇总结<strong>Yarn资源调度</strong>的基础知识。</p><a id="more"></a><hr><h1 id="Yarn介绍"><a href="#Yarn介绍" class="headerlink" title="Yarn介绍"></a>Yarn介绍</h1><p>Apache Hadoop YARN （Yet Another Resource Negotiator，另一种资源协调者）是一 种新的 Hadoop 资源管理器，它是一个通用资源管理系统和调度平台，可为上层应用提供统 一的资源管理和调度，它的引入为集群在利用率、资源统一管理和数据共享等方面带来了巨大好处。</p><p>Yarn核心出发点是为了分离资源管理与作业监控，实现分离的做法是拥有一个全局的资源管理（ResourceManager，RM），以及每个应用程序对应一个的应用管理器（ApplicationMaster，AM）</p><p>总结一句话就是：<strong>Yarn就是为了调度资源，管理任务的</strong> 。</p><p>其调度分为两个层次：</p><ul><li>一级调度管理：计算资源管理（CPU，内存，网络IO，磁盘）</li><li>二级调度管理：任务内部的计算模型管理（AppMaster的任务精细化管理）</li></ul><h1 id="Yarn的主要组件介绍和作用"><a href="#Yarn的主要组件介绍和作用" class="headerlink" title="Yarn的主要组件介绍和作用"></a>Yarn的主要组件介绍和作用</h1><p>Yarn总体上是Master/Slave结构，主要由ResourceManager、NodeManager、ApplicationMaster和Container等几个组件构成。</p><img src="/2020/08/20/%E5%A4%A7%E6%95%B0%E6%8D%AE-07/1.gif" class><h2 id="1-ResourceManager"><a href="#1-ResourceManager" class="headerlink" title="1. ResourceManager"></a>1. ResourceManager</h2><p>每个Hadoop集群只会有一个ResourceManager（如果是HA的话会存在两个，但是有且只有一个处于active状态），它负责管理整个集群的计算资源，并将这些资源分别给应用程序。ResourceManager 内部主要有<strong>两个组件</strong>：</p><ol><li><strong>调度器</strong>（Scheduler）:负责资源的 分配。</li><li><strong>应用程序管理器</strong>  ApplicationsManager (AsM):这个组件用于管理整个集群应用程序的application masters，负责接收应用程序的提交；为application master启动提供资源；监控应用程序的运行进度以及在应用程序出现故障时重启它。</li></ol><h2 id="2-NodeManager"><a href="#2-NodeManager" class="headerlink" title="2. NodeManager"></a>2. NodeManager</h2><p>NodeManager是YARN中每个节点上的代理，它管理Hadoop集群中单个计算节点，根据相关的设置来启动容器的。NodeManager会定期向ResourceManager发送心跳信息来更新其健康状态。同时其也会监督Container的生命周期管理，监控每个Container的资源使用（内存、CPU等）情况，追踪节点健康状况，管理日志和不同应用程序用到的附属服务（auxiliary service）。</p><h2 id="3-ApplicationMaster"><a href="#3-ApplicationMaster" class="headerlink" title="3. ApplicationMaster"></a>3. ApplicationMaster</h2><p>ApplicationMaster是应用程序级别的，每个ApplicationMaster管理运行在YARN上的应用程序。YARN 将 ApplicationMaster看做是第三方组件，ApplicationMaster负责和ResourceManager scheduler协商资源，并且和NodeManager通信来运行相应的task。ResourceManager 为 ApplicationMaster 分配容器，这些容器将会用来运行task。ApplicationMaster 也会追踪应用程序的状态，监控容器的运行进度。当容器运行完成， ApplicationMaster 将会向 ResourceManager 注销这个容器；如果是整个作业运行完成，其也会向 ResourceManager 注销自己，这样这些资源就可以分配给其他的应用程序使用了。</p><h2 id="4-Container"><a href="#4-Container" class="headerlink" title="4. Container"></a>4. Container</h2><p>是Yarn中的资源抽象。Container是与特定节点绑定的，其包含了内存、CPU磁盘等逻辑资源。不过在现在的容器实现中，这些资源只包括了内存和CPU。容器是由 ResourceManager scheduler 服务动态分配的资源构成。容器授予 ApplicationMaster 使用特定主机的特定数量资源的权限。ApplicationMaster 也是在容器中运行的，其在应用程序分配的第一个容器中运行。</p><p>作业的完整运行如下所示：</p><img src="/2020/08/20/%E5%A4%A7%E6%95%B0%E6%8D%AE-07/2.jpeg" class>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇总结&lt;strong&gt;Yarn资源调度&lt;/strong&gt;的基础知识。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://iceWind-R.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="http://iceWind-R.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>大数据_06(MapReduce 的 Shuffle 详解（分区、排序、规约、分组）)</title>
    <link href="http://icewind-r.github.io/2020/08/13/%E5%A4%A7%E6%95%B0%E6%8D%AE-06/"/>
    <id>http://icewind-r.github.io/2020/08/13/%E5%A4%A7%E6%95%B0%E6%8D%AE-06/</id>
    <published>2020-08-13T02:25:03.000Z</published>
    <updated>2020-08-20T00:05:27.823Z</updated>
    
    <content type="html"><![CDATA[<p>本篇总结MapReduce<strong>分区、排序、规约</strong>的基础知识。</p><a id="more"></a><hr><h1 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h1><p>在 MapReduce 中, 通过我们指定分区, 会将同一个分区的数据发送到同一个 Reduce 当中进行处理</p><p>​    例如: 为了数据的统计, 可以把一批类似的数据发送到同一个 Reduce 当中, 在同一个 Reduce 当中统计相同类型的数据, 就可以实现类似的数据分区和统计等</p><p>​    其实就是相同类型的数据, 有共性的数据, 送到一起去处理</p><p>​    Reduce 当中默认的分区只有一个</p><h2 id="Mapper"><a href="#Mapper" class="headerlink" title="Mapper"></a>Mapper</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> partition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Counter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">* K1：行偏移量 LongWritable</span></span><br><span class="line"><span class="comment">* V1:行文本数据 Text</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">* K2:行文本数据 Text</span></span><br><span class="line"><span class="comment">* V2:NullWritable</span></span><br><span class="line"><span class="comment">* */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PartitionMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>,<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">// map 方法：将 K1 V1 转为 K2 V2</span></span><br><span class="line">    <span class="comment">// K2 就是 V1</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 方法1：定义计数器</span></span><br><span class="line">        <span class="comment">// 参数： 计数器类型，  计数器变量</span></span><br><span class="line">        Counter counter = context.getCounter(<span class="string">"MY_COUNTER"</span>, <span class="string">"partition_counter"</span>);</span><br><span class="line">        <span class="comment">//每次执行map方法，计数器被执行，1L 表示每次执行 +1</span></span><br><span class="line">        counter.increment(<span class="number">1L</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        context.write(value,NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Reducer"><a href="#Reducer" class="headerlink" title="Reducer"></a>Reducer</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PartitionerReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>,<span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;NullWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        context.write(key, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Partitioner"><a href="#Partitioner" class="headerlink" title="Partitioner"></a>Partitioner</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> partition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * 定义分区规则</span></span><br><span class="line"><span class="comment">    * 返回对应分区编号</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text text, NullWritable nullWritable, <span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 拆分行文本数据(K2),获取中奖字段值</span></span><br><span class="line">        String[] split = text.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">        String numStr = split[<span class="number">5</span>];</span><br><span class="line">        <span class="comment">// 判断中奖字段值和15的关系，然后返回对应的分区编号</span></span><br><span class="line">        <span class="keyword">if</span> (Integer.parseInt(numStr) &gt; <span class="number">15</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="JobMain"><a href="#JobMain" class="headerlink" title="JobMain"></a>JobMain</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> partition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobMain</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 创建job任务对象</span></span><br><span class="line">        Job job = Job.getInstance(<span class="keyword">super</span>.getConf(), <span class="string">"PartitionMapReduce"</span>);</span><br><span class="line">        <span class="comment">// 对job任务进行配置(8个步骤)</span></span><br><span class="line">        <span class="comment">// 1、设置输入类和输入的路径</span></span><br><span class="line">        job.setInputFormatClass(TextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        TextInputFormat.addInputPath(job,<span class="keyword">new</span> Path(<span class="string">"hdfs://bigdata1:8020/input"</span>));</span><br><span class="line">        TextInputFormat.addInputPath(job,<span class="keyword">new</span> Path(<span class="string">"file:///D:\\input"</span>));</span><br><span class="line">        <span class="comment">//2、设置mapper类和数据类型</span></span><br><span class="line">        job.setMapperClass(PartitionMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">//3、指定分区类</span></span><br><span class="line">        job.setPartitionerClass(MyPartitioner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">//4、5、6</span></span><br><span class="line">        <span class="comment">//7、指定reducer类和数据类型（K3 和 V3）</span></span><br><span class="line">        job.setReducerClass(PartitionerReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">// 设置reduceTask的个数</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//8、指定输出类和输出类型</span></span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        TextOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"hdfs://bigdata1:8020/out/partition_out"</span>));</span><br><span class="line">        TextOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"file:///D:\\out\\partition_out"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">// 启动job任务</span></span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(configuration, <span class="keyword">new</span> JobMain(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h2><p>其中在 mapper 和 reducer 里添加了计数器的相关使用，得到结果如下。</p><img src="/2020/08/13/%E5%A4%A7%E6%95%B0%E6%8D%AE-06/1.png" class><img src="/2020/08/13/%E5%A4%A7%E6%95%B0%E6%8D%AE-06/2.png" class><h1 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h1><ul><li>序列化 (Serialization) 是指把结构化对象转化为字节流</li><li>反序列化 (Deserialization) 是序列化的逆过程. 把字节流转为结构化对象. 当要在进程间传递对象或持久化对象的时候, 就需要序列化对象成字节流, 反之当要将接收到或从磁盘读取的字节流转换为对象, 就要进行反序列化</li><li>Java 的序列化 (Serializable) 是一个重量级序列化框架, 一个对象被序列化后, 会附带很多额外的信息 (各种校验信息, header, 继承体系等）, 不便于在网络中高效传输. 所以, Hadoop 自己开发了一套序列化机制(Writable), 精简高效. 不用像 Java 对象类一样传输多层的父子关系, 需要哪个属性就传输哪个属性值, 大大的减少网络传输的开销</li><li>Writable 是 Hadoop 的序列化格式, Hadoop 定义了这样一个 Writable 接口. 一个类要支持可序列化只需实现这个接口即可</li><li>另外 Writable 有一个子接口是 WritableComparable, WritableComparable 是既可实现序列化, 也可以对key进行比较, 我们这里可以通过自定义 Key 实现 WritableComparable 来实现我们的排序功能</li></ul><p>数据格式如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a   1</span><br><span class="line">a   9</span><br><span class="line">b   3</span><br><span class="line">a   7</span><br><span class="line">b   8</span><br><span class="line">b   10</span><br><span class="line">a   5</span><br></pre></td></tr></table></figure><p>要求:</p><ul><li>第一列按照字典顺序进行排列</li><li>第一列相同的时候, 第二列按照升序进行排列</li></ul><p>解决思路:</p><ul><li>将 Map 端输出的 <code>&lt;key,value&gt;</code> 中的 key 和 value 组合成一个新的 key (newKey), value值不变</li><li>这里就变成 <code>&lt;(key,value),value&gt;</code>, 在针对 newKey 排序的时候, 如果 key 相同, 就再对value进行排序</li></ul><h2 id="自定义类型和比较器"><a href="#自定义类型和比较器" class="headerlink" title="自定义类型和比较器"></a>自定义类型和比较器</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.cpz.mapreduce.sort;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> com.sun.xml.internal.fastinfoset.algorithm.BuiltInEncodingAlgorithm;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SortBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">SortBean</span>&gt; </span>&#123;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">private</span> String word;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> num;</span><br><span class="line"> </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span>  word + <span class="string">"\t"</span> + num ;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getWord</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> word;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setWord</span><span class="params">(String word)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.word = word;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getNum</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> num;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setNum</span><span class="params">(<span class="keyword">int</span> num)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.num = num;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 实现比较器，指定排序的规则</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    规则：</span></span><br><span class="line"><span class="comment">        第一列（word）按照字典顺序进行排列     //字典顺序：aac aad abc (按照ASCII码相减)</span></span><br><span class="line"><span class="comment">        第一列相同的私事后，第二列（num）按照升序进行排列</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(SortBean sortBean)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// // 先对第一列排序：word排序</span></span><br><span class="line">        <span class="keyword">int</span> result = <span class="keyword">this</span>.word.compareTo(sortBean.word);</span><br><span class="line">        <span class="comment">// 如果第一列相同，则按照第二列进行排序</span></span><br><span class="line">        <span class="keyword">if</span> (result == <span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">this</span>.num - sortBean.num;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 实现序列化</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dataOutput.writeUTF(word);</span><br><span class="line">        dataOutput.writeInt(num);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 实现反序列化</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.word = dataInput.readUTF();</span><br><span class="line">        <span class="keyword">this</span>.num = dataInput.readInt();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Mapper-1"><a href="#Mapper-1" class="headerlink" title="Mapper"></a>Mapper</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SortMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>,<span class="title">Text</span>,<span class="title">SortBean</span>,<span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 将行文本数据（V1)拆分，并将数据封装到 SortBean 对象，就可以得到K2</span></span><br><span class="line">        String[] split = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line"> </span><br><span class="line">        SortBean sortBean = <span class="keyword">new</span> SortBean();</span><br><span class="line">        sortBean.setWord(split[<span class="number">0</span>]);</span><br><span class="line">        sortBean.setNum(Integer.parseInt(split[<span class="number">1</span>]));</span><br><span class="line"> </span><br><span class="line">        <span class="comment">// 将K2 V2 写入上下文中</span></span><br><span class="line">        context.write(sortBean,NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Reducer-1"><a href="#Reducer-1" class="headerlink" title="Reducer"></a>Reducer</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SortReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">SortBean</span>,<span class="title">NullWritable</span>,<span class="title">SortBean</span>,<span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(SortBean key, Iterable&lt;NullWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        context.write(key,NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="JobMain-1"><a href="#JobMain-1" class="headerlink" title="JobMain"></a>JobMain</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobMain</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"> </span><br><span class="line">        Job job = Job.getInstance(<span class="keyword">super</span>.getConf(), <span class="string">"mapreduce_sort"</span>);</span><br><span class="line"> </span><br><span class="line">        job.setInputFormatClass(TextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        TextInputFormat.setInputPaths(job,<span class="keyword">new</span> Path(<span class="string">"d:\\mapreduce\\sort_in"</span>));</span><br><span class="line"> </span><br><span class="line">        job.setMapperClass(SortMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputKeyClass(SortBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"> </span><br><span class="line">        <span class="comment">// 排序：不需要配置，只需要定义排序规则（SortBean中的排序规则）</span></span><br><span class="line"> </span><br><span class="line">        job.setReducerClass(SortReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputKeyClass(SortBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"> </span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        TextOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(<span class="string">"d:\\mapreduce\\sort_out"</span>));</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">boolean</span> bl = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">return</span> bl ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(configuration, <span class="keyword">new</span> JobMain(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="规约"><a href="#规约" class="headerlink" title="规约"></a>规约</h1><p><strong>概念：</strong></p><p>​    每一个 map 都可能会产生大量的本地输出，Combiner 的作用就是对 map 端的输出先做一次合并，以减少在 map 和 reduce 节点之间的数据传输量，以提高网络IO 性能，是 MapReduce 的一种优化手段之一</p><ul><li>combiner 是 MR 程序中 Mapper 和 Reducer 之外的一种组件</li><li>combiner 组件的父类就是 Reducer</li><li>combiner 和 reducer 的区别在于运行的位置<ul><li>Combiner 是在每一个 maptask 所在的节点运行</li><li>Reducer 是接收全局所有 Mapper 的输出结果</li></ul></li><li>combiner 的意义就是对每一个 maptask 的输出进行局部汇总，以减小网络传输量</li></ul><p><strong>实现步骤：</strong></p><ol><li>自定义一个 combiner 继承 Reducer，重写 reduce 方法（与之前步骤几乎一致）</li><li>在 job 中设置 <code>job.setCombinerClass(CustomCombiner.class)</code></li></ol><p>Combiner 能够应用的前提是不能影响最终的业务逻辑，而且，combiner 的输出 kv 应该跟 reducer 的输入 kv 类型要对应起来</p><h2 id="Mapper-2"><a href="#Mapper-2" class="headerlink" title="Mapper"></a>Mapper</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CombinerMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>,<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">LongWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        Text text = <span class="keyword">new</span> Text();</span><br><span class="line">        LongWritable longWritable = <span class="keyword">new</span> LongWritable();</span><br><span class="line">        String[] split = value.toString().split(<span class="string">","</span>);</span><br><span class="line">        <span class="keyword">for</span> (String s : split) &#123;</span><br><span class="line">            text.set(s);</span><br><span class="line">            longWritable.set(<span class="number">1</span>);</span><br><span class="line">            context.write(text,longWritable);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="MyCombiner"><a href="#MyCombiner" class="headerlink" title="MyCombiner"></a>MyCombiner</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyCombiner</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>,<span class="title">LongWritable</span>,<span class="title">Text</span>,<span class="title">LongWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;LongWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (LongWritable value : values) &#123;</span><br><span class="line">            count += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        context.write(key,<span class="keyword">new</span> LongWritable(count));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Reducer-2"><a href="#Reducer-2" class="headerlink" title="Reducer"></a>Reducer</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CombinerReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>,<span class="title">LongWritable</span>,<span class="title">Text</span>,<span class="title">LongWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;LongWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (LongWritable value : values) &#123;</span><br><span class="line">            count += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        context.write(key,<span class="keyword">new</span> LongWritable(count));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="JobMain-2"><a href="#JobMain-2" class="headerlink" title="JobMain"></a>JobMain</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobMain</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"> </span><br><span class="line">        Job job = Job.getInstance(<span class="keyword">super</span>.getConf(), <span class="string">"mapreduce_combiner"</span>);</span><br><span class="line"> </span><br><span class="line">        job.setInputFormatClass(TextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        TextInputFormat.setInputPaths(job,<span class="keyword">new</span> Path(<span class="string">"d:\\mapreduce\\combiner_in"</span>));</span><br><span class="line"> </span><br><span class="line">        job.setMapperClass(CombinerMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(LongWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"> </span><br><span class="line">        job.setCombinerClass(MyCombiner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"> </span><br><span class="line">        job.setReducerClass(CombinerReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(LongWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"> </span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        TextOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(<span class="string">"d:\\mapreduce\\combiner_out"</span>));</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">boolean</span> bl = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">return</span> bl ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(configuration, <span class="keyword">new</span> JobMain(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇总结MapReduce&lt;strong&gt;分区、排序、规约&lt;/strong&gt;的基础知识。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://iceWind-R.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="http://iceWind-R.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>大数据_05(MapReduce基础和入门案例(单词统计))</title>
    <link href="http://icewind-r.github.io/2020/08/11/%E5%A4%A7%E6%95%B0%E6%8D%AE-05/"/>
    <id>http://icewind-r.github.io/2020/08/11/%E5%A4%A7%E6%95%B0%E6%8D%AE-05/</id>
    <published>2020-08-11T09:30:52.000Z</published>
    <updated>2020-08-13T02:10:47.942Z</updated>
    
    <content type="html"><![CDATA[<p>本篇总结MapReduce的入门基础知识。</p><a id="more"></a><hr><h1 id="MapReduce介绍"><a href="#MapReduce介绍" class="headerlink" title="MapReduce介绍"></a>MapReduce介绍</h1><p>MapReduce的核心思想是“分而治之”，适用于大量复杂的任务处理场景。</p><ul><li><p>Map负责“分”，即把复杂的任务分解为若干“简单的任务”来并行处理。可以进行拆分的前提是这些小任务可以并行计算，彼此间几乎没有依赖关系。</p></li><li><p>Reduce负责“合”，即对Map阶段的结果进行全局汇总。</p></li><li><p>MapReduce运行在Yarn集群，是一个“主从” 结构。</p><ol><li>ResourceManager</li><li>NodeManager</li></ol><p>这两个阶段合起来正是MapReduce思想的体现。</p></li></ul><img src="/2020/08/11/%E5%A4%A7%E6%95%B0%E6%8D%AE-05/1.png" class><h2 id="MapReduce设计构思"><a href="#MapReduce设计构思" class="headerlink" title="MapReduce设计构思"></a>MapReduce设计构思</h2><p>MapReduce是一个分布式运算程序的编程框架，核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在Hadoop集群上。</p><p>​    MapReduce设计并提供了统一的计算框架，为程序员隐藏了绝大多数系统层面的处理细节。为程序员提供一个抽象和高层的编程接口和框架。程序员仅需要关心其应用层的具体计算问题，仅需编写少量的处理应用本身计算问题的程序代码。如何具体完成这个并行计算任务所相关的诸多系统层细节被隐藏起来,交给计算框架去处理：</p><p>​    Map和Reduce为程序员提供了一个清晰的操作接口抽象描述。MapReduce中定义了如下的Map和Reduce两个抽象的编程接口，由用户去编程实现.Map和Reduce,MapReduce处理的数据类型是&lt;key,value&gt;键值对。</p><ul><li>Map: <code>(k1; v1) → [(k2; v2)]</code></li><li>Reduce: <code>(k2; [v2]) → [(k3; v3)]</code></li></ul><p>一个完整的mapreduce程序在分布式运行时有三类实例进程：</p><ol><li><code>MRAppMaster</code> 负责整个程序的过程调度及状态协调</li><li><code>MapTask</code> 负责map阶段的整个数据处理流程</li><li><code>ReduceTask</code> 负责reduce阶段的整个数据处理流程</li></ol><img src="/2020/08/11/%E5%A4%A7%E6%95%B0%E6%8D%AE-05/2.png" class><h1 id="MapReduce编程规范"><a href="#MapReduce编程规范" class="headerlink" title="MapReduce编程规范"></a>MapReduce编程规范</h1><p>MapReduce的开发一共有8个步骤：</p><h2 id="Map阶段2个步骤"><a href="#Map阶段2个步骤" class="headerlink" title="Map阶段2个步骤"></a>Map阶段2个步骤</h2><ul><li>设置InputFormat类，将数据切分为 key-value(<strong>K1</strong>和<strong>V1</strong>)对，输入到第二步</li><li>自定义Map逻辑，将第一步的结果转换成另一种的key-value(<strong>K2</strong>和<strong>V2</strong>)对，输出结果</li></ul><h2 id="Shuffle阶段4个步骤"><a href="#Shuffle阶段4个步骤" class="headerlink" title="Shuffle阶段4个步骤"></a>Shuffle阶段4个步骤</h2><ul><li>对输出的key-value对进行<strong>分区</strong></li><li>对不同分区的数据按照相同的Key<strong>排序</strong></li><li>（可选）对分组过的数据初步<strong>规约</strong>，降低数据的网络拷贝</li><li>对数据进行<strong>分组</strong>，相同Key的Value放入一个集合中</li></ul><h2 id="Reduce阶段2个步骤"><a href="#Reduce阶段2个步骤" class="headerlink" title="Reduce阶段2个步骤"></a>Reduce阶段2个步骤</h2><ul><li>对多个Map任务的结果进行排序以及合并，编写Reduce函数实现自己的逻辑，对输入的Key-Value进行处理，转为新的Key-Value（K3 和 V3）输出</li><li>设置OutputFormat处理并保存Reduce输出的 Key-Value数据</li></ul><h1 id="入门案例：单词统计"><a href="#入门案例：单词统计" class="headerlink" title="入门案例：单词统计"></a>入门案例：单词统计</h1><h2 id="数据格式准备"><a href="#数据格式准备" class="headerlink" title="数据格式准备"></a>数据格式准备</h2><h3 id="创建一个新的文件"><a href="#创建一个新的文件" class="headerlink" title="创建一个新的文件"></a>创建一个新的文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd &#x2F;export&#x2F;servers</span><br><span class="line">vim wordcount.txt</span><br></pre></td></tr></table></figure><h3 id="向其中放入以下内容并保存"><a href="#向其中放入以下内容并保存" class="headerlink" title="向其中放入以下内容并保存"></a>向其中放入以下内容并保存</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hello,world,hadoop</span><br><span class="line">hive,sqoop,flume,hello</span><br><span class="line">kitty,tom,jerry,world</span><br><span class="line">hadoop</span><br></pre></td></tr></table></figure><h3 id="上传到-HDFS"><a href="#上传到-HDFS" class="headerlink" title="上传到 HDFS"></a>上传到 HDFS</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir &#x2F;wordcount&#x2F;</span><br><span class="line">hdfs dfs -put wordcount.txt &#x2F;wordcount&#x2F;</span><br></pre></td></tr></table></figure><h2 id="具体程序代码实现"><a href="#具体程序代码实现" class="headerlink" title="具体程序代码实现"></a>具体程序代码实现</h2><h3 id="WordCountMapper-java"><a href="#WordCountMapper-java" class="headerlink" title="WordCountMapper.java"></a>WordCountMapper.java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">* 其中 Mapper类的四个泛型解释：</span></span><br><span class="line"><span class="comment">* KETIN：K1的类型</span></span><br><span class="line"><span class="comment">* VALUEIN:V1的类型</span></span><br><span class="line"><span class="comment">* KEYOUT:K2的类型</span></span><br><span class="line"><span class="comment">* VALUEOUT:V2的类型</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">* 使用已经给出定义好的类型，基本类型的封装，操作序列化起来更加方便</span></span><br><span class="line"><span class="comment">* &lt;Long, String, Long, String&gt;  - &gt; &lt;LongWritable, Text, Text, LongWritable&gt;</span></span><br><span class="line"><span class="comment">* */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">/* map方法就是将 K1 和 V1 转换为 K2 和 V2</span></span><br><span class="line"><span class="comment">    * 参数：</span></span><br><span class="line"><span class="comment">    *   key  : K1 行偏移量</span></span><br><span class="line"><span class="comment">    *   value: V1 每一行的文本数据</span></span><br><span class="line"><span class="comment">    *   context : 上下文对象</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 如何将 K1 和 V1 转换为 K2 和 V2</span></span><br><span class="line"><span class="comment">    * K1            V1</span></span><br><span class="line"><span class="comment">    * 0         hello,world,hadoop</span></span><br><span class="line"><span class="comment">    * 18        hdfs,hello,hadoop</span></span><br><span class="line"><span class="comment">    * ---------------------------</span></span><br><span class="line"><span class="comment">    * K2           V2</span></span><br><span class="line"><span class="comment">    * hello         1</span></span><br><span class="line"><span class="comment">    * world         1</span></span><br><span class="line"><span class="comment">    * hadoop        1</span></span><br><span class="line"><span class="comment">    * hdfs          1</span></span><br><span class="line"><span class="comment">    * hello         1</span></span><br><span class="line"><span class="comment">    * hadoop        1</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        Text text = <span class="keyword">new</span> Text();</span><br><span class="line">        LongWritable longWritable = <span class="keyword">new</span> LongWritable();</span><br><span class="line">        <span class="comment">// 将一行的文本数据进行拆分</span></span><br><span class="line">        String[] split = value.toString().split(<span class="string">","</span>);</span><br><span class="line">        <span class="comment">// 遍历数组，组装K2 和 V2</span></span><br><span class="line">        <span class="keyword">for</span> (String word : split) &#123;</span><br><span class="line">            text.set(word);</span><br><span class="line">            longWritable.set(<span class="number">1</span>);</span><br><span class="line">            <span class="comment">// 将 K2 和 V2 写入上下文</span></span><br><span class="line">            context.write(text, longWritable);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="WordCountReducer-java"><a href="#WordCountReducer-java" class="headerlink" title="WordCountReducer.java"></a>WordCountReducer.java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">* 四个泛型解释：</span></span><br><span class="line"><span class="comment">* KEYIN:K2类型</span></span><br><span class="line"><span class="comment">* VALUEIN:V2类型</span></span><br><span class="line"><span class="comment">* KEYOUT：K3类型</span></span><br><span class="line"><span class="comment">* VALUEOUT：V3类型</span></span><br><span class="line"><span class="comment">* */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * reduce方法作用：将新的 K2 和 V2 转换为 K3 和 V3，并写入上下文</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 参数：</span></span><br><span class="line"><span class="comment">    *   key：新 K2</span></span><br><span class="line"><span class="comment">    *   values：集合 新V2</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 如何将新的 K2 和 V2 转换为 K3 和 V3</span></span><br><span class="line"><span class="comment">    * 新 K2          V2</span></span><br><span class="line"><span class="comment">    *   world       &lt;1,1,1&gt;</span></span><br><span class="line"><span class="comment">    *   hello       &lt;1,1&gt;</span></span><br><span class="line"><span class="comment">    *   hadoop      &lt;1&gt;</span></span><br><span class="line"><span class="comment">    * ---------------------</span></span><br><span class="line"><span class="comment">    *   K3          V3</span></span><br><span class="line"><span class="comment">    *   hello       2</span></span><br><span class="line"><span class="comment">    *   world       3</span></span><br><span class="line"><span class="comment">    *   hadoop      1</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;LongWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> count = <span class="number">0</span>;</span><br><span class="line">        LongWritable longWritable = <span class="keyword">new</span> LongWritable();</span><br><span class="line">        <span class="comment">// 遍历values集合，将集合中的数字相加得到 V3</span></span><br><span class="line">        <span class="keyword">for</span> (LongWritable value : values) &#123;</span><br><span class="line">            count += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 将 K3 和 V3 写入上下文中</span></span><br><span class="line">        longWritable.set(count);</span><br><span class="line">        context.write(key, longWritable);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="JobMain-java"><a href="#JobMain-java" class="headerlink" title="JobMain.java"></a>JobMain.java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobMain</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 该方法用于指定一个Job任务</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 创建一个job任务对象</span></span><br><span class="line">        <span class="comment">// 第一个参数是一个configuration，下面的main方法调用时已经传入，存在Configured类中，通过getConf()方法获取</span></span><br><span class="line">        Job job = Job.getInstance(<span class="keyword">super</span>.getConf(), <span class="string">"wordCount"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//job.setJarByClass(JobMain.class); // 如果打包出错，则需要该行代码</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置job任务对象 (8个步骤)</span></span><br><span class="line">        <span class="comment">//1、指定文件的读取方式和读取路径</span></span><br><span class="line">        job.setInputFormatClass(TextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        TextInputFormat.addInputPath(job, <span class="keyword">new</span> Path(<span class="string">"hdfs://bigdata1:8020/wordcount"</span>));</span><br><span class="line">        <span class="comment">// 本地执行方法，执行JobMain主函数即可，提前准备好文件，该路径下的wordcount.txt</span></span><br><span class="line">        <span class="comment">//TextInputFormat.addInputPath(job, new Path("file:///D:\\mapReduce\\input"));</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//2、指定Map阶段的处理方式 和 数据类型</span></span><br><span class="line">        job.setMapperClass(WordCountMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">//设置Map阶段K2的类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">// 设置Map阶段 V2 的类型</span></span><br><span class="line">        job.setMapOutputValueClass(LongWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 第3，4，5，6采用默认的方式</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7、指定Reduce阶段的处理方式和数据类型</span></span><br><span class="line">        job.setReducerClass(WordCountReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(LongWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//8、设置输出类型</span></span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">// 设置输出路径，并判断该目录是否存在，存在则删除</span></span><br><span class="line">        Path path = <span class="keyword">new</span> Path(<span class="string">"hdfs://bigdata1:8020/wordCount_out"</span>);</span><br><span class="line">        TextOutputFormat.setOutputPath(job,path);</span><br><span class="line">        <span class="comment">//TextOutputFormat.setOutputPath(job,new Path("file:///D:\\mapReduce\\output"));</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 判断目标目录是否存在</span></span><br><span class="line">        FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://bigdata1:8020/"</span>), <span class="keyword">new</span> Configuration());</span><br><span class="line">        <span class="keyword">if</span>(fileSystem.exists(path))&#123;</span><br><span class="line">            fileSystem.delete(path, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 等待任务结束</span></span><br><span class="line">        <span class="keyword">boolean</span> bl = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">return</span> bl? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">// 启动 job 任务，实际就是调用上面的run方法</span></span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(configuration, <span class="keyword">new</span> JobMain(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="运行方式"><a href="#运行方式" class="headerlink" title="运行方式"></a>运行方式</h2><p>写好程序，有两种运行方式，一种是本地运行，在JobMain函数中注释的部分即是。然后运行主函数即可，这种方法一般用于本地的测试。</p><p>另一种是<strong>集群运行</strong>模式</p><ul><li><p>将MapReduce程序提交给Yarn集群，分发到很多的结点上并执行</p></li><li><p>处理的数据和输出结果应该位于HDFS文件系统</p></li><li><p>提交代码的实现步骤：将程序打成jar包并上传到虚拟机服务器上，使用hdfs命令执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar original-mapreduce-1.0-SNAPSHOT.jar(jar包名) JobMain(主函数名，函数右键copy reference)</span><br></pre></td></tr></table></figure><blockquote><p>打成jar包方式：Maven项目里的Lifecycle下的package功能，得到的包位于target目录下，会有两个jar包，体积一大一小，本质并无不同，都可使用上传。</p></blockquote></li></ul><p>本程序将结果输出到HDFS的/wordCount_out目录下，可以 <code>http://bigdata1:50070/explorer.html#/</code>下查看。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇总结MapReduce的入门基础知识。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://iceWind-R.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="http://iceWind-R.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>大数据_04(HDFS_API操作)</title>
    <link href="http://icewind-r.github.io/2020/08/10/%E5%A4%A7%E6%95%B0%E6%8D%AE-04/"/>
    <id>http://icewind-r.github.io/2020/08/10/%E5%A4%A7%E6%95%B0%E6%8D%AE-04/</id>
    <published>2020-08-10T10:11:20.000Z</published>
    <updated>2020-08-12T02:23:40.927Z</updated>
    
    <content type="html"><![CDATA[<p>本篇总结HDFS在windows操作系统Java环境下的API操作。</p><a id="more"></a><hr><h1 id="HDFS的Java-api操作"><a href="#HDFS的Java-api操作" class="headerlink" title="HDFS的Java_api操作"></a>HDFS的Java_api操作</h1><h2 id="配置Windows下的-Hadoop环境"><a href="#配置Windows下的-Hadoop环境" class="headerlink" title="配置Windows下的 Hadoop环境"></a>配置Windows下的 Hadoop环境</h2><p>在Windows系统需要配置Hadoop运行环境，相当于Windows是一个Hadoop客户端。</p><p>不配置而直接运行代码会出现以下问题：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Could not locate executable null\bin\winutils.exe <span class="keyword">in</span> the hadoop binaries</span><br><span class="line">1</span><br></pre></td></tr></table></figure><p><strong>原因：</strong> 缺少winutils.exe</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Unable to load native-hadoop library <span class="keyword">for</span> your platform..using <span class="built_in">builtin</span>-Java classes <span class="built_in">where</span> applicable</span><br><span class="line">1</span><br></pre></td></tr></table></figure><p><strong>原因：</strong> 缺少hadoop.dll</p><h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><p><strong>1、</strong>首先下载Hadoop在Windows上的工具包，下载地址：<a href="https://github.com/steveloughran/winutils" target="_blank" rel="noopener">https://github.com/steveloughran/winutils</a> 。</p><p><strong>2、</strong>得到后，将其解压到一个无中文无空格的目录下，并配置环境变量。</p><img src="/2020/08/10/%E5%A4%A7%E6%95%B0%E6%8D%AE-04/1.png" class><p>Path下添加：<code>%HADOOP_HOME%\bin</code></p><p><strong>3、</strong>将下载的包内的 <code>hadoop.dll</code> 拷贝一份到 C:\Windows\System32 目录下。</p><p><strong>4、</strong> 重启电脑，完成。</p><h2 id="导入Maven依赖"><a href="#导入Maven依赖" class="headerlink" title="导入Maven依赖"></a>导入Maven依赖</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-mapreduce-client-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">encoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">encoding</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">minimizeJar</span>&gt;</span>true<span class="tag">&lt;/<span class="name">minimizeJar</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="使用URL方式访问数据（了解）"><a href="#使用URL方式访问数据（了解）" class="headerlink" title="使用URL方式访问数据（了解）"></a>使用URL方式访问数据（了解）</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">urlHDFS</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 注册URL</span></span><br><span class="line">    URL.setURLStreamHandlerFactory(<span class="keyword">new</span> FsUrlStreamHandlerFactory());</span><br><span class="line">    <span class="comment">//获取hdfs文件的输入流</span></span><br><span class="line">    InputStream inputStream = <span class="keyword">new</span> URL(<span class="string">"hdfs://bigdata1:8020/a.txt"</span>).openStream();</span><br><span class="line">    <span class="comment">// 获取本地文件的输出流</span></span><br><span class="line">    FileOutputStream fileOutputStream = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"D:\\hello.txt"</span>));</span><br><span class="line">    <span class="comment">// 实现文件的拷贝</span></span><br><span class="line">    IOUtils.copy(inputStream, fileOutputStream);</span><br><span class="line">    <span class="comment">// 关流</span></span><br><span class="line">    IOUtils.closeQuietly(inputStream);</span><br><span class="line">    IOUtils.closeQuietly(fileOutputStream);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="使用文件系统方式访问数据（掌握）"><a href="#使用文件系统方式访问数据（掌握）" class="headerlink" title="使用文件系统方式访问数据（掌握）"></a>使用文件系统方式访问数据（掌握）</h2><h3 id="涉及的主要类"><a href="#涉及的主要类" class="headerlink" title="涉及的主要类"></a>涉及的主要类</h3><p>在Java中操作HDFS，主要涉及以下Class：</p><ul><li><p><code>Configuration</code>：该类的对象封装了客户端或者服务器的配置</p></li><li><p><code>FileSystem</code>：该类的对象是一个文件系统对象，可以用该对象的一些方法来对文件进行操作，通过FileSystem的静态方法 get 获得该对象</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FileSystem fs = FileSystem.get(conf) # 就是Configuration类的对象</span><br></pre></td></tr></table></figure><ul><li><code>get</code> 方法从 <code>conf</code> 中的一个参数 <code>fs.defaultFS</code> 的配置值判断具体是什么类型的文件系统</li><li>如果我们的代码中没有指定<code>fs.defaultFS</code>，并且工程ClassPath下也没有给定相应的配置，<code>conf</code>中的默认值来自于Hadoop的Jar包中的<code>core-default.xml</code></li><li>默认值为<code>file:///</code>，则获取的不是一个DistributedFileSystem的实例，而是一个本地文件系统的客户端对象</li></ul></li></ul><h3 id="获取FileSystem的四种方式"><a href="#获取FileSystem的四种方式" class="headerlink" title="获取FileSystem的四种方式"></a>获取FileSystem的四种方式</h3><ul><li><p>第一种</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getFileSystem1</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 创建一个Configuration对象</span></span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 设置文件系统类型</span></span><br><span class="line">    configuration.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://bigdata1:8020"</span>);</span><br><span class="line">    <span class="comment">// 获取指定的文件系统</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(configuration);</span><br><span class="line">    <span class="comment">// 输出 DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_373282607_1, ugi=11655 (auth:SIMPLE)]]</span></span><br><span class="line">    System.out.println(fileSystem);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>第二种</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getFileSystem2</span><span class="params">()</span> <span class="keyword">throws</span> IOException, URISyntaxException </span>&#123;</span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://bigdata1:8020"</span>), <span class="keyword">new</span> Configuration());</span><br><span class="line">    System.out.println(fileSystem);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>第三种</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getFileSystem3</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 创建一个Configuration对象</span></span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 设置文件系统类型</span></span><br><span class="line">    configuration.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://bigdata1:8020"</span>);</span><br><span class="line">    <span class="comment">// 获取指定的文件系统</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.newInstance(configuration);</span><br><span class="line">    <span class="comment">// 输出 DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_373282607_1, ugi=11655 (auth:SIMPLE)]]</span></span><br><span class="line">    System.out.println(fileSystem);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>第四种</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getFileSystem4</span><span class="params">()</span> <span class="keyword">throws</span> IOException, URISyntaxException </span>&#123;</span><br><span class="line">    FileSystem fileSystem = FileSystem.newInstance(<span class="keyword">new</span> URI(<span class="string">"hdfs://bigdata1:8020"</span>), <span class="keyword">new</span> Configuration());</span><br><span class="line">    System.out.println(fileSystem);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><blockquote><p>注意：1、3比较相似，2、4比较相似，主要是get方法和 newInstance方法的使用</p></blockquote><h3 id="遍历HDFS所有文件信息"><a href="#遍历HDFS所有文件信息" class="headerlink" title="遍历HDFS所有文件信息"></a>遍历HDFS所有文件信息</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listFiles</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 获取FileSystem实例</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://bigdata1:8020"</span>), <span class="keyword">new</span> Configuration());</span><br><span class="line">    <span class="comment">// 调用方法listFiles 获取一个目录下的文件信息，为一个迭代器对象</span></span><br><span class="line">    <span class="comment">// 第一个参数：指定目录</span></span><br><span class="line">    <span class="comment">// 第二个参数，是否迭代获取</span></span><br><span class="line">    RemoteIterator&lt;LocatedFileStatus&gt; iterator = fileSystem.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>), <span class="keyword">true</span>);</span><br><span class="line">    <span class="comment">//遍历迭代器，获取文件的详细信息</span></span><br><span class="line">    <span class="keyword">while</span> (iterator.hasNext())&#123;</span><br><span class="line">        LocatedFileStatus fileStatus = iterator.next();</span><br><span class="line">        <span class="comment">// 获取文件的绝对路径："hdfs://bigdata1:8020/xxx"</span></span><br><span class="line">        System.out.println(fileStatus.getPath() + <span class="string">"  ---  "</span> + fileStatus.getPath().getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//文件的Block信息</span></span><br><span class="line">        BlockLocation[] blockLocations = fileStatus.getBlockLocations();</span><br><span class="line">        System.out.println(<span class="string">"Block数："</span> + blockLocations.length);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="HDFS创建文件夹"><a href="#HDFS创建文件夹" class="headerlink" title="HDFS创建文件夹"></a>HDFS创建文件夹</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">mkdirs</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException </span>&#123;</span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://bigdata1:8020/a.txt"</span>), <span class="keyword">new</span> Configuration());</span><br><span class="line">    <span class="comment">// 创建文件夹</span></span><br><span class="line">    <span class="keyword">boolean</span> bl = fileSystem.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/aaa/bbb/ccc"</span>));</span><br><span class="line">    <span class="comment">//创建文件</span></span><br><span class="line">    fileSystem.create(<span class="keyword">new</span> Path(<span class="string">"/aaa/aaa.txt"</span>));</span><br><span class="line">    <span class="comment">// 两个创建方法都为递归创建</span></span><br><span class="line">    System.out.println(bl);</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="文件的下载"><a href="#文件的下载" class="headerlink" title="文件的下载"></a>文件的下载</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">downloadFile</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 获取FileSystem</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://bigdata1:8020/a.txt"</span>), <span class="keyword">new</span> Configuration());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取hdfs的输入流 </span></span><br><span class="line">    FSDataInputStream inputStream = fileSystem.open(<span class="keyword">new</span> Path(<span class="string">"/a.txt"</span>));</span><br><span class="line">    <span class="comment">// 获取本地路径的输出流 </span></span><br><span class="line">    FileOutputStream outputStream = <span class="keyword">new</span> FileOutputStream(<span class="string">"D://a.txt"</span>);</span><br><span class="line">    <span class="comment">// 文件的拷贝 </span></span><br><span class="line">    IOUtils.copy(inputStream, outputStream);</span><br><span class="line">    <span class="comment">// 关闭流 </span></span><br><span class="line">    IOUtils.closeQuietly(inputStream); </span><br><span class="line">    IOUtils.closeQuietly(outputStream); </span><br><span class="line">    fileSystem.close(); </span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 实现文件的下载 2</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">downloadFile2</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException </span>&#123;</span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://bigdata1:8020/a.txt"</span>), <span class="keyword">new</span> Configuration());</span><br><span class="line">    fileSystem.copyToLocalFile(<span class="keyword">new</span> Path(<span class="string">"/a.txt"</span>), <span class="keyword">new</span> Path(<span class="string">"D://a.txt"</span>));</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="文件的上传"><a href="#文件的上传" class="headerlink" title="文件的上传"></a>文件的上传</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">uploadFile</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException </span>&#123;</span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://bigdata1:8020/a.txt"</span>), <span class="keyword">new</span> Configuration());</span><br><span class="line">    fileSystem.copyFromLocalFile(<span class="keyword">new</span> Path(<span class="string">"D://b.txt"</span>),<span class="keyword">new</span> Path(<span class="string">"/"</span>));</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="HDFS的权限访问控制"><a href="#HDFS的权限访问控制" class="headerlink" title="HDFS的权限访问控制"></a>HDFS的权限访问控制</h3><p>首先进入Hadoop的安装目录下的/etc/hadoop/hdfs-site.xml，修改permission为true，代表启动权限。启动后通过命令行的权限修改才能生效，修改配置文件需要重启才能生效。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -chmod 000 /a.txt</span><br></pre></td></tr></table></figure><p>数字代表权限等级，当开启权限控制时，文件会有其对应的Owner，不是相应的Owner仍然无法访问资源。这时我们可以在get方法内指定伪装用户对资源进行访问：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://bigdata1:8020/a.txt"</span>), <span class="keyword">new</span> Configuration(), <span class="string">"root"</span>);</span><br></pre></td></tr></table></figure><h3 id="小文件合并"><a href="#小文件合并" class="headerlink" title="小文件合并"></a>小文件合并</h3><p>由于Hadoop擅长存储大文件，因为大文件的元数据信息比较少。如果集群中有大量的小文件，则需要维护大量的元数据，增大内存压力。所以有必要将小文件合并成大文件一起处理。</p><p>在HDFS的Shell命令下，可以用如下命令讲很多HDFS文件合并成一个大文件下载到本地</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers</span><br><span class="line">hdfs dfs -getmerge /config/*.xml ./hello.xml # 表示合并文件，下载到当前目录下的hello.xml</span><br></pre></td></tr></table></figure><p>同样也可以在上传时将小文件合并到一个大文件里面去</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">mergeFile</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 获取FileSystem</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://bigdata1:8020/a.txt"</span>), <span class="keyword">new</span> Configuration(), <span class="string">"root"</span>);</span><br><span class="line">    <span class="comment">// 获取hdfs大文件的输出流，创建一个承载所有内容的大文件</span></span><br><span class="line">    FSDataOutputStream outputStream = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">"/big.txt"</span>));</span><br><span class="line">    <span class="comment">// 获取一个本地文件系统</span></span><br><span class="line">    LocalFileSystem localFileSystem = FileSystem.getLocal(<span class="keyword">new</span> Configuration());</span><br><span class="line">    <span class="comment">// 获取本地文件夹下所有文件的详情,input是提前准备的文件夹，里面有一些小文件</span></span><br><span class="line">    FileStatus[] fileStatuses = localFileSystem.listStatus(<span class="keyword">new</span> Path(<span class="string">"D:\\input"</span>));</span><br><span class="line">    <span class="comment">// 遍历每个文件，获得每个文件的输入流</span></span><br><span class="line">    <span class="keyword">for</span> (FileStatus fileStatus : fileStatuses) &#123;</span><br><span class="line">        FSDataInputStream inputStream = localFileSystem.open(fileStatus.getPath());</span><br><span class="line">        <span class="comment">// 将小文件的数据复制到大文件</span></span><br><span class="line">        IOUtils.copy(inputStream, outputStream);</span><br><span class="line">        IOUtils.closeQuietly(inputStream);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 关闭流</span></span><br><span class="line">    IOUtils.closeQuietly(outputStream);</span><br><span class="line">    localFileSystem.close();</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="HDFS的高可用机制"><a href="#HDFS的高可用机制" class="headerlink" title="HDFS的高可用机制"></a>HDFS的高可用机制</h1><p>在Hadoop2.X之前，Namenode是HDFS集群中可能发生单点故障的节点，每个HDFS集群只有一个namenode，一旦这个节点不可用，则整个HDFS集群将处于不可用状态。<br>HDFS高可用（HA）方案就是为了解决上述问题而产生的，在HA HDFS集群中会同时运行两个Namenode，一个作为活动的Namenode（Active），一个作为备份的Namenode（Standby）。备份的Namenode的命名空间与活动的Namenode是实时同步的，所以当活动的Namenode发生故障而停止服务时，备份Namenode可以立即切换为活动状态，而不影响HDFS集群服务。</p><p>详情：<a href="https://blog.csdn.net/u012736748/article/details/79534019" target="_blank" rel="noopener">https://blog.csdn.net/u012736748/article/details/79534019</a></p><h1 id="Hadoop联邦机制"><a href="#Hadoop联邦机制" class="headerlink" title="Hadoop联邦机制"></a>Hadoop联邦机制</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇总结HDFS在windows操作系统Java环境下的API操作。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://iceWind-R.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="http://iceWind-R.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>大数据_03(HDFS基础)</title>
    <link href="http://icewind-r.github.io/2020/08/09/%E5%A4%A7%E6%95%B0%E6%8D%AE-03/"/>
    <id>http://icewind-r.github.io/2020/08/09/%E5%A4%A7%E6%95%B0%E6%8D%AE-03/</id>
    <published>2020-08-09T01:45:01.000Z</published>
    <updated>2020-08-10T10:11:46.531Z</updated>
    
    <content type="html"><![CDATA[<p>本篇开始学习大数据Hadoop技术中的<strong>核心</strong>之一 —— HDFS。</p><a id="more"></a><hr><h1 id="HDFS概述"><a href="#HDFS概述" class="headerlink" title="HDFS概述"></a>HDFS概述</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>HDFS （Hadoop Distributed File System）是Hadoop抽象文件系统的一种实现。Hadoop抽象文件系统可以与本地系统、Amazon S3等集成，甚至可以通过Web协议（webhsfs）来操作。HDFS的文件分布在集群机器上，同时提供副本进行容错及可靠性保证。例如客户端写入读取文件的直接操作都是分布在集群各个机器上的，没有单点性能压力。</p><h2 id="HDFS设计原则"><a href="#HDFS设计原则" class="headerlink" title="HDFS设计原则"></a>HDFS设计原则</h2><h3 id="设计目标"><a href="#设计目标" class="headerlink" title="设计目标"></a>设计目标</h3><ul><li><strong>存储非常大的文件</strong>：这里非常大指的是几百M、G、或者TB级别。实际应用中已有很多集群存储的数据达到PB级别。根据Hadoop官网，Yahoo！的Hadoop集群约有10万颗CPU，运行在4万个机器节点上。更多世界上的Hadoop集群使用情况，参考<a href="https://wiki.apache.org/hadoop/PoweredBy" target="_blank" rel="noopener">Hadoop官网</a>.</li><li><strong>采用流式的数据访问方式</strong>: HDFS基于这样的一个假设：最有效的数据处理模式是一次写入、多次读取数据集经常从数据源生成或者拷贝一次，然后在其上做很多分析工作<br>分析工作经常读取其中的大部分数据，即使不是全部。 因此读取整个数据集所需时间比读取第一条记录的延时更重要。</li><li><strong>运行于商业硬件上</strong>: Hadoop不需要特别贵的、reliable的（可靠的）机器，可运行于普通商用机器（可以从多家供应商采购） ，商用机器不代表低端机器。在集群中（尤其是大的集群），节点失败率是比较高的HDFS的目标是确保集群在节点失败的时候不会让用户感觉到明显的中断。</li></ul><h3 id="HDFS不适合的应用类型"><a href="#HDFS不适合的应用类型" class="headerlink" title="HDFS不适合的应用类型"></a>HDFS不适合的应用类型</h3><p>有些场景不适合使用HDFS来存储数据。下面列举几个：</p><p>1） <strong>低延时的数据访问</strong><br>对延时要求在毫秒级别的应用，不适合采用HDFS。HDFS是为高吞吐数据传输设计的,因此可能牺牲延时HBase更适合低延时的数据访问。</p><p>2）<strong>大量小文件</strong><br>文件的元数据（如目录结构，文件block的节点列表，block-node mapping）保存在NameNode的内存中， 整个文件系统的文件数量会受限于NameNode的内存大小。<br>经验而言，一个文件/目录/文件块一般占有150字节的元数据内存空间。如果有100万个文件，每个文件占用1个文件块，则需要大约300M的内存。因此十亿级别的文件数量在现有商用机器上难以支持。</p><p>3）<strong>多方读写，需要任意的文件修改</strong><br>HDFS采用追加（append-only）的方式写入数据。不支持文件任意offset的修改。不支持多个写入器（writer）。</p><h1 id="HDFS的架构"><a href="#HDFS的架构" class="headerlink" title="HDFS的架构"></a>HDFS的架构</h1><p>HDFS是一个 主 / 从（Master / Slave）体系结构。</p><p>HDFS由四部分组成，HDFS Client、NameNode，DataNode 和 Secondary NameNode。</p><h2 id="1、Client：就是客户端"><a href="#1、Client：就是客户端" class="headerlink" title="1、Client：就是客户端"></a>1、Client：就是客户端</h2><ul><li>文件切片。文件上传到HDFS时，Client将文件切分成一个一个的 Block进行存储。</li><li>与NameNode交互，获取文件的位置信息。</li><li>与DataNode交互，读取或者写入数据。</li><li>Client提供一些命令来管理和访问HDFS，比如启动或关闭HDFS。</li></ul><h2 id="2、NameNode：就是Master，是一个主管、管理者"><a href="#2、NameNode：就是Master，是一个主管、管理者" class="headerlink" title="2、NameNode：就是Master，是一个主管、管理者"></a>2、NameNode：就是Master，是一个主管、管理者</h2><ul><li>管理HDFS的名称空间 和 文件数据块（Block）的映射信息，整个HDFS可存储的文件数受限于NameNode的内存大小。在内存中加载文件系统中每个文件和每个数据块的引用关系（文件、Block 和 DataNode之间的映射关系），数据会定期保存在本地磁盘（fsImage 镜像 文件 和 edits 日志 文件）。</li><li>配置副本策略：文件数据块到底存放到那些DataNode上，是由NameNode决定的，它根据全局情况做出放置副本的决定。</li><li>处理客户端读写请求。数据流不经过NameNode，会询问它与那个DataNode联系</li><li>NameNode心跳机制：DataNode定期发给NameNode一个个包，称为心跳机制。若NameNode没有收到，则认为相应的DataNode已经宕机，这时候NN准备要把DN上的数据块进行重新复制</li></ul><h2 id="3、DataNode-：就是Slave。"><a href="#3、DataNode-：就是Slave。" class="headerlink" title="3、DataNode ：就是Slave。"></a>3、DataNode ：就是Slave。</h2><p>Name Node下达命令，DataNode执行</p><ul><li>存储实际的数据块</li><li>执行数据的 读/写 操作</li><li>周期性的向NameNode汇报心跳信息、数据块信息 和 缓存数据块信息</li></ul><h2 id="4、Secondary-NameNode"><a href="#4、Secondary-NameNode" class="headerlink" title="4、Secondary NameNode"></a>4、Secondary NameNode</h2><p>并非NameNode的热备。当NameNode挂掉时，它并不能马上替换NameNode并提供服务。</p><ul><li>辅助NameNode，分担其工作量。</li><li>定期合并fsimage 和 fsedits，并推送给NameNode。</li></ul><img src="/2020/08/09/%E5%A4%A7%E6%95%B0%E6%8D%AE-03/1.gif" class><h1 id="HDFS的副本机制和机架感知"><a href="#HDFS的副本机制和机架感知" class="headerlink" title="HDFS的副本机制和机架感知"></a>HDFS的副本机制和机架感知</h1><h2 id="HDFS文件副本机制"><a href="#HDFS文件副本机制" class="headerlink" title="HDFS文件副本机制"></a>HDFS文件副本机制</h2><p>所有文件都是以 Block 块的方式存放在HDFS文件系统中，作用如下：</p><ol><li>一个文件可能大于集群中任意一个磁盘，引入块机制分布存储可以解决该问题。</li><li>使用块作为文件存储的逻辑单位可以简化存储子系统</li><li>块非常适用于数据备份进而提供数据容错能力</li></ol><p>Block的块大小可以通过hdfs-site.xml当中的配置文件进行指定，默认128M。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.block.size<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>块大小 以字节为单位<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="机架感知"><a href="#机架感知" class="headerlink" title="机架感知"></a>机架感知</h2><p>HDFS分布式文件系统的内部有一个副本存放策略：以默认的副本数 = 3 为例：</p><p>1、第一个副本块存本机</p><p>2、第二个副本块跟本机同机架内的其他服务器结点</p><p>3、第三个副本块存不同机架的一个服务器结点上</p><h1 id="HDFS的命令行使用"><a href="#HDFS的命令行使用" class="headerlink" title="HDFS的命令行使用"></a>HDFS的命令行使用</h1><p><code>ls</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">格式：hdfs dfs -ls URI</span><br><span class="line">作用：类似于Linux的ls命令，显示文件列表</span><br><span class="line"></span><br><span class="line">hdfs dfs -ls /</span><br></pre></td></tr></table></figure><p><code>ls -R</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">格式 ： hdfs dfs -ls -R URI</span><br><span class="line">作用 ： 在整个目录下递归执行ls，与UNIX中的ls-R类似</span><br><span class="line"></span><br><span class="line">hdfs dfs -ls -R /</span><br></pre></td></tr></table></figure><p><code>mkdir</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">格式 ： hdfs dfs [-p] -mkdir &lt;paths&gt;</span><br><span class="line">作用 ： 以&lt;paths&gt;中的URI作为参数，创建目录。使用-p参数可以递归创建目录</span><br><span class="line"></span><br><span class="line">hdfs dfs -mkdir /TestDir1</span><br><span class="line">hdfs dfs -mkdir -p /TestDir2/test</span><br></pre></td></tr></table></figure><p>在hdfs文件系统中，可以通过50070端口查看文件系统的结构。</p><img src="/2020/08/09/%E5%A4%A7%E6%95%B0%E6%8D%AE-03/2.png" class><p><code>put</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">格式 ： hdfs dfs -put &lt;localsrc&gt; ... &lt;dst&gt;</span><br><span class="line">作用 ： 将单个的源文件src或者多个源文件srcs从本地文件系统拷贝到目标文件系统中（&lt;dst&gt;对应的目录）。也可以从标准输入中读取输入，写入目标文件系统中。</span><br><span class="line"></span><br><span class="line">hdfs dfs -put  /home/a.txt  /TestDir1</span><br></pre></td></tr></table></figure><p><code>moveFromLocal</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">格式 ： hdfs dfs -moveFromLocal &lt;localsrc&gt; &lt;dst&gt;</span><br><span class="line">作用 ： 和put命令类似，但put相当于复制，此命令相当于将本地文件 剪切 到hdfs中。</span><br><span class="line"></span><br><span class="line">hdfs dfs -moveFromLocal a.txt /TestDir2</span><br></pre></td></tr></table></figure><p><code>get</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">格式 ： hdfs dfs -get [-ignorecrc] [-crc] &lt;src&gt; &lt;localhost&gt;</span><br><span class="line">作用 ： 将文件拷贝到本地文件系统，CRC 校验失败的文件通过-ignorecrc选项进行忽略 拷贝。</span><br><span class="line"></span><br><span class="line">hdfs dfs -get /TestDir2/a.txt ./</span><br></pre></td></tr></table></figure><p><code>mv</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">格式 ： hdfs dfs -mv URI &lt;dst&gt;</span><br><span class="line">作用 ： 将hdfs上的文件从原路径移动到目标路径（移动之后文件删除），该命令不能跨文件系统</span><br><span class="line"></span><br><span class="line">hdfs dfs -mv /TestDir2/a.txt /TestDir1</span><br></pre></td></tr></table></figure><p><code>rm</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">格式 ： hdfs dfs -rm [-r] [skipTrash] URI [URI...]</span><br><span class="line">作用 ： 删除参数指定的文件，参数可以有多个。 -r 表示删除目录 ， -skipTrash 表示删除后不放入回收站</span><br><span class="line"></span><br><span class="line">hdfs dfs -rm /TestDir1/a.txt</span><br><span class="line">hdfs dfs -rm -r /TestDir1</span><br></pre></td></tr></table></figure><img src="/2020/08/09/%E5%A4%A7%E6%95%B0%E6%8D%AE-03/3.png" class><p><code>cp</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">格式 ： hdfs dfs -cp URI [URI ...] &lt;dest&gt;</span><br><span class="line">作用 ： 将文件拷贝到目标路径中，如果&lt;dest&gt;为目录的话，可以将多个文件拷贝到该目录下</span><br><span class="line">-f : 选项将覆盖目标，如果他已经存在。</span><br><span class="line">-p : 选项将保留文件属性(时间戳，所有权，许可，ACL，XAttr)</span><br><span class="line"></span><br><span class="line">hdfs dfs -cp /TestDir1/a.txt /TestDir2/b.txt</span><br></pre></td></tr></table></figure><p><code>cat</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -cat URI [uri...]</span><br><span class="line">作用 ： 将参数所指示的文件内容输出到控制台</span><br><span class="line"></span><br><span class="line">hdfs dfs -cat /TestDir1/a.txt</span><br></pre></td></tr></table></figure><p><code>chmod</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">格式 ： hdfs dfs -chmod [-R] URI[URI...]</span><br><span class="line">作用 ： 改变文件权限。如果使用 -R 选项，则对整个目录有效递归执行。使用这一命令的用户必须是文件的所属用户，或者超级用户。</span><br><span class="line"></span><br><span class="line">hdfs dfs -chmod -R 777 /TestDir1</span><br></pre></td></tr></table></figure><p><code>chown</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">格式 ： hdfs dfs -chmod [-R] URI[URI...]</span><br><span class="line">作用 ： 改变文件的所属用户和用户组。如果使用 -R 选项，则对整个目录有效递归执行。使用这一命令必须是文件的所属用户或者超级管理员。</span><br><span class="line"></span><br><span class="line">hdfs dfs -chown hadoop:hadoop /TestDir2/test</span><br></pre></td></tr></table></figure><p><code>appendToFile</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">格式 ： hdfs dfs -appendToFile &lt;localsrc&gt; ... &lt;dest&gt;</span><br><span class="line">作用 ： 追加一个或者多个文件到hdfs指定文件中。也可以从命令行读取输入</span><br><span class="line"></span><br><span class="line">hdfs dfs -appendToFile a.xml b.xml /TestDir2/big.xml</span><br><span class="line">将本地的a b文件合并一起，存为TestDir2下的 big.xml</span><br></pre></td></tr></table></figure><h1 id="HDFS的高级使用命令"><a href="#HDFS的高级使用命令" class="headerlink" title="HDFS的高级使用命令"></a>HDFS的高级使用命令</h1><h2 id="HDFS文件限额配置"><a href="#HDFS文件限额配置" class="headerlink" title="HDFS文件限额配置"></a>HDFS文件限额配置</h2><p>在多人共用HDFS的 环境下，配置设置非常重要。特别是在Hadoop处理大量资料的环境，如果没有配额管理，很容易把所有空间用完造成别人无法存取。HDFS的配额设定是针对目录而不是账号，可以让每个账号仅操作某一个目录，然后对目录设置配置。</p><p>HDFS文件的限额配置允许我们以文件个数，或者文件大小来限制我们在某个目录上传的 文件数量或者文件内容总量，以便达到我们类似网盘等限制每个用户允许上传的最大的文件的量。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir /user/root/dir</span><br><span class="line"></span><br><span class="line">hdfs dfs -count -q -h dir # 查看某个目录是否具有限额配置</span><br><span class="line">其中路径可以用绝对路径/user/root/dir，相对路径则直接写dir，因为HDFS默认即在user/root/目录下</span><br></pre></td></tr></table></figure><h3 id="数量限额"><a href="#数量限额" class="headerlink" title="数量限额"></a>数量限额</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -setQuota 2 dir # 设置该文件下最多只能上传两个文件</span><br></pre></td></tr></table></figure><blockquote><p>注意：设置为2，但只能上传 1 个文件，设置为 n，上传 n - 1</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -clrQuota dir # 清除文件数量限额</span><br></pre></td></tr></table></figure><h3 id="空间大小限额"><a href="#空间大小限额" class="headerlink" title="空间大小限额"></a>空间大小限额</h3><p>在设置空间配额时，设置的 空间至少是Block_size * 3（128 * 3 = 384M）大小</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -setSpaceQuota 4k /user/root/dir # 限制空间大小4KB，报错，至少384M</span><br><span class="line"></span><br><span class="line">hdfs dfsadmin -clrSpaceQuota dir # 清除空间限额配置</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 生成任意大小的文件命令：</span></span><br><span class="line">dd if=/dev/zero of=1.txt bs=129M count=1 # bs * count 便是想要文件的大小</span><br></pre></td></tr></table></figure><blockquote><p>分析：129M需要被分为两个Block（128M 和 1M），空间限额至少为 2 * 3 * Block_size = 768M。</p></blockquote><h2 id="HDFS的安全模式"><a href="#HDFS的安全模式" class="headerlink" title="HDFS的安全模式"></a>HDFS的安全模式</h2><p>安全模式是Hadoop的一种保护机制，用于保证集群中的数据块的安全性。当集群启动时，会首先进入安全模式。当系统出于安全模式时会检查数据块的完整性。</p><p>假设我们设置的副本数（即参数dfs.replication）是 3，那么在DataNode上就应该有 3 个副本，若只存在 2 个副本，那么比例就是 2 / 3 , HDFS默认的副本率为0.999，小于副本率，系统会自动的复制副本到其他DataNode。若系统超过我们设定的副本数，那么系统也会删除多余的副本。</p><p>在安全模式下，文件系统只接受<strong>读</strong>数据请求，而不接受删除、修改等变更请求。当整个系统达到安全标准时，HDFS会自动离开安全模式。</p><p><strong>安全模式操作命令</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -safemode get# 查看安全模式状态</span><br><span class="line">hdfs dfsadmin -safemode enter# 进入安全模式</span><br><span class="line">hdfs dfsadmin -safemode leave# 离开安全模式</span><br></pre></td></tr></table></figure><h2 id="HDFS基准测试"><a href="#HDFS基准测试" class="headerlink" title="HDFS基准测试"></a>HDFS基准测试</h2><p>实际生产环境中，Hadoop环境搭建完成后，第一件事情就是进行压力测试，测试我们的集群的读取和写入速度，测试我们的网络带宽是否满足一些测试基准</p><h3 id="测试写入速度"><a href="#测试写入速度" class="headerlink" title="测试写入速度"></a>测试写入速度</h3><p>向HDFS文件系统中写入数据，10文件，每个文件 10M ，文件存放的地点：／benchmarks/TestDFSIO中。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/ # 测试会生成结果文件，放到该目录下</span><br><span class="line">hadoop jar /export/servers/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7.jar TestDFSIO -write -nrFiles 10 -fileSize 10MB</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> TestDFSIO：测试DFS的IO</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -write：测试写</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> nrFiles：写入文件数</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> fileSize：每个文件的大小</span></span><br></pre></td></tr></table></figure><p>命令执行完，该目录下生成测试结果的文件<code>TestDFSIO_results.log</code>，可以通过vi命令查看。</p><img src="/2020/08/09/%E5%A4%A7%E6%95%B0%E6%8D%AE-03/4.png" class><p>也可通过命令查看写入速度结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -text /benchmarks/TestDFSIO/io_write/part-00000</span><br></pre></td></tr></table></figure><h3 id="测试读取速度"><a href="#测试读取速度" class="headerlink" title="测试读取速度"></a>测试读取速度</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar /export/servers/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7.jar TestDFSIO -read -nrFiles 10 -fileSize 10MB</span><br></pre></td></tr></table></figure><p>只需把 write 改为 read即可，其他文件查看等与上步一致。</p><h3 id="清除测试数据"><a href="#清除测试数据" class="headerlink" title="清除测试数据"></a>清除测试数据</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar /export/servers/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7.jar TestDFSIO clean</span><br></pre></td></tr></table></figure><blockquote><p>注意：清除的是测试文件，但benchmarks还在，里面的数据清空；同样，TestDFSIO_results.log 也在。</p></blockquote><h1 id="HDFS的写入和读取过程"><a href="#HDFS的写入和读取过程" class="headerlink" title="HDFS的写入和读取过程"></a>HDFS的写入和读取过程</h1><h2 id="写入过程"><a href="#写入过程" class="headerlink" title="写入过程"></a>写入过程</h2><img src="/2020/08/09/%E5%A4%A7%E6%95%B0%E6%8D%AE-03/5.png" class><h2 id="读取过程"><a href="#读取过程" class="headerlink" title="读取过程"></a>读取过程</h2><img src="/2020/08/09/%E5%A4%A7%E6%95%B0%E6%8D%AE-03/6.png" class><h1 id="HDFS的元数据辅助管理"><a href="#HDFS的元数据辅助管理" class="headerlink" title="HDFS的元数据辅助管理"></a>HDFS的元数据辅助管理</h1><p>当Hadoop的集群中，NameNode的所有元数据信息都保存在了FsImage 与 Edits 文件当中。元数据信息的 保存目录配置在了hdfs-site.xml当中。</p><h2 id="FsImage-和-Edits-详解"><a href="#FsImage-和-Edits-详解" class="headerlink" title="FsImage 和 Edits 详解"></a>FsImage 和 Edits 详解</h2><h3 id="edits"><a href="#edits" class="headerlink" title="edits"></a>edits</h3><ul><li>edits 存放了客户端最近一段时间的操作日志</li><li>客户端对HDFS进行写文件时会首先被记录在edits文件中</li><li>edits修改时元数据也会更新</li></ul><h3 id="fsimage"><a href="#fsimage" class="headerlink" title="fsimage"></a>fsimage</h3><ul><li>NameNode中关于元数据的镜像，一般称为检查点，fsimage存放了一份比较完整的元数据信息</li><li>因为fsimage是NameNode的完整镜像，如果每次都加载进就非常损耗内存和CPU，所以一般开始时对NameNode的操作都放在edits中</li><li>随着edits内容增大，就需要在一定时间点和fsimage合并</li></ul><h2 id="fsimage中的文件信息查看"><a href="#fsimage中的文件信息查看" class="headerlink" title="fsimage中的文件信息查看"></a>fsimage中的文件信息查看</h2><p>使用命令 <code>hdfs oiv</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/hadoop-2.7.7/hadoopDatas/namenodeDatas/current</span><br><span class="line">hdfs oiv -i fsimage_0000000000000000165 -p XML -o Test.xml</span><br><span class="line">vi Test.xml</span><br></pre></td></tr></table></figure><h2 id="edits文件信息查看"><a href="#edits文件信息查看" class="headerlink" title="edits文件信息查看"></a>edits文件信息查看</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/hadoop-2.7.7/hadoopDatas/nn/edits/current/</span><br><span class="line">hdfs oev -i edits_0000000000000000166-0000000000000000234 -p XML -o myEdits.xml</span><br><span class="line"> vi myEdits.xml</span><br></pre></td></tr></table></figure><h2 id="SecondaryNameNode如何辅助管理fsimage和-edits文件？"><a href="#SecondaryNameNode如何辅助管理fsimage和-edits文件？" class="headerlink" title="SecondaryNameNode如何辅助管理fsimage和 edits文件？"></a>SecondaryNameNode如何辅助管理fsimage和 edits文件？</h2><p>只有在NameNode重启时，edit logs才会合并到fsimage文件中，从而得到一个文件系统的最新快照。但是在产品集群中NameNode是很少重启的，这也意味着当NameNode运行了很长时间后，edit logs文件会变得很大。在这种情况下就会出现下面一些问题：</p><ol><li>edit logs文件会变的很大，怎么去管理这个文件是一个挑战。</li><li>NameNode的重启会花费很长时间，因为有很多改动[笔者注:在edit logs中]要合并到fsimage文件上。</li><li>如果NameNode挂掉了，那我们就丢失了很多改动因为此时的fsimage文件非常旧。[笔者注: 笔者认为在这个情况下丢失的改动不会很多, 因为丢失的改动应该是还在内存中但是没有写到edit logs的这部分。]</li></ol><p>现在我们明白了NameNode的功能和所面临的挑战 - 保持文件系统最新的元数据。那么，这些跟Secondary NameNode又有什么关系呢？</p><p>SecondaryNameNode就是来帮助解决上述问题的。SecondaryNameNode定期合并fsimage 和 edits，把 edits 控制在一个范围内。</p><img src="/2020/08/09/%E5%A4%A7%E6%95%B0%E6%8D%AE-03/7.png" class><p>上面的图片展示了Secondary NameNode是<strong>怎么工作</strong>的。</p><ol><li>首先，它定时到NameNode去获取edit logs，并更新到fsimage上。[笔者注：Secondary NameNode自己的fsimage]</li><li>一旦它有了新的fsimage文件，它将其拷贝回NameNode中。</li><li>NameNode在下次重启时会使用这个新的fsimage文件，从而减少重启的时间。</li></ol><p>Secondary NameNode的整个目的是在HDFS中提供一个检查点。它只是NameNode的一个助手节点。这也是它在社区内被认为是检查点节点的原因。</p><p>现在，我们明白了Secondary NameNode所做的不过是在文件系统中设置一个检查点来帮助NameNode更好的工作。它不是要取代掉NameNode也不是NameNode的备份。所以从现在起，让我们养成一个习惯，称呼它为<strong>检查点节点</strong>吧。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇开始学习大数据Hadoop技术中的&lt;strong&gt;核心&lt;/strong&gt;之一 —— HDFS。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://iceWind-R.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="http://iceWind-R.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>大数据_02(Hadoop概述与安装)</title>
    <link href="http://icewind-r.github.io/2020/08/05/%E5%A4%A7%E6%95%B0%E6%8D%AE-02/"/>
    <id>http://icewind-r.github.io/2020/08/05/%E5%A4%A7%E6%95%B0%E6%8D%AE-02/</id>
    <published>2020-08-05T00:38:08.000Z</published>
    <updated>2020-08-09T01:46:06.667Z</updated>
    
    <content type="html"><![CDATA[<p>本篇为大数据的Hadoop技术入门，主要是环境的安装和Hadoop的编译。</p><a id="more"></a><hr><h1 id="Hadoop介绍"><a href="#Hadoop介绍" class="headerlink" title="Hadoop介绍"></a>Hadoop介绍</h1><p><strong>狭义上</strong>，Hadoop就是指Hadoop这个软件，它包括：</p><ul><li>HDFS：分布式文件系统</li><li>MapReduce：分布式计算系统</li><li>Yarn：集群资源管理系统</li></ul><p><strong>广义上</strong>，Hadoop指代大数据的一个生态圈，包括很多其他软件。</p><img src="/2020/08/05/%E5%A4%A7%E6%95%B0%E6%8D%AE-02/1.png" class><h1 id="Hadoop的安装"><a href="#Hadoop的安装" class="headerlink" title="Hadoop的安装"></a>Hadoop的安装</h1><h2 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h2><table><thead><tr><th>服务器IP</th><th>主机名</th><th>NameNode</th><th>SecondaryNameNode</th><th>dataNode</th><th>ResourceManager</th><th>NodeManger</th></tr></thead><tbody><tr><td>192.168.2.128</td><td>bigdata1</td><td>是</td><td>是</td><td>是</td><td>是</td><td>是</td></tr><tr><td>192.168.2.129</td><td>bigdata2</td><td>否</td><td>否</td><td>是</td><td>否</td><td>是</td></tr><tr><td>192.168.2.130</td><td>bigdata3</td><td>否</td><td>否</td><td>是</td><td>否</td><td>是</td></tr></tbody></table><p>说明：</p><ul><li><p>NameNode ：是HDFS的主节点。</p></li><li><p>SecondaryNameNode：对 NameNode 做一个辅助管理。</p></li><li><p>dataNode：从结点。</p></li><li><p>ResourceManager：分布式计算MapReduce的主节点。</p></li><li><p>NodeManger：分布式计算MapReduce的从结点。</p></li></ul><h2 id="编译配置过程"><a href="#编译配置过程" class="headerlink" title="编译配置过程"></a>编译配置过程</h2><p>主要是根据 <a href="https://www.bilibili.com/video/BV1JT4y1g7nM?p=45" target="_blank" rel="noopener">B站视频</a> 和 <a href="https://blog.csdn.net/weixin_37490221/article/details/77650388" target="_blank" rel="noopener">一篇博客</a> 完成的。过程较为麻烦，但自己尝试几乎没有踩坑，需要细心细致的编译源码，并进行后面的配置文件的仔细修改。</p><p>这里使用的各个软件的版本号，主机名，目录名等都与教程有些出入，需要自己做出合适修改。</p><img src="/2020/08/05/%E5%A4%A7%E6%95%B0%E6%8D%AE-02/2.png" class><h1 id="Hadoop集群的启动"><a href="#Hadoop集群的启动" class="headerlink" title="Hadoop集群的启动"></a>Hadoop集群的启动</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/hadoop-2.7.7</span><br><span class="line">./bin/hdfs namenode -format</span><br><span class="line">./sbin/start-dfs.sh</span><br><span class="line">./sbin/start-yarn.sh</span><br><span class="line">./sbin/mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure><p>可以通过查看界面完成是否配置成功。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">http://bigdata1:50070/explorer.html#/</span><br><span class="line">http://bigdata1:8088/cluster</span><br><span class="line">http://bigdata1:19888/jobhistory</span><br></pre></td></tr></table></figure><p>成功页面：</p><img src="/2020/08/05/%E5%A4%A7%E6%95%B0%E6%8D%AE-02/3.png" class><img src="/2020/08/05/%E5%A4%A7%E6%95%B0%E6%8D%AE-02/4.png" class><img src="/2020/08/05/%E5%A4%A7%E6%95%B0%E6%8D%AE-02/5.png" class>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇为大数据的Hadoop技术入门，主要是环境的安装和Hadoop的编译。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://iceWind-R.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="http://iceWind-R.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>大数据_01(Zookeeper)</title>
    <link href="http://icewind-r.github.io/2020/08/04/%E5%A4%A7%E6%95%B0%E6%8D%AE-01/"/>
    <id>http://icewind-r.github.io/2020/08/04/%E5%A4%A7%E6%95%B0%E6%8D%AE-01/</id>
    <published>2020-08-04T00:33:15.000Z</published>
    <updated>2020-08-08T04:32:20.574Z</updated>
    
    <content type="html"><![CDATA[<p>ZooKeeper是一个<a href="https://baike.baidu.com/item/分布式/19276232" target="_blank" rel="noopener">分布式</a>的，开放源码的<a href="https://baike.baidu.com/item/分布式应用程序/9854429" target="_blank" rel="noopener">分布式应用程序</a>协调服务，是<a href="https://baike.baidu.com/item/Google" target="_blank" rel="noopener">Google</a>的Chubby一个<a href="https://baike.baidu.com/item/开源/246339" target="_blank" rel="noopener">开源</a>的实现，是Hadoop和Hbase的重要组件。重点内容记录如下。</p><a id="more"></a><hr><h1 id="Zookeeper的介绍和安装"><a href="#Zookeeper的介绍和安装" class="headerlink" title="Zookeeper的介绍和安装"></a>Zookeeper的介绍和安装</h1><h2 id="Zookeeper概述"><a href="#Zookeeper概述" class="headerlink" title="Zookeeper概述"></a>Zookeeper概述</h2><p>Zookeeper是一个开源的分布式协调服务框架，主要用来解决分布式集群中应用系统的一致性问题和数据管理问题。</p><p>zookeeper是一个集群，可以分布在多台主机上，每台主机都可以看做成一个小型的文件系统，则这个整体就可看作一个分布式文件系统。</p><p>关于存储，zookeeper将这个整体看成一个树形结构，每一个结点都是一个Znode。</p><ul><li>Znode是有路径的，例如：/app2/data.conf，这个路径可以理解为是Znode的Name</li><li>Znode可以携带数据，即 既可表示文件目录，也可表示文件。</li></ul><p>正因为Znode的特性，所以Zookeeper可以对外提供一个类似于文件系统的试图，可以通过操作文件系统的方式操作Zookeeper</p><img src="/2020/08/04/%E5%A4%A7%E6%95%B0%E6%8D%AE-01/1.png" class><h2 id="Zookeeper的架构"><a href="#Zookeeper的架构" class="headerlink" title="Zookeeper的架构"></a>Zookeeper的架构</h2><p>zookeeper集群是一个基于主从架构的高可用集群，可以24小时不间断工作。</p><img src="/2020/08/04/%E5%A4%A7%E6%95%B0%E6%8D%AE-01/2.png" class><blockquote><p>其中，客户机发出读取数据的非事务请求 和 写数据的事务性请求，写操作一般由从主机转发给主主机</p></blockquote><p>每个服务器承担如下三种角色中的一种</p><ul><li><strong>Leader</strong>：一个zookeeper集群同一时间只会有一个实际工作的Leader，它会发起并维护各Follower及Observer的心跳。所有的写操作必须要通过Leader完成再由Leader将写操作广播给其他服务器。</li><li><strong>Follower</strong>：一个zookeeper集群可能有多个Followers，它会影响Leader的心跳。Follower可直接处理并返回客户端的读请求，同时将请求转发给Leader处理，并且负责在Leader处理写请求时对请求进行投票。</li><li><strong>Observer</strong>：Observer和Follower类似，但无投票权。</li></ul><img src="/2020/08/04/%E5%A4%A7%E6%95%B0%E6%8D%AE-01/3.png" class><h2 id="Zookeeper的安装和开启"><a href="#Zookeeper的安装和开启" class="headerlink" title="Zookeeper的安装和开启"></a>Zookeeper的安装和开启</h2><p>下载zookeeper的压缩包，网址：<a href="http://archive.apache.org/dist/" target="_blank" rel="noopener">http://archive.apache.org/dist/</a></p><p>找到zookeeper软件包，下载的是3.4.9版本，下载好.tar.gz压缩包后，传到虚拟机上然后解压到自己定义的安装目录。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/softwares</span><br><span class="line">tar -xvf zookeeper-3.4.9.tar.gz -C ../servers</span><br></pre></td></tr></table></figure><p>解压好后，修改配置文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/zookeeper-3.4.9/conf/</span><br><span class="line">cp zoo_sample.cfg zoo.cfg</span><br><span class="line">mkdir -p /export/servers/zookeeper-3.4.9/zkdatas/  # 创建zkdatas文件.里面存入数据</span><br></pre></td></tr></table></figure><p>然后对复制的 <code>zoo.cfg</code> 配置文件进行配置。</p><p><code>vi zoo.cfg</code>，修改内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dataDir=/export/servers/zookeeper-3.4.9/zkdatas/ # 上面创建的zkdatas目录</span><br><span class="line"><span class="meta">#</span><span class="bash"> 保留多少个快照</span></span><br><span class="line">autopurge.snapRetainCount=3</span><br><span class="line"><span class="meta">#</span><span class="bash"> 日志多少小时清理一次</span></span><br><span class="line">autopurge.purgeInterval=1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 集群服务器设置</span></span><br><span class="line">server.1=bigdata1:2888:3888</span><br><span class="line">server.2=bigdata2:2888:3888</span><br><span class="line">server.3=bigdata3:2888:3888</span><br></pre></td></tr></table></figure><p>接下来添加myid配置</p><p>在第一台虚拟机上，在上面创建的zkdatas文件夹下创建myid文件，内容为 1 。</p><p>然后，把配置好的安装包分发给另外两台虚拟机，并且修改它们的myid，分别设置为 2 和 3 。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp -r /export/servers/zookeeper-3.4.9/ bigdata2:/export/servers</span><br><span class="line"></span><br><span class="line">scp -r /export/servers/zookeeper-3.4.9/ bigdata3:/export/servers</span><br></pre></td></tr></table></figure><p>启动zookeeper访问</p><p>进入到bin目录，使用命令 <code>./zkServer.sh start</code>启动服务。（启动时需要注意，命令使用规范）</p><p>然后验证是否启动成功可以使用命令 <code>jps</code>。</p><img src="/2020/08/04/%E5%A4%A7%E6%95%B0%E6%8D%AE-01/4.png" class><p>出现 QuorumPeerMain 表示启动成功。若未启动成功，可能的原因就是上面的zoo.cfg未配置正确。</p><p>然后我们可以用命令 <code>./zkServer.sh status</code> 查看状态，可以发现，虚拟机2为 Leader，1 和 3 为 Follower。</p><p>至此，一个zookeeper集群搭建完成。</p><h1 id="Zookeeper内部结构"><a href="#Zookeeper内部结构" class="headerlink" title="Zookeeper内部结构"></a>Zookeeper内部结构</h1><h2 id="Zookeeper的数据模型"><a href="#Zookeeper的数据模型" class="headerlink" title="Zookeeper的数据模型"></a>Zookeeper的数据模型</h2><ul><li><p>zookeeper的数据模型，在结构上和标准文件系统非常相似，都是采用树形层次结构。</p></li><li><p>zookeeper树中的每个结点被称为一个Znode。</p></li></ul><p>不同之处：</p><ol><li>Znode<strong>兼具</strong>文件和目录两种特点。</li><li>Znode存数据大小有限制，以KB为单位，主要存储状态信息、配置信息。</li><li>Znode通过路径引用。路径必须是绝对的，因此他们必须由斜杠字符来开头（根节点为 / ）。其中 “ /zookeeper ”为系统默认创建的Znode，用以保存管理信息。</li><li>每个Znode由3部分组成：<ol><li>stat：此为状态信息，描述该Znode的版本，权限等信息。</li><li>data：与该Znode关联的数据</li><li>children：该Znode下的子节点</li></ol></li></ol><h2 id="Znode的结点类型"><a href="#Znode的结点类型" class="headerlink" title="Znode的结点类型"></a>Znode的结点类型</h2><p>1、Znode结点有两种，分别为 <strong>临时节点</strong> 和 <strong>永久结点</strong> 。结点的类型在创建时即被确定，并且不能改变。</p><ul><li>临时结点：该节点的生命周期依赖于创建它们的会话。会话结束就会被自动删除。临时结点不允许拥有子节点。。</li><li>永久节点：该节点的生命周期不依赖于会话。</li></ul><p>2、Znode还有序列化的特性，如果创建的时候指定的话，该Znode的名字后面会自动追加一个不断增加的序列号。它的格式为 “ %10d ”（10位数字，没有数值的数位用 0 补充，例如“0000000001”）</p><p>3、这样便会存在四种类型的Znode结点，分别对应：</p><ul><li>PERSISTENT：永久结点</li><li>EPHEMERAL：临时结点</li><li>PERSISTENT_SEQUENTIAL：永久结点、序列化</li><li>EPHEMERAL_SEQUENTIAL：临时结点、序列化</li></ul><h1 id="Zookeeper的Shell客户端操作"><a href="#Zookeeper的Shell客户端操作" class="headerlink" title="Zookeeper的Shell客户端操作"></a>Zookeeper的Shell客户端操作</h1><h2 id="1、登录Zookeeper客户端"><a href="#1、登录Zookeeper客户端" class="headerlink" title="1、登录Zookeeper客户端"></a>1、登录Zookeeper客户端</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./zkCli.sh -server 主机名:2181</span><br></pre></td></tr></table></figure><img src="/2020/08/04/%E5%A4%A7%E6%95%B0%E6%8D%AE-01/5.png" class><p>表示登录成功。</p><p>退出命令 quit。登录时，省略-server及之后的命令，代表登录本主机的客户端。</p><h2 id="2、Zookeeper的常用命令"><a href="#2、Zookeeper的常用命令" class="headerlink" title="2、Zookeeper的常用命令"></a>2、Zookeeper的常用命令</h2><img src="/2020/08/04/%E5%A4%A7%E6%95%B0%E6%8D%AE-01/6.png" class><p><strong>注意</strong>：create命令：如果不加参数，创建永久性结点；如果加参数 - s，创建永久性序列化结点；如果只用 - e，创建临时结点；都加上代表临时性序列化结点。</p><p><strong>操作实例：</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">列出Path下的所有Znode</span><br><span class="line">ls /</span><br><span class="line">创建永久结点</span><br><span class="line">create /hello world</span><br><span class="line">创建临时结点</span><br><span class="line">create -e /abc 123</span><br><span class="line">创建永久序列化结点</span><br><span class="line">create -s /zhangsan boy</span><br><span class="line">创建临时序列化结点</span><br><span class="line">create -e -s /list boy</span><br><span class="line">获取数据</span><br><span class="line">get /hello</span><br><span class="line">修改结点数据</span><br><span class="line">set /hello data</span><br><span class="line">删除结点，如果要删除的结点有子Znode则无法删除</span><br><span class="line">delete /hello</span><br><span class="line">删除结点，如果有子Znode则递归删除</span><br><span class="line">rmr /abc</span><br><span class="line">列出历史记录</span><br><span class="line">history</span><br></pre></td></tr></table></figure><h2 id="3、Znode结点属性"><a href="#3、Znode结点属性" class="headerlink" title="3、Znode结点属性"></a>3、Znode结点属性</h2><p>每个Znode都包含了一系列的属性，通过命令get，可以获得结点的属性。</p><img src="/2020/08/04/%E5%A4%A7%E6%95%B0%E6%8D%AE-01/7.png" class><p>首先第一行是该节点携带的信息。</p><p>dataVersion：数据版本号。每次对结点进行set操作，该值都会增1。</p><p>cversion：子节点的版本号。当Znode的子节点有变化时，cversion的值就会增1。</p><p>aclVersion：ACL的版本号。</p><p>cZxid：Znode创建事务的ID。</p><p>mZxid：Znode被修改的事务ID，即每次对Znode的修改都会更新mZxid。</p><p>ctime：结点创建时的时间戳。</p><p>mtime：最近一次更新的时间。</p><p>ephemeralOwner：如果该节点为临时结点，该值表示与该节点绑定的session id，如果不是，值为 0。</p><h1 id="Zookeeper的watch机制"><a href="#Zookeeper的watch机制" class="headerlink" title="Zookeeper的watch机制"></a>Zookeeper的watch机制</h1><p>类似于数据库中的触发器，对某个Znode设置 <code>Watcher</code>，当Znode发生变化时，<code>WatchManager</code>会调用对应的Watcher。</p><p>当Znode发生删除，修改，创建，子节点的修改时，对应的Watcher会得到通知。</p><p>Watcher的特点</p><ul><li>一次性触发：一个Watcher只会被触发一次，如果需要继续监听，则需要再次添加Watcher。</li><li>事件封装：Watcher得到的事件是被封装过的，包括三个内容 <code>KeeperState</code> ， <code>eventType</code> ， <code>path</code></li></ul><img src="/2020/08/04/%E5%A4%A7%E6%95%B0%E6%8D%AE-01/8.png" class><p><strong>主要作用</strong>：</p><p>1、发布和订阅</p><p>2、监听集群中主机的存活状态</p><h1 id="Zookeeper的JavaAPI操作"><a href="#Zookeeper的JavaAPI操作" class="headerlink" title="Zookeeper的JavaAPI操作"></a>Zookeeper的JavaAPI操作</h1><p>这里操作Zookeeper的JavaAPI使用的是一套zookeeper客户端框架Curator，解决了很多Zookeeper客户端非常底层的细节开发工作。</p><p>Curator包含了几个包：</p><ul><li>curator-framework：对zookeeper的底层api的一些封装</li><li>curator-recipes：封装了一些高级特性，如：Cache事件监听、选举、分布式锁、分布式计数器等。</li></ul><p>Maven依赖（使用curator的版本：2.12.0，对应zookeeper的3.4.x版本，如果不对应，可能有兼容性问题，很可能导致结点操作失败）</p><p><strong>实例代码</strong>：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.thorine.zookeeper_api;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.curator.RetryPolicy;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.CuratorFramework;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.CuratorFrameworkFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.recipes.cache.ChildData;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.recipes.cache.TreeCache;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.recipes.cache.TreeCacheEvent;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.recipes.cache.TreeCacheListener;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.retry.ExponentialBackoffRetry;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.CreateMode;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">    <span class="meta">@org</span>.junit.Test</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">operateZnode</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//1：定制一个重试策略</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        * param1：重试的间隔时间，单位ms</span></span><br><span class="line"><span class="comment">        * param2：重试的最大次数</span></span><br><span class="line"><span class="comment">        * */</span></span><br><span class="line">        RetryPolicy retryPolicy = <span class="keyword">new</span> ExponentialBackoffRetry(<span class="number">1000</span>, <span class="number">1</span>);</span><br><span class="line">        <span class="comment">//2：获取一个客户端对象</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        * param1：要连接的Zookeeper服务器列表</span></span><br><span class="line"><span class="comment">        * param2:会话的超时时间</span></span><br><span class="line"><span class="comment">        * param3：连接的超时时间</span></span><br><span class="line"><span class="comment">        * param4：重试策略</span></span><br><span class="line"><span class="comment">        * */</span></span><br><span class="line">        String connStr = <span class="string">"bigdata1:2181,bigdata2:2181,bigdata3:2181"</span>; <span class="comment">// 有多个主机时，每个主机之间用逗号连接，并且注意：不要为美观而乱加空格！</span></span><br><span class="line">        CuratorFramework client = CuratorFrameworkFactory.newClient(connStr, <span class="number">3000</span>, <span class="number">3000</span>, retryPolicy);</span><br><span class="line">        <span class="comment">//3：开启客户端</span></span><br><span class="line">        client.start();</span><br><span class="line">        <span class="comment">//4：创建结点</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        * 客户端创建结点，creatingParentsIfNeeded表示：创建hello，但也会创建父节点Test</span></span><br><span class="line"><span class="comment">        * withMode表示类型，有四种</span></span><br><span class="line"><span class="comment">        * forPath表示结点路径和携带的数据，第二个参数为byte类型</span></span><br><span class="line"><span class="comment">        * */</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建永久结点</span></span><br><span class="line">        client.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath(<span class="string">"/Test/hello4"</span>, <span class="string">"JavaApiTest"</span>.getBytes());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建临时性结点</span></span><br><span class="line">        client.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL).forPath(<span class="string">"/Test/tmp"</span>, <span class="string">"临时"</span>.getBytes());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 因为临时结点只在会话时有效，为保证看到效果，使程序休眠10s</span></span><br><span class="line">        Thread.sleep(<span class="number">10000</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 修改结点数据</span></span><br><span class="line">        client.setData().forPath(<span class="string">"/Test/hello"</span>, <span class="string">"被修改"</span>.getBytes());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取结点数据</span></span><br><span class="line">        <span class="keyword">byte</span>[] bytes = client.getData().forPath(<span class="string">"/hello"</span>);</span><br><span class="line">        System.out.println(<span class="keyword">new</span> String(bytes));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//5：关闭客户端</span></span><br><span class="line">        client.close();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * 结点的watch机制</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line">    <span class="meta">@org</span>.junit.Test</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">watchZnode</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        RetryPolicy retryPolicy = <span class="keyword">new</span> ExponentialBackoffRetry(<span class="number">3000</span>,<span class="number">1</span>);</span><br><span class="line">        String connStr = <span class="string">"bigdata1:2181,bigdata2:2181,bigdata3:2181"</span>;</span><br><span class="line">        CuratorFramework client = CuratorFrameworkFactory.newClient(connStr, <span class="number">8000</span>, <span class="number">8000</span>, retryPolicy);</span><br><span class="line">        client.start();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建一个TreeCache对象，指定要监控的结点路径</span></span><br><span class="line">        TreeCache treeCache = <span class="keyword">new</span> TreeCache(client, <span class="string">"/hello"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 自定义监听器</span></span><br><span class="line">        treeCache.getListenable().addListener(<span class="keyword">new</span> TreeCacheListener() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">childEvent</span><span class="params">(CuratorFramework curatorFramework, TreeCacheEvent treeCacheEvent)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                ChildData data = treeCacheEvent.getData();</span><br><span class="line">                <span class="keyword">if</span> (data != <span class="keyword">null</span>)&#123;</span><br><span class="line">                    <span class="keyword">switch</span> (treeCacheEvent.getType())&#123; <span class="comment">//获取事件的触发类型</span></span><br><span class="line">                        <span class="keyword">case</span> NODE_ADDED: <span class="comment">// 表示新增结点触发的watch</span></span><br><span class="line">                            System.out.println(<span class="string">"NODE_ADDED"</span>);</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                        <span class="keyword">case</span> NODE_REMOVED:</span><br><span class="line">                            System.out.println(<span class="string">"NODE_REMOVED"</span>);</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                        <span class="keyword">case</span> NODE_UPDATED:</span><br><span class="line">                            System.out.println(<span class="string">"NODE_UPDATED"</span>);</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                        <span class="keyword">default</span>:</span><br><span class="line">                            System.out.println(<span class="string">"default"</span>);</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 开始监听</span></span><br><span class="line">        treeCache.start();</span><br><span class="line"></span><br><span class="line">        Thread.sleep(<span class="number">100000</span>);</span><br><span class="line">        <span class="comment">// 之后运行程序，在zookeeper中对指定的结点进行操作时，控制台便会打印相应信息。</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ZooKeeper是一个&lt;a href=&quot;https://baike.baidu.com/item/分布式/19276232&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;分布式&lt;/a&gt;的，开放源码的&lt;a href=&quot;https://baike.baidu.com/item/分布式应用程序/9854429&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;分布式应用程序&lt;/a&gt;协调服务，是&lt;a href=&quot;https://baike.baidu.com/item/Google&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Google&lt;/a&gt;的Chubby一个&lt;a href=&quot;https://baike.baidu.com/item/开源/246339&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;开源&lt;/a&gt;的实现，是Hadoop和Hbase的重要组件。重点内容记录如下。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://iceWind-R.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Zookeeper" scheme="http://iceWind-R.github.io/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Shell基础命令</title>
    <link href="http://icewind-r.github.io/2020/08/03/Shell%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4/"/>
    <id>http://icewind-r.github.io/2020/08/03/Shell%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4/</id>
    <published>2020-08-03T12:03:06.000Z</published>
    <updated>2020-08-03T14:53:02.668Z</updated>
    
    <content type="html"><![CDATA[<p>总结Linux中的shell命令</p><a id="more"></a><hr><h2 id="1、基本语法"><a href="#1、基本语法" class="headerlink" title="1、基本语法"></a>1、基本语法</h2><p>使用vi编辑器新建一个文件 hello.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash<span class="comment"># 开头固定</span></span></span><br><span class="line">echo "Hello world!"</span><br></pre></td></tr></table></figure><h3 id="执行方式"><a href="#执行方式" class="headerlink" title="执行方式"></a>执行方式</h3><ol><li><p><code>sh hello.sh</code></p></li><li><pre><code class="shell">chmod +x ./hello.sh    # 使脚本具有执行权限./hello.sh            # 执行脚本<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">## 3、变量</span><br><span class="line"></span><br><span class="line">### 局部变量</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;shell</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">str&#x3D;&quot;hello&quot;</span><br><span class="line">echo $&#123;str&#125;world# helloworld</span><br></pre></td></tr></table></figure></code></pre></li></ol><h3 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo $PATH</span><br><span class="line">echo $HOME</span><br></pre></td></tr></table></figure><p>若想查看所有的环境变量，使用命令：<code>env</code></p><p>编辑环境变量，在/etc/profile 中，通过 export 关键字。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export MYENV=/root/myenv</span><br></pre></td></tr></table></figure><h2 id="4、特殊字符"><a href="#4、特殊字符" class="headerlink" title="4、特殊字符"></a>4、特殊字符</h2><table><thead><tr><th>符号</th><th>含义</th></tr></thead><tbody><tr><td>$#</td><td>传递到将本的参数个数</td></tr><tr><td>$*</td><td>以一个但字符串显示所有向脚本传递的参数</td></tr><tr><td>$$</td><td>脚本运行的当前进程ID号</td></tr><tr><td>$!</td><td>后台运行的最后一个进程的ID号</td></tr><tr><td>$@</td><td>与$*相同，但是使用时加引号，并在引号中返回每个参数</td></tr><tr><td>$?</td><td>显示最后命令的退出状态，0表示没有错误，其他任何值表示有错误</td></tr></tbody></table><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">echo "第一个参数为： $1"</span><br><span class="line">echo "参数个数为：$#"</span><br><span class="line">echo "传递的参数作为一个字符串显示：$*"</span><br></pre></td></tr></table></figure><p>执行：./test.sh 1 2 3</p><h2 id="5、运算符"><a href="#5、运算符" class="headerlink" title="5、运算符"></a>5、运算符</h2><p>以加法 + 为例。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">! /bin/bash</span></span><br><span class="line">a=1;</span><br><span class="line">b=2;</span><br><span class="line">echo `expr $a + $b`;</span><br><span class="line">echo $((a+b));</span><br><span class="line">echo $[a+b];</span><br></pre></td></tr></table></figure><h2 id="6、if语句"><a href="#6、if语句" class="headerlink" title="6、if语句"></a>6、if语句</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">read -p "Please input your name:" NAME # read命令用于从控制台读取输入的数据，然后存入NAME变量</span><br><span class="line">if [ $NAME = root ]</span><br><span class="line">then</span><br><span class="line">echo "hello $&#123;NAME&#125;"</span><br><span class="line">elif [ $NAME = dongao ]</span><br><span class="line">then</span><br><span class="line">echo "hello $&#123;NAME&#125;"</span><br><span class="line">else</span><br><span class="line">echo "Get out"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h2 id="7、for语句"><a href="#7、for语句" class="headerlink" title="7、for语句"></a>7、for语句</h2><p>方式1：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">for N in 1 2 3</span><br><span class="line">do</span><br><span class="line">echo $N</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>方式2：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">for ((i = 0; i &lt;= 5; i++))</span><br><span class="line">do</span><br><span class="line">echo "$i"</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h2 id="8、函数"><a href="#8、函数" class="headerlink" title="8、函数"></a>8、函数</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">funWithReturn()&#123;</span><br><span class="line">echo "输入第一个字"</span><br><span class="line">read aNum</span><br><span class="line">echo "输入第二个字"</span><br><span class="line">read bNum</span><br><span class="line">return $(($aNum+$bNum))</span><br><span class="line">&#125;</span><br><span class="line">funWithReturn         # 调用函数</span><br><span class="line">echo "输入的两个数字之和为 $? " # $? 提取上一个返回值</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;总结Linux中的shell命令&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="http://iceWind-R.github.io/categories/Linux/"/>
    
    
      <category term="Shell" scheme="http://iceWind-R.github.io/tags/Shell/"/>
    
  </entry>
  
  <entry>
    <title>Python数据分析_01</title>
    <link href="http://icewind-r.github.io/2020/07/27/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-01/"/>
    <id>http://icewind-r.github.io/2020/07/27/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-01/</id>
    <published>2020-07-27T02:31:09.000Z</published>
    <updated>2020-08-09T01:55:54.731Z</updated>
    
    <content type="html"><![CDATA[<p>本篇开始学习python中重要的数据分析技术，它是python数据科学的技术，是机器学习课程的基础。</p><a id="more"></a><hr><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><h2 id="什么是数据分析"><a href="#什么是数据分析" class="headerlink" title="什么是数据分析"></a>什么是数据分析</h2><p>数据分析是用适当的方法对收集来的大量数据进行分析，帮助人们做出判断，以便采取适当行动。</p><h2 id="数据分析的流程"><a href="#数据分析的流程" class="headerlink" title="数据分析的流程"></a>数据分析的流程</h2><p>提出问题 –&gt; 准备数据 –&gt; <strong>分析数据</strong> –&gt; 获得结论 –&gt; 成果可视化</p><h2 id="两个工具环境的安装"><a href="#两个工具环境的安装" class="headerlink" title="两个工具环境的安装"></a>两个工具环境的安装</h2><h3 id="CONDA环境安装"><a href="#CONDA环境安装" class="headerlink" title="CONDA环境安装"></a>CONDA环境安装</h3><p>里面有大量的安装包等内容，几乎可以解决pip下载失败的所有问题。</p><h3 id="jupyter-notebook"><a href="#jupyter-notebook" class="headerlink" title="jupyter notebook"></a>jupyter notebook</h3><p>是一款编程/文档/笔记/展示软件。</p><img src="/2020/07/27/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-01/1.png" class><h1 id="认识-matplotlib"><a href="#认识-matplotlib" class="headerlink" title="认识 matplotlib"></a>认识 matplotlib</h1><h2 id="为什么学习matplotlib？"><a href="#为什么学习matplotlib？" class="headerlink" title="为什么学习matplotlib？"></a>为什么学习matplotlib？</h2><ol><li>能够将数据进行可视化，更直观的呈现</li><li>使数据更加客观，更具说服力</li></ol><h2 id="什么是matplotlib？"><a href="#什么是matplotlib？" class="headerlink" title="什么是matplotlib？"></a>什么是matplotlib？</h2><p>最流行的python底层绘图库，主要做数据可视化图表，名字取材于matlab。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇开始学习python中重要的数据分析技术，它是python数据科学的技术，是机器学习课程的基础。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://iceWind-R.github.io/categories/Python/"/>
    
    
      <category term="Python" scheme="http://iceWind-R.github.io/tags/Python/"/>
    
      <category term="数据分析" scheme="http://iceWind-R.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫_09(CrawlSpider)</title>
    <link href="http://icewind-r.github.io/2020/07/09/Python%E7%88%AC%E8%99%AB-09/"/>
    <id>http://icewind-r.github.io/2020/07/09/Python%E7%88%AC%E8%99%AB-09/</id>
    <published>2020-07-09T07:43:20.000Z</published>
    <updated>2020-08-09T01:55:47.294Z</updated>
    
    <content type="html"><![CDATA[<p>本篇是python中 <strong>CrawlSpider</strong> 的介绍，还有<strong>Scrapy Shell</strong>、<strong>request 和 response</strong> 对象、<strong>发送POST请求</strong> 等。</p><a id="more"></a><hr><h1 id="CrawlSpider"><a href="#CrawlSpider" class="headerlink" title="CrawlSpider"></a>CrawlSpider</h1><p>在之前的爬虫中定义了新的功能，可以定义爬取url的规则，以后scrapy碰到满足条件的url都进行爬取，而不用手动的<code>yield Request</code>。</p><h2 id="创建CrawlSpider"><a href="#创建CrawlSpider" class="headerlink" title="创建CrawlSpider"></a>创建CrawlSpider</h2><p>之前创建爬虫的方式是通过 <code>scrapy genspider [爬虫名] [域名]</code> 的方式创建的。如果想要创建<code>CrawlSpider</code>爬虫，则通过以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider -t crawl [爬虫名] [域名]</span><br></pre></td></tr></table></figure><h2 id="LinkExtractors链接提取器"><a href="#LinkExtractors链接提取器" class="headerlink" title="LinkExtractors链接提取器"></a>LinkExtractors链接提取器</h2><p>使用<code>LinkExtractors</code> 可以不用我们手动提取想要的URL，然后发送请求。这些工作可以交给<code>LinkExtractors</code> ，它会在所有爬的页面中找到满足规则的URL，实现自动爬取。下面是简单介绍：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">scrapy</span>.<span class="title">linkextractors</span>.<span class="title">LinkExtractor</span><span class="params">(</span></span></span><br><span class="line"><span class="class"><span class="params">    allow = <span class="params">()</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    deny = <span class="params">()</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    allow_domains = <span class="params">()</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    deny_domains = <span class="params">()</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    deny_extensions = None,</span></span></span><br><span class="line"><span class="class"><span class="params">    restrict_xpaths = <span class="params">()</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    tags = <span class="params">(<span class="string">'a'</span>, <span class="string">'area'</span>)</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    attrs = <span class="params">(<span class="string">'href'</span>)</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    canonicalize = True,</span></span></span><br><span class="line"><span class="class"><span class="params">    unique = True,</span></span></span><br><span class="line"><span class="class"><span class="params">    process_value = None</span></span></span><br><span class="line"><span class="class"><span class="params">)</span></span></span><br></pre></td></tr></table></figure><p>主要参数讲解：</p><ul><li>allow：允许的URL，所有满足里面正则表达式的URL都会被提取</li><li>allow_domains：允许的域名，只有这里指定的域名的URL才会被提取</li></ul><h2 id="Rule规则类"><a href="#Rule规则类" class="headerlink" title="Rule规则类"></a>Rule规则类</h2><p>定义爬虫的规则类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">scrapy</span>.<span class="title">spiders</span>.<span class="title">Rule</span><span class="params">(</span></span></span><br><span class="line"><span class="class"><span class="params">    link_extractor,</span></span></span><br><span class="line"><span class="class"><span class="params">    callback = None,</span></span></span><br><span class="line"><span class="class"><span class="params">    cb_kwargs = None,</span></span></span><br><span class="line"><span class="class"><span class="params">    follow = None,</span></span></span><br><span class="line"><span class="class"><span class="params">    process_links = None,</span></span></span><br><span class="line"><span class="class"><span class="params">    process_request = None</span></span></span><br><span class="line"><span class="class"><span class="params">)</span></span></span><br></pre></td></tr></table></figure><p>主要参数讲解：</p><ul><li>link_extractor：一个<code>LinkExtractor</code>对象，用于定义爬取规则。</li><li>callback：满足规则的URL，应该执行哪个回调函数。</li><li>follow：根据该规则，指定response中提取的链接是否需要跟进。</li><li>process_links：从link_extractor中获取的链接会传递给这个函数，用来过滤不需要爬取的链接。</li></ul><h1 id="Scrapy-Shell"><a href="#Scrapy-Shell" class="headerlink" title="Scrapy Shell"></a>Scrapy Shell</h1><p>可以简单的运行Xpath、BeautifulSoup、正则表达式或CSS选择器等，来判断我们的提取规则是否正确。</p><p>在 cmd（或pycharm的Terminal）进入scrapy爬虫的工程目录（可以读取对应项目里的配置信息，若不进入，则只能运行一些通用测试，平常使用足够），运行下述代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell 域名</span><br></pre></td></tr></table></figure><h1 id="scrapy-中的-request-和-response-对象"><a href="#scrapy-中的-request-和-response-对象" class="headerlink" title="scrapy 中的 request 和 response 对象"></a>scrapy 中的 request 和 response 对象</h1><h2 id="Request对象"><a href="#Request对象" class="headerlink" title="Request对象"></a>Request对象</h2><p>request对象在我们写爬虫，爬取一页的数据，需要重新发送一个请求的时候调用。这个类需要传递一些参数，其中比较常用的参数有：</p><ul><li>url：发送请求的url</li><li>callback：在下载器完成相应的下载任务后执行的回调函数</li><li>method：默认为GET方法</li><li>headers：请求头，固定部分放入setting.py 中，需要改变的再可在发送时指定</li><li>meta：比较常用，用于在不同的请求之间传递数据</li><li>encoding：默认utf-8</li><li>dot_filter：表示不由调度器过滤，在执行多次重复的请求时用的比较多</li><li>errback：在发生错误时执行的函数</li></ul><h2 id="Response对象"><a href="#Response对象" class="headerlink" title="Response对象"></a>Response对象</h2><p>Response对象一般是由scrapy自动生成的，因此我们只需关系如何使用。它有很多属性，主要用来提取数据，依次介绍：</p><ul><li><p>meta：从请求穿过来的meta属性，可以用来保持多个请求之间的数据连接</p></li><li><p>encoding：返回字符串编码和解码格式</p></li><li><p>text：将返回来的数据作为Unicode字符串返回</p></li><li><p>body：将返回来的数据作为Bytes字符串返回</p></li><li><p>xpath：xpath选择器</p></li><li><p>css：css选择器</p></li></ul><h1 id="发送POST请求"><a href="#发送POST请求" class="headerlink" title="发送POST请求"></a>发送POST请求</h1><p>有时我们想要在请求数据的时候发送POST请求，那么这时候需要使用 <code>Request</code> 的子类 <code>FormRequest</code> 来实现（专门为表单提交设计的类）。如果想要在爬虫一开始的时候就发送POST请求，那么需要在爬虫类中重写 <code>start_requests(self)</code> 方法，并且不再调用 <code>start_url</code> 里的URL。</p><h2 id="模拟登录"><a href="#模拟登录" class="headerlink" title="模拟登录"></a>模拟登录</h2><h3 id="案例一：模拟人人网登录"><a href="#案例一：模拟人人网登录" class="headerlink" title="案例一：模拟人人网登录"></a>案例一：模拟人人网登录</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RenrenSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'renren'</span></span><br><span class="line">    allowed_domains = [<span class="string">'renren.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://renren.com/'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模拟登录，重写该方法(因为在父类中，该方法默认发送GET请求)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        url = <span class="string">'http://www.renren.com/PLogin.do'</span></span><br><span class="line">        data = &#123;<span class="string">'email'</span>:<span class="string">'13287857692'</span>, <span class="string">'password'</span>:<span class="string">'dongao123'</span>&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 专门发送POST请求，并且携带表单数据</span></span><br><span class="line">        request = scrapy.FormRequest(url, formdata = data, callback = self.parse_page)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> request</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_page</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># with open ('renren.html', 'w', encoding='utf-8') as f:</span></span><br><span class="line">        <span class="comment">#     f.write(response.text)</span></span><br><span class="line">        <span class="comment"># 请求只有登录才能访问的个人主页</span></span><br><span class="line">        request = scrapy.Request(url=<span class="string">'http://www.renren.com/974726184/profile'</span>,callback=self.parse_profile)</span><br><span class="line">        <span class="keyword">yield</span> request</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_profile</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">'myprofile.html'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f :</span><br><span class="line">            f.write(response.text)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇是python中 &lt;strong&gt;CrawlSpider&lt;/strong&gt; 的介绍，还有&lt;strong&gt;Scrapy Shell&lt;/strong&gt;、&lt;strong&gt;request 和 response&lt;/strong&gt; 对象、&lt;strong&gt;发送POST请求&lt;/strong&gt; 等。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://iceWind-R.github.io/categories/Python/"/>
    
    
      <category term="Python" scheme="http://iceWind-R.github.io/tags/Python/"/>
    
      <category term="爬虫" scheme="http://iceWind-R.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>The College Entrance Examination in Despair</title>
    <link href="http://icewind-r.github.io/2020/07/07/%E7%BB%9D%E5%A2%83%E9%87%8C%E7%9A%84%E9%AB%98%E8%80%83/"/>
    <id>http://icewind-r.github.io/2020/07/07/%E7%BB%9D%E5%A2%83%E9%87%8C%E7%9A%84%E9%AB%98%E8%80%83/</id>
    <published>2020-07-07T11:32:58.000Z</published>
    <updated>2020-07-20T15:00:19.674Z</updated>
    
    <content type="html"><![CDATA[<p>2020年7月7日~2020年7月8日高考。</p><a id="more"></a><hr><p>今年真是命运多舛的一年，就在今天下午，本应所有高三考生紧张答题的时候，却传来了南方载有考生的大巴车出事，21人死亡。真是令人心痛，最美的年纪，可命运对他们并不公平。</p><blockquote><p>那会难受的想跳河，不过我使劲儿的控制住自己了。</p></blockquote><p>每个人都在努力过好自己的生活，但有些时候，真的很难。</p><p>除了生老病死，其他都是小事。</p><p>生活很难，但请务必坚持。</p><p>有人欢喜有人悲，黑马很多，但考砸的也不会少。</p><blockquote><p>别急着崩溃，以后让你崩溃的事多着呢</p><p>真的崩溃不叫崩溃，那叫绝望。</p></blockquote><p>失常只是正常的一部分，平常心。</p><blockquote><p>试卷，答案往往只有一个，如果没有找到它，那就是不及格。</p><p>但是啊，人生不一样，人生有很多正确答案</p><p>读大学是，不读也是</p><p>读好的是，读所谓的不好的，也是</p><p>热爱运动是，喜欢音乐是，做你想做的事，更是</p><p>不要畏惧，不管这次如何，都不要否定自己的可能性</p><p>挺起胸膛，理直气壮</p><p>人生是一场马拉松，起跑落后一点</p><p>又怎样</p></blockquote><p>罗翔：</p><blockquote><p> 古希腊哲学家<strong>埃比克泰德</strong> 曾经说过的一句话：我们登上并非我们所选择的舞台，演出并非我们所选择的剧本。</p></blockquote><p>各位同学有谁是自愿来到这个世界上的，有谁来到这个世界上父母征求过你的意见？没有征求。我们登上并非我们所选择的舞台，各位同学有谁的人生剧本是你选择的？大家是不是很羡慕别人的剧本，但是没有办法，你的剧本不是你选择的。你只有努力的把自己的剧本给演好。虽然很痛苦，但是只要努力演好你的剧本，在痛苦中也有精彩。每个人的人生剧本都是独特的，每个人的人生剧本都是精彩的。不要去羡慕别人，因为你没有可以羡慕的。人生中绝大多数，说实话都不是我们所能决定的。人生中95%的东西是我们决定不了的。我们的出生，我们的智商，我们一生的机遇，其实都不是我们所能决定的。这就是为什么如果你真的取得了成就你应该感谢谁，是感谢你自己，还是感谢你自己以外的力量。其实不应该感谢你自己，因为你能决定的东西很少。这就是为什么，如果你真正取得了成就，你要积极的回报这个社会，因为给你的不一定真正属于你。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2020年7月7日~2020年7月8日高考。&lt;/p&gt;
    
    </summary>
    
    
      <category term="随笔" scheme="http://iceWind-R.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="idea" scheme="http://iceWind-R.github.io/tags/idea/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫_08(糗事百科Scrapy爬虫)</title>
    <link href="http://icewind-r.github.io/2020/07/05/Python%E7%88%AC%E8%99%AB-08/"/>
    <id>http://icewind-r.github.io/2020/07/05/Python%E7%88%AC%E8%99%AB-08/</id>
    <published>2020-07-05T02:40:14.000Z</published>
    <updated>2020-08-09T01:55:43.161Z</updated>
    
    <content type="html"><![CDATA[<p>本篇是python实例编写：<strong>糗事百科</strong>，目的是介绍最基础的scrapy框架的写法和运行过程。</p><a id="more"></a><hr><h1 id="功能描述"><a href="#功能描述" class="headerlink" title="功能描述"></a>功能描述</h1><p>目标：获取 糗事百科 所所有信息的作者，url和内容，需要爬取多个页面，/page/1~/page/13。 </p><p>输出：保存到文件中（json格式）</p><h1 id="编写步骤"><a href="#编写步骤" class="headerlink" title="编写步骤"></a>编写步骤</h1><p>编写<strong>spider</strong>处理链接爬取和页面解析，编写<strong>pipelines</strong>处理信息存储</p><h2 id="步骤1：建立工程和Spider模板"><a href="#步骤1：建立工程和Spider模板" class="headerlink" title="步骤1：建立工程和Spider模板"></a>步骤1：建立工程和Spider模板</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">\&gt;scrapy startproject qsbk</span><br><span class="line">\&gt;cd BaiduStocks</span><br><span class="line">\&gt;scrapy genspider qsbk_spider qiushibaike.com</span><br></pre></td></tr></table></figure><h2 id="步骤2：编写Spider"><a href="#步骤2：编写Spider" class="headerlink" title="步骤2：编写Spider"></a>步骤2：编写Spider</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line"><span class="keyword">from</span> qsbk.items <span class="keyword">import</span> QsbkItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QsbkSpiderSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'qsbk_spider'</span></span><br><span class="line">    allowed_domains = [<span class="string">'qiushibaike.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://qiushibaike.com/text/page/1/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># selectorList 类型</span></span><br><span class="line">        duanziDivs = response.xpath(<span class="string">"//div[@class='col1 old-style-col1']/div"</span>)</span><br><span class="line"></span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> div <span class="keyword">in</span> duanziDivs:</span><br><span class="line">            <span class="comment"># div: Selector类型</span></span><br><span class="line">            <span class="comment"># div.xpath()：selectorList类型</span></span><br><span class="line">            author = div.xpath(<span class="string">".//h2/text()"</span>).get().strip()</span><br><span class="line">            <span class="comment"># get()函数，取到第一个结果（str类型）, get()&lt;=&gt;extract_first()</span></span><br><span class="line">            <span class="comment"># extract(): 提取所有结果组成列表，每个元素是str类型</span></span><br><span class="line">            <span class="comment"># strip(): 去掉前后的空格</span></span><br><span class="line"></span><br><span class="line">            href = div.xpath(<span class="string">'./a/@href'</span>).get() <span class="comment"># 获取a标签内的href的属性值</span></span><br><span class="line">            url = urljoin(self.start_urls[<span class="number">0</span>],href) <span class="comment"># 跳转到详情页</span></span><br><span class="line"></span><br><span class="line">            content = div.xpath(<span class="string">".//div[@class='content']//text()"</span>).extract()</span><br><span class="line">            <span class="comment"># html中该div下有span标签，span中才是内容，//text()直接提取该内容</span></span><br><span class="line"></span><br><span class="line">            content = <span class="string">''</span>.join(content).strip()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># tplt = "&#123;0:&#123;2&#125;&lt;20&#125;\t&#123;1:^50&#125;"</span></span><br><span class="line">            <span class="comment"># print(tplt.format(author,url,chr(12288)))</span></span><br><span class="line">            <span class="comment"># print(content)</span></span><br><span class="line">            <span class="comment"># print('=' * 100)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># duanzi = &#123;'author':author,'content':content&#125; # 即为一个个的存储项，item，写法如下 ⬇</span></span><br><span class="line">            item = QsbkItem(author=author, content=content)</span><br><span class="line"></span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">            print(<span class="string">'生成器调用：'</span>,i)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># yield duanzi</span></span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 若不使用生成器，即注释上面的yield item，可使用以下方法</span></span><br><span class="line">            <span class="comment"># items = []</span></span><br><span class="line">            <span class="comment"># items.append(item)</span></span><br><span class="line">            <span class="comment"># return items</span></span><br><span class="line">            <span class="comment"># 返回所有的items=，在pipelines中也可以被解析</span></span><br></pre></td></tr></table></figure><blockquote><p>测试该函数可以注释最后的生成器步骤，取消注释前面的打印函数。</p></blockquote><h3 id="对setting-py的设置"><a href="#对setting-py的设置" class="headerlink" title="对setting.py的设置"></a>对setting.py的设置</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">user-agent = <span class="string">'Mozilla/5.0'</span> <span class="comment"># 设置头信息</span></span><br><span class="line"></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span> <span class="comment"># 不遵循robots协议</span></span><br><span class="line"></span><br><span class="line">LOG_LEVEL = <span class="string">"WARN"</span> <span class="comment"># 取消warning以下的提示信息，使控制台更干净</span></span><br></pre></td></tr></table></figure><h2 id="步骤3：编写ITEM-Pipelines"><a href="#步骤3：编写ITEM-Pipelines" class="headerlink" title="步骤3：编写ITEM Pipelines"></a>步骤3：编写ITEM Pipelines</h2><p>注意该文件中，类里的三个方法: 打开、运行、关闭。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define your item pipelines here</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Don't forget to add your pipeline to the ITEM_PIPELINES setting</span></span><br><span class="line"><span class="comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># useful for handling different item types with a single interface</span></span><br><span class="line"><span class="keyword">from</span> itemadapter <span class="keyword">import</span> ItemAdapter</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QsbkPipeline</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.fp = open(<span class="string">'duanzi.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 爬虫启动时执行的函数，比如打开文件，也可写在构造函数中</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        print(<span class="string">'--------&gt; 爬虫开始了... &lt;---------'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 爬虫运行时，即qsbk_spider.py 中的生成器依次调用, item就是yield依次传来的值</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="comment"># item_json = json.dumps(item,ensure_ascii=False) # 第一次，未指定item</span></span><br><span class="line">        item_json = json.dumps(dict(item),ensure_ascii=<span class="literal">False</span>) <span class="comment"># 第二次，指定items.py中的类，此处用dict()函数将其转换为字典</span></span><br><span class="line">        self.fp.write(item_json + <span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">        print(<span class="string">'pipelines调用'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 爬虫结束时执行的函数，关闭文件</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.fp.close()</span><br><span class="line">        print(<span class="string">'--------&gt; 爬虫结束了... &lt;---------'</span>)</span><br></pre></td></tr></table></figure><h3 id="items-py编写"><a href="#items-py编写" class="headerlink" title="items.py编写"></a>items.py编写</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define here the models for your scraped items</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># See documentation in:</span></span><br><span class="line"><span class="comment"># https://docs.scrapy.org/en/latest/topics/items.html</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QsbkItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    </span><br><span class="line">    author = scrapy.Field() <span class="comment"># 选择需要输出结果的属性</span></span><br><span class="line">    content = scrapy.Field()</span><br></pre></td></tr></table></figure><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p>若要使用ITEM Pipelines，需要在setting.py中开启。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Configure item pipelines</span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'qsbk.pipelines.QsbkPipeline'</span>: <span class="number">300</span>, <span class="comment"># 当有多个pipelines，后面的值越小代表越优先执行</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="优化数据存储的方式"><a href="#优化数据存储的方式" class="headerlink" title="优化数据存储的方式"></a>优化数据存储的方式</h2><h3 id="方法一：使用JsonItemExporter"><a href="#方法一：使用JsonItemExporter" class="headerlink" title="方法一：使用JsonItemExporter"></a>方法一：使用<code>JsonItemExporter</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.exporters <span class="keyword">import</span> JsonItemExporter <span class="comment"># scrapy框架里的JSON导出器</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QsbkPipeline</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.fp = open(<span class="string">'duanzi.json'</span>, <span class="string">'wb'</span>) <span class="comment"># 二进制方法打开 , 不能指定 encoding = 'utf-8'</span></span><br><span class="line">        self.exporter = JsonItemExporter(self.fp, ensure_ascii = <span class="literal">False</span>, encoding = <span class="string">'utf-8'</span>)</span><br><span class="line">        self.exporter.start_exporting() <span class="comment"># 开始</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        print(<span class="string">'--------&gt; 爬虫开始了... &lt;---------'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.exporter.export_item(item)</span><br><span class="line">        print(<span class="string">'pipelines调用'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.exporter.finish_exporting() <span class="comment"># 该方法是把所有的item存为一个列表，每个元素是一个item</span></span><br><span class="line">        self.fp.close()</span><br><span class="line">        print(<span class="string">'--------&gt; 爬虫结束了... &lt;---------'</span>)</span><br></pre></td></tr></table></figure><blockquote><p>缺点：所有item存在一个列表中，一起写入文件，较消耗内存。</p><p>​            需要 开始 和 结束 。</p><p>但整体满足格式要求，一个大列表，每个元素是一个item的json格式数据。</p></blockquote><h3 id="方法二：使用JsonLinesItemExporter"><a href="#方法二：使用JsonLinesItemExporter" class="headerlink" title="方法二：使用JsonLinesItemExporter"></a>方法二：使用<code>JsonLinesItemExporter</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.exporters <span class="keyword">import</span> JsonLinesItemExporter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QsbkPipeline</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.fp = open(<span class="string">'duanzi.json'</span>, <span class="string">'wb'</span>) </span><br><span class="line">        self.exporter = JsonLinesItemExporter(self.fp, ensure_ascii = <span class="literal">False</span>, encoding = <span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        print(<span class="string">'--------&gt; 爬虫开始了... &lt;---------'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.exporter.export_item(item)</span><br><span class="line">        print(<span class="string">'pipelines调用'</span>) </span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.fp.close()</span><br><span class="line">        print(<span class="string">'--------&gt; 爬虫结束了... &lt;---------'</span>)</span><br></pre></td></tr></table></figure><blockquote><p>此方法把每个item作为一个单独的字典类型存入文件。</p><p>不需要 开始 和 结束 。</p><p>缺点：每行数据为一个字典（json格式），但整个文件并不满足json格式。</p></blockquote><h2 id="抓取多个页面"><a href="#抓取多个页面" class="headerlink" title="抓取多个页面"></a>抓取多个页面</h2><p>主要是最后几行代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line"><span class="keyword">from</span> qsbk.items <span class="keyword">import</span> QsbkItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QsbkSpiderSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'qsbk_spider'</span></span><br><span class="line">    allowed_domains = [<span class="string">'qiushibaike.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://qiushibaike.com/text/page/1/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        duanziDivs = response.xpath(<span class="string">"//div[@class='col1 old-style-col1']/div"</span>)</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> div <span class="keyword">in</span> duanziDivs:</span><br><span class="line">            author = div.xpath(<span class="string">".//h2/text()"</span>).get().strip()</span><br><span class="line">            href = div.xpath(<span class="string">'./a/@href'</span>).get() <span class="comment"># 获取a标签内的href的属性值</span></span><br><span class="line">            url = urljoin(self.start_urls[<span class="number">0</span>],href) <span class="comment"># 跳转到详情页</span></span><br><span class="line">            content = div.xpath(<span class="string">".//div[@class='content']//text()"</span>).extract()</span><br><span class="line">            content = <span class="string">''</span>.join(content).strip()</span><br><span class="line">            item = QsbkItem(author=author, content=content)</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">            print(<span class="string">'生成器调用：'</span>,i)</span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">        next_path = response.xpath(<span class="string">"//ul[@class='pagination']/li[last()]/a/@href"</span>).get() <span class="comment"># 底部显示页面的最后一个li标签，‘下一页’</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> next_path: <span class="comment"># 如果没有下一页</span></span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            next_url = urljoin(self.start_urls[<span class="number">0</span>], next_path)</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(next_url, callback=self.parse) <span class="comment"># 利用当前页的url，执行上面的parse解析函数</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇是python实例编写：&lt;strong&gt;糗事百科&lt;/strong&gt;，目的是介绍最基础的scrapy框架的写法和运行过程。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://iceWind-R.github.io/categories/Python/"/>
    
    
      <category term="Python" scheme="http://iceWind-R.github.io/tags/Python/"/>
    
      <category term="爬虫" scheme="http://iceWind-R.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Planan_03(7.6~7.12)</title>
    <link href="http://icewind-r.github.io/2020/07/04/Plan-02-%E7%AC%AC%E4%BA%8C%E5%91%A8/"/>
    <id>http://icewind-r.github.io/2020/07/04/Plan-02-%E7%AC%AC%E4%BA%8C%E5%91%A8/</id>
    <published>2020-07-04T10:17:35.000Z</published>
    <updated>2020-08-09T01:54:01.700Z</updated>
    
    <content type="html"><![CDATA[<p>第三周开始了，高考也将至，时间飞逝啊。</p><a id="more"></a><hr><p>本周目标：</p><ul><li><p>Scrapy框架学习，尝试独立完成一个爬取实例。</p></li><li><p>学习小组确定主题，尝试开始或者一起学习，（Github Page搭建官网？）。</p></li><li><p>APP小组尝试开始。</p></li><li><p>python Web？</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;第三周开始了，高考也将至，时间飞逝啊。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Learning" scheme="http://iceWind-R.github.io/categories/Learning/"/>
    
    
      <category term="plan" scheme="http://iceWind-R.github.io/tags/plan/"/>
    
  </entry>
  
</feed>
